{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size: 3em'>SentiLens - Uncover reviews' hidden emotion</div>\n",
    "\n",
    "__Prepared by:__ Tina Vu</br>\n",
    "__Date:__ 20231208</br>\n",
    "\n",
    "Employing aspect-based sentiment analysis (ABSA) to extract valuable feature insights from e-commerce product reviews, thereby empowering consumers to make more informed purchasing decisions and enhancing their overall user experience on the platform.\n",
    "\n",
    "Utilizing manually annotated reviews for aspect sentiment analysis to extract aspects and predict sentiments from reviews. This enables consumers to obtain a condensed overview of sentiments related to various product features, eliminating the need to delve into an extensive array of reviews. As a result, the decision-making process becomes more streamlined and user-friendly.\n",
    "\n",
    "__Approach:__\n",
    "\n",
    "ABSA\n",
    "\n",
    "__Phase:__\n",
    "1. Supervised ABSA (What, How)\n",
    "2. Unsupervised ABSA\n",
    "3. Add 'Why' into ABSA\n",
    "\n",
    "<div style='font-size: 2em'>Phase 1 - Aspect Extration</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Import & prepare dataset](#toc1_)    \n",
    "  - 1.1. [Import data](#toc1_1_)    \n",
    "  - 1.2. [Preparing dataset for modelling](#toc1_2_)    \n",
    "    - 1.2.1. [Unified BIO tagging encode](#toc1_2_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tina.vu/work/fun/brainstation_capstone_env/brainstation_capstone/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "en_stop_words = set(stopwords.words('english'))\n",
    "# nltk.download()\n",
    "\n",
    "from sklearn_crfsuite import CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Import & prepare dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Import data](#toc0_)\n",
    "\n",
    "We will load laptop reviews dataset with aspect term & sentiment annotations.\n",
    "\n",
    "The dataset comes in two parts:\n",
    "- train: 3,048 records\n",
    "- test: 800 records\n",
    "\n",
    "Each record is a sentence with zero, one or multiple aspect terms. Each aspect term has the following features:\n",
    "- start character index\n",
    "- end character index\n",
    "- sentiment/ polarity (positive or negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3048, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_json('data/laptop/train.json')\n",
    "df_val = pd.read_json('data/laptop/validate.json')\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[{'term': 'cord', 'polarity': 'neutral', 'from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>812</td>\n",
       "      <td>I bought a HP Pavilion DV4-1222nr laptop and h...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>[{'term': 'service center', 'polarity': 'negat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2328</td>\n",
       "      <td>I investigated netbooks and saw the Toshiba NB...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2193</td>\n",
       "      <td>The other day I had a presentation to do for a...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  \\\n",
       "0  2339  I charge it at night and skip taking the cord ...   \n",
       "1   812  I bought a HP Pavilion DV4-1222nr laptop and h...   \n",
       "2  1316  The tech guy then said the service center does...   \n",
       "3  2328  I investigated netbooks and saw the Toshiba NB...   \n",
       "4  2193  The other day I had a presentation to do for a...   \n",
       "\n",
       "                                             aspects  \n",
       "0  [{'term': 'cord', 'polarity': 'neutral', 'from...  \n",
       "1  [{'term': '', 'polarity': '', 'from': 0, 'to':...  \n",
       "2  [{'term': 'service center', 'polarity': 'negat...  \n",
       "3  [{'term': '', 'polarity': '', 'from': 0, 'to':...  \n",
       "4  [{'term': '', 'polarity': '', 'from': 0, 'to':...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Preparing dataset for modelling](#toc0_)\n",
    "\n",
    "The task we are solving is Named Entity Recognition (NER) which is a sequential labeling task, a.k.a we would like to predict whether a token (word) in each sentence is part of an aspect term or not.\n",
    "\n",
    "In order to prepare the data for NER task, we need to label our tokens. Here, I implemented a unified BIO tagging technique which combines aspect boundaries and aspect sentiment.\n",
    "\n",
    "Word boundaries:\n",
    "- B: indicates the 1st word in the aspect term\n",
    "- I: indicates the subsequent word in the aspect term\n",
    "- O: indicates words that are not part of any aspect term\n",
    "\n",
    "Aspect sentiment:\n",
    "- POS: positive\n",
    "- NEU: neutral\n",
    "- NEG: conflict\n",
    "\n",
    "This BIO label technique is more effective in recognizing unigram and n-gram aspect terms comparing to a binary classification (whether a token is part of an aspect). By using a unified a approach, we can combine two tasks: aspect extraction and sentiment classification into one task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3036, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, I will need to drop some duplicated data in our training dataset, as identified in the EDA process.\n",
    "df_train.drop_duplicates(subset='text', inplace=True)\n",
    "\n",
    "# We have removed 12 duplicated records in our training dataset\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. <a id='toc1_2_1_'></a>[BIO tagging encode](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I defined a function to encode our sentences' aspects using a unified BIO tagging technique (<a href='https://arxiv.org/pdf/1811.05082.pdf'>reference</a>) that combines aspect boundaries and aspect sentiment in a single label.\n",
    "\n",
    "Word boundaries:\n",
    "- B: indicates the 1st word in the aspect term\n",
    "- I: indicates the subsequent word in the aspect term\n",
    "- O: indicates words that are not part of any aspect term\n",
    "\n",
    "Aspect sentiment:\n",
    "- POS: positive\n",
    "- NEU: neutral\n",
    "- NEG: conflict\n",
    "\n",
    "Unified BIO tagging will be like: B-NEU, I-NEU\n",
    "\n",
    "For example:\n",
    "['I', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'cord', 'with', 'me', 'because', 'of', 'the', 'good', 'battery', 'life', '.']\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEU', 'I-NEU', 'O', 'O', 'O', 'O', 'O', 'B-POS', 'I-POS', 'I-POS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset provides a full sentence and annotates aspect term using character index, we cannot perform word_tokenize directly on the raw text due to:\n",
    "  1. Word_tokenize separates punctuation as a single normal token which makes it difficult to re-string (combine) word tokens (using ' '.join(tokens)) for character index, as it adds extra spaces between word and punctuations, thus invalidate character index for aspect terms. This makes it very difficult to align the aspect term character index and word index accurately.</br>\n",
    "  E.g. \"I love pizza, cheese.\", term indexes are (7,12),(16,22), re-string tokens from word_tokenize (' '.join(tokens)) can turn the sentence into \"I love pizza__\\<extra space\\>__, cheese__\\<extra space\\>__.\". The char index of the aspect terms now become (7,12), `(17,23)`.\n",
    "  \n",
    "  2. Word_tokenize tends to not separate words that have special characters between them (other than space and common punctuations like <,.:;>), while some terms treated those chunks as separated terms. \n",
    "  E.g. \"size/screen\" is a single token based on word_tokenize, while terms defined this as two separate tokens. \n",
    "\n",
    "Therefore, I applied the below approach to acoomodate the above short commings:\n",
    "  1. Add aspect_prefix & aspect_suffix (with additional spaces in order to overcome issue #2) to the start & end of each aspect term (to overcome issue #1) in the sentence using from, to char index as supplied by the dataset\n",
    "  2. Perform word_tokenize on the new aspect_annotated_sentence\n",
    "  3. Perform BIO tagging on the sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_unified_BIO (x, sentiment_tag=False):\n",
    "  '''  This function puts aspect's details into a dictionary, and multiple aspect as an array\n",
    "  \n",
    "  Parameter:\n",
    "  - ASPECTS: dictionary array\n",
    "    dictionary of\n",
    "    - term\n",
    "    - polarity\n",
    "    - term_start\n",
    "    - term_end\n",
    "\n",
    "    For example:\n",
    "    [\n",
    "      {'term':'cord', 'polarity':'neutral', 'from': 41, 'to': 45},\n",
    "      {'term':'battery life', 'polarity':'positive', 'from': 74, 'to': 86}\n",
    "    ]\n",
    "  - SENTIMENT_TAG: boolean\n",
    "    True: if we want to return sentiment polarity with BIO tagging (unified BIO)\n",
    "    False: if we do not want to return sentiment polarity with BIO tagging (just pure BIO)\n",
    "    \n",
    "  Output: \n",
    "  - TEXT_TOKENS: array of string\n",
    "    Sentence tokens\n",
    "    E.g. ['Boot', 'time', 'is', 'super', 'fast', ',', 'around', 'anywhere', 'from', '35', 'seconds', 'to', '1', 'minute', '.']\n",
    "    \n",
    "  - TAGS: array of string\n",
    "    Unified BIO tags (if sentiment_tag paratement is set to True) or BIO tags (if sentiment_tag is set to False)\n",
    "      Aspect boundaries:\n",
    "        B: beginning of aspect term\n",
    "        I: subsequent words of aspect term\n",
    "        O: outside of aspect term\n",
    "      Sentiment:\n",
    "        POS: positive\n",
    "        NEG: negative\n",
    "        NEU: neutral\n",
    "        CON: conflict\n",
    "    E.g. ['B-POS', 'I-POS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "  - ASPECT_COMPUTE_PAIRS: tuple of two strings\n",
    "    1st String combining of all aspect terms as provided by the dataset\n",
    "    2nd String combining of all aspect terms from annotated BIO tagged using text_token\n",
    "    E.g.(' Boot time', ' Boot time')\n",
    "\n",
    "  - IS_INCORRECT_TAGGING: boolean\n",
    "    True: when ASPECT_COMPUTE_PAIRS is different from each other\n",
    "    False: when ASPECT_COMPUTE_PAIRS is exactly like each other\n",
    "    This may return False, when there is some special characters in aspect term that causing them to be splitted into a different token.\n",
    "    E.g. aspect_term `15\" TV`, computed_term can return `15 \" TV`. Thus, they are not exactly matching each other, but they should be okay\n",
    "  '''\n",
    "\n",
    "  aspects = x['aspects']\n",
    "  aspects = sorted(aspects, key=lambda d: int(d['from']))  # sort aspects based on from, some aspects are not sorted: later terms in the sentence sometimes are placed before terms appear earlier. \n",
    "  \n",
    "  text = x['text']\n",
    "\n",
    "  sentiment_tag_map = {'neutral': '-NEU'\n",
    "                       ,'conflict':'-CON'\n",
    "                       ,'positive':'-POS'\n",
    "                       ,'negative':'-NEG'}\n",
    "  \n",
    "  aspect_prefix = ' XXATBXX' # add leading space to break words if they are in the same chunk. E.g. \"size/window\" --> \"size / window\"\n",
    "  aspect_suffix = 'XXATEXX ' # add trailing space\n",
    "\n",
    "  # these are for validation to ensure the BIO tagging is accurate\n",
    "  aspect_terms = ''\n",
    "  aspect_terms_compute = ''\n",
    "\n",
    "  # we cannot perform word_tokenize directly on the raw text due to:\n",
    "  # 1. there is no space between punctuation and word, which makes it difficult to calculate word index from char index when concatenating word tokens for word index search\n",
    "  # 2. terms can be partial of a word token, e.g. \"size/screen\" is a single token based on word_token, while terms defined this as two separate terms. \n",
    "  # Therefore, I applied the below approach:\n",
    "  # 1. Add aspect_prefix & aspect_suffix to the start & end of each aspect term in the sentence using from, to char index as supplied by the dataset\n",
    "  # 2. Perform word_tokenize on the new aspect_annotated_sentence\n",
    "  # 3. Perform BIO tagging on the sentence token\n",
    "\n",
    "  # 1. Add aspect prefix & suffix to the sentence\n",
    "  aspect_annotated_sentence = text\n",
    "\n",
    "  for i, k in enumerate(aspects):\n",
    "    term = k['term']\n",
    "    \n",
    "    if k['term'] != '': # there are empty aspects but still have an empty dict structure, so we only perform tagging for those that has `term` != ''\n",
    "      # this is for validation purposes only\n",
    "      aspect_terms += ' ' + term \n",
    "\n",
    "      if k['polarity'] == '':\n",
    "        print(k['id'])\n",
    "\n",
    "      polarity = sentiment_tag_map[k['polarity']] if sentiment_tag == True else '' # get polarity encode\n",
    "  \n",
    "      i_from = int(k['from']) + i * (len(aspect_prefix) + len(aspect_suffix) + len(polarity)) # re-calculate from & by shifting them by the length of additional character added for aspect prefix & suffix\n",
    "      i_to = int(k['to']) + i * (len(aspect_prefix) + len(aspect_suffix) + len(polarity))\n",
    "      \n",
    "      aspect_annotated_sentence = aspect_annotated_sentence[:i_from] + aspect_prefix+ polarity + aspect_annotated_sentence[i_from:i_to] + aspect_suffix + aspect_annotated_sentence[i_to:]\n",
    "  \n",
    "  # Tokenize aspect annotated sentence\n",
    "  text_tokens = word_tokenize(aspect_annotated_sentence)\n",
    "\n",
    "  # Perfom BIO tagging\n",
    "  tags = []\n",
    "  aspect_start = False\n",
    "  polarity = ''\n",
    "\n",
    "  for i,k in enumerate(text_tokens):\n",
    "    tag = 'O' # default token tag as 'O' outside of aspect term\n",
    "\n",
    "    if k[:7] == aspect_prefix.strip(): # if we see aspect prefix in a term, update tag as 'B' & set aspect_start as True\n",
    "      aspect_start = True\n",
    "      polarity = k[7:11] # extract polarity for next following tokens if there is\n",
    "      tag = 'B' + polarity\n",
    "      \n",
    "    elif aspect_start == True: # if token does not have aspect_prefx, set tag to I if aspect_start is still True\n",
    "      tag = 'I' + polarity\n",
    "      \n",
    "    else: # if aspect_start is False or if there is no aspect prefix\n",
    "      tag = 'O'\n",
    "\n",
    "    text_tokens[i] = re.sub(aspect_prefix.strip() + '.{4}', '',text_tokens[i]).replace(aspect_suffix.strip(),'') # clean text token by removing aspect prefix, polarity & aspect suffix if any\n",
    "    tags.append(tag) \n",
    "    \n",
    "    if k[-7:] == aspect_suffix.strip(): # If token contains aspect_suffix, restart aspect_start as False and polarity as empty\n",
    "      aspect_start = False\n",
    "      polarity = ''\n",
    "    \n",
    "  # This is for validation purposes only\n",
    "  for i, k in enumerate(tags):\n",
    "    if k != 'O':\n",
    "      aspect_terms_compute += ' ' + text_tokens[i]\n",
    "  \n",
    "  # This is for validation purposes only\n",
    "  aspect_compute_pairs = (aspect_terms, aspect_terms_compute)\n",
    "  is_incorrect_tagging = True if aspect_terms != aspect_terms_compute  else False\n",
    "  \n",
    "  return pd.Series([text_tokens, tags, aspect_compute_pairs, is_incorrect_tagging])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['text_token','aspect_unified_bio','aspect_compute_pairs','is_incorrect_tagging']] = df_train.apply(lambda x: encode_unified_BIO(x, True), axis=1)\n",
    "df_val[['text_token','aspect_unified_bio','aspect_compute_pairs','is_incorrect_tagging']] = df_val.apply(lambda x: encode_unified_BIO(x, True), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing BIO encode, we want to check if there is any misclassification using is_incorrect_tagging.\n",
    "\n",
    "There are only 20 possible inccorect BIO labels in df_train and 10 for df_val.\n",
    "\n",
    "Looking through a few examples, it seems like the issues are mainly around special characters inside aspect terms that cause the comparision strings to be different, however, the BIO labels are still accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of possible incorrect labels in df_train:  20\n",
      "# of possible incorrect labels in df_train:  10\n",
      "\n",
      "\n",
      " Some examples\n",
      "['The', 'tech', 'guy', 'then', 'said', 'the', 'service', 'center', 'does', 'not', 'do', '1-to-1', 'exchange', 'and', 'I', 'have', 'to', 'direct', 'my', 'concern', 'to', 'the', '', \"''\", 'sales', \"''\", 'team', ',', 'which', 'is', 'the', 'retail', 'shop', 'which', 'I', 'bought', 'my', 'netbook', 'from', '.']\n",
      "[{'term': 'service center', 'polarity': 'negative', 'from': '27', 'to': '41'}, {'term': '\"sales\" team', 'polarity': 'negative', 'from': '109', 'to': '121'}, {'term': 'tech guy', 'polarity': 'neutral', 'from': '4', 'to': '12'}]\n",
      "(' tech guy service center \"sales\" team', \" tech guy service center  '' sales '' team\")\n",
      "['O', 'B-NEU', 'I-NEU', 'O', 'O', 'O', 'B-NEG', 'I-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'I-NEG', 'I-NEG', 'I-NEG', 'I-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "40\n",
      "40\n",
      "\n",
      "\n",
      "['I', 'also', 'got', 'the', 'added', 'bonus', 'of', 'a', '30', \"''\", 'HD', 'Monitor', ',', 'which', 'really', 'helps', 'to', 'extend', 'my', 'screen', 'and', 'keep', 'my', 'eyes', 'fresh', '!']\n",
      "[{'term': '30\" HD Monitor', 'polarity': 'positive', 'from': '32', 'to': '46'}, {'term': 'screen', 'polarity': 'neutral', 'from': '80', 'to': '86'}]\n",
      "(' 30\" HD Monitor screen', \" 30 '' HD Monitor screen\")\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-POS', 'I-POS', 'I-POS', 'I-POS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEU', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "26\n",
      "26\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('# of possible incorrect labels in df_train: ', df_train['is_incorrect_tagging'].sum())\n",
    "print('# of possible incorrect labels in df_train: ', df_val['is_incorrect_tagging'].sum())\n",
    "print('\\n\\n Some examples')\n",
    "\n",
    "n_samples = 2\n",
    "sample = df_train[df_train['is_incorrect_tagging']].copy().reset_index()\n",
    "\n",
    "for i in range(0, n_samples):\n",
    "  s = sample.iloc[i]\n",
    "  print(s['text_token'])\n",
    "  print(s['aspects'])\n",
    "  print(s['aspect_compute_pairs'])\n",
    "  print(s['aspect_unified_bio'])\n",
    "  print(len(s['text_token']))\n",
    "  print(len(s['aspect_unified_bio']))\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word features\n",
    "Here we will populate some word features for each token in the sentence, such as:\n",
    "- word\n",
    "- stemming / lemming versions of word\n",
    "- part of speech (POS) of word\n",
    "- words sentiment POS\n",
    "- context words within a pre-defined window (5 words surrounding the token)\n",
    "- context words stemming/ lemming\n",
    "- context words POS\n",
    "- context words sentiment POS\n",
    "- ...\n",
    "The list is not exhaustive, and is an iterative process as we explore on EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sentences into features\n",
    "def word2features(sent, i, window_size=5): \n",
    "    word = sent[i]\n",
    "\n",
    "    _, pos = zip(*nltk.pos_tag(sent))\n",
    "\n",
    "    window_size = int((window_size - 1)/ 2 if (window_size % 2) == 1 else window_size / 2)\n",
    "    \n",
    "    features = {\n",
    "        'word.lower()': word.lower(), # word\n",
    "        'word.index()': i,\n",
    "        'word.reverseindex()': len(sent) - 1 - i, # reverse index - nth word from end of sentence\n",
    "        'word.pos': pos[i],\n",
    "        'word.isstopword()': word in en_stop_words,\n",
    "        'word[-3:]': word[-3:], # last 4 char\n",
    "        'word[-2:]': word[-2:], # last 3 char - in case of -ing, -ion, etc.\n",
    "        'word.isupper()': word.isupper(), # is the word in upper case\n",
    "        'word.istitle()': word.istitle(), # is the first letter of the word in upper case\n",
    "        'word.isdigit()': word.isdigit(), # is the word full of digit\n",
    "        'word.isspecialchar()': re.sub('[^\\w,\\d,\\s]', '', word.lower()) == '', # is punctuation/ special characters\n",
    "    }\n",
    "    if i > 0:\n",
    "        for k in range(1, min(window_size, i)+1):\n",
    "            prev_word = sent[i - k]\n",
    "            prev_pos = pos[i - k]\n",
    "            \n",
    "            features.update({\n",
    "                f'-{k}:word.lower()': prev_word.lower(),\n",
    "                f'-{k}:word.pos': prev_pos,\n",
    "                f'-{k}:word.isstopword()': prev_word in en_stop_words,\n",
    "                f'-{k}:word.istitle()': prev_word.istitle(),\n",
    "                f'-{k}:word.isupper()': prev_word.isupper(),\n",
    "                f'-{k}:word.isspecialchar()': re.sub('[^\\w,\\d,\\s]', '', prev_word.lower()) == '', # is punctuation/ special characters\n",
    "            })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sentence\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        for k in range(1, min(window_size, len(sent) - i - 1)+1):\n",
    "            next_word = sent[i + k]\n",
    "            next_pos = pos[i + k]\n",
    "\n",
    "            features.update({\n",
    "                f'+{k}:word.lower()': next_word.lower(),\n",
    "                f'+{k}:word.pos': next_pos,\n",
    "                f'+{k}:word.isstopword()': next_word in en_stop_words,\n",
    "                f'+{k}:word.istitle()': next_word.istitle(),\n",
    "                f'+{k}:word.isupper()': next_word.isupper(),\n",
    "                f'-{k}:word.isspecialchar()': re.sub('[^\\w,\\d,\\s]', '', next_word.lower()) == '', # is punctuation/ special characters\n",
    "            })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to convert sentences into feature sequences\n",
    "def sent2features(sent, window_size=5):\n",
    "    return [word2features(sent, i, window_size) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train = [sent2features(sentence, 5) for sentence in df_train['text_token']]\n",
    "# y_train = df_train['aspect_unified_bio']\n",
    "\n",
    "# # X_val = [sent2features(sentence,5) for sentence in df_val['text_token']]\n",
    "# # y_val = df_val['aspect_encode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data processing functions\n",
    "These functions help us streamline and ensure consistencies in our data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df, window_size=5):\n",
    "  df[['text_token','aspect_unified_bio']], _, _ = df.apply(lambda x: encode_unified_BIO(x, True), axis=1)\n",
    "  X =  [sent2features(sentence, window_size) for sentence in df['text_token']]\n",
    "  y = df['aspect_unified_bio']\n",
    "  return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_features(sentences, window_size=5):\n",
    "  sentences_token = [word_tokenize(sentence) for sentence in sentences]\n",
    "  X =  [sent2features(sentence, window_size) for sentence in sentences_token]\n",
    "  return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. POS:\n",
    "  - aspect - sentiment - neither\n",
    "  - context\n",
    "2. Word form (compact Xx)\n",
    "3. Word index\n",
    "4. Sentiment terms around aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "- Scale the data\n",
    "- Add regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert X_train to tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf = pd.DataFrame()\n",
    "\n",
    "def features2df(sentence_index, X):\n",
    "  df = pd.DataFrame(X)\n",
    "  df['nth_sentence'] = sentence_index\n",
    "  return df\n",
    "\n",
    "# Assuming X_train is a list of DataFrames\n",
    "X_train_rf = pd.concat([features2df(i,X) for i,X in enumerate(X_train)], ignore_index=True)\n",
    "X_train_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf.replace(True,1, inplace=True)\n",
    "X_train_rf.replace(False,0, inplace=True)\n",
    "X_train_rf.fillna(-1, inplace=True)\n",
    "X_train_rf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pd.Series(X_train_rf.columns)\n",
    "drop_cols = cols[cols.str.contains('.*word\\.lower\\(\\)', regex=True)]\n",
    "drop_cols = pd.concat([drop_cols, cols[cols.str.contains('.*word\\[-\\d\\:]', regex=True)]])\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf.drop(columns=drop_cols, inplace=True)\n",
    "X_train_rf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf = pd.get_dummies(X_train_rf, drop_first=True)\n",
    "X_train_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\"B-POS\": 0, \"I-POS\": 1, \"B-NEU\": 2, \"I-NEU\": 3, \"B-NEG\": 4, \"I-NEG\": 5, \"O\":6}\n",
    "y_train_rf = [labels[y] for sentence in y_train for y in sentence]\n",
    "y_train_rf[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pred = cross_val_predict(RandomForestClassifier(n_estimators=20),X=X_train_rf.drop(columns='nth_sentence'), y=y_train_rf, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_pred=pred, y_true=y_train_rf)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train CRF model\n",
    "crf_model = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100)\n",
    "try:\n",
    "  crf_model.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def post_process_predictions(predictions):\n",
    "#   for i in range(1, len(predictions)):\n",
    "#       if predictions[i] == 'I' and predictions[i - 1] == 'O':\n",
    "#           predictions[i] = 'O'  # Change 'I' to 'O' if not preceded by 'B'\n",
    "#   return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random\n",
    "from highlight_text import HighlightText, ax_text, fig_text\n",
    "\n",
    "def vizualize_samples (sentences, pred_tags, tags = None):\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(30,10))\n",
    "  font = {'family' : 'arial',\n",
    "          'size'   : 16}\n",
    "  matplotlib.rc('font', **font)\n",
    "  final_text = []\n",
    "  color = []\n",
    "  pos_element = {\"bbox\": {\"edgecolor\": \"Green\", \"facecolor\": \"#99FF00\", \"linewidth\": 1.5, \"pad\": 1}} \n",
    "  neu_element = {\"bbox\": {\"edgecolor\": \"Orange\", \"facecolor\": \"Yellow\", \"linewidth\": 1.5, \"pad\": 1.5}} \n",
    "  neg_element = {\"bbox\": {\"edgecolor\": \"Red\", \"facecolor\": \"#FF99CC\", \"linewidth\": 1.5, \"pad\": 1}}\n",
    "  \n",
    "  color_map = {'POS': pos_element, 'NEU': neu_element, 'NEG':neg_element}\n",
    "\n",
    "  for s in range(0, len(sentences)):\n",
    "    final_text.append('(P) ')\n",
    "    chunk = []\n",
    "    next_tag = ''\n",
    "    \n",
    "    for w in range(0, len(sentences[s])):\n",
    "      # print(w)\n",
    "      word = sentences[s][w]\n",
    "      tag = pred_tags[s][w]\n",
    "      # print(tag[2:])  \n",
    "      if w + 1 < len(sentences[s]):\n",
    "        next_tag = pred_tags[s][w+1]\n",
    "      else:\n",
    "        next_tag = 'O'\n",
    "      \n",
    "      # print(word, tag, next_tag)\n",
    "\n",
    "      if tag == 'O':\n",
    "        final_text.append(word)\n",
    "      elif (tag in ['I-POS','I-NEG', 'I-NEU','B-POS','B-NEG', 'B-NEU']):\n",
    "        chunk.append(word)\n",
    "\n",
    "      if (next_tag in ['B-POS','B-NEG', 'B-NEU', 'O']) & (len(chunk) > 0):\n",
    "        final_text.append(f'  <{\" \".join(chunk)}>  ')\n",
    "        color.append(color_map[tag[2:]])\n",
    "        chunk = []\n",
    "\n",
    "    if tags is not None:\n",
    "      final_text.append('\\n')\n",
    "      final_text.append('(A) ')\n",
    "      for w in range(0, len(sentences[s])):\n",
    "        # print(w)\n",
    "        word = sentences[s][w]\n",
    "        tag = tags[s][w]\n",
    "        if w + 1 < len(sentences[s]):\n",
    "          next_tag = tags[s][w+1]\n",
    "        else:\n",
    "          next_tag = 'O'\n",
    "          \n",
    "        if tag == 'O':\n",
    "          final_text.append(word)\n",
    "        elif (tag in ['I-POS','I-NEG', 'I-NEU','B-POS','B-NEG', 'B-NEU']):\n",
    "          chunk.append(word)\n",
    "\n",
    "        if (next_tag in ['B-POS','B-NEG', 'B-NEU', 'O']) & (len(chunk) > 0):\n",
    "          final_text.append(f'  <{\" \".join(chunk)}>  ')\n",
    "          color.append(pos_element)\n",
    "          chunk = []\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # if tags is not None:\n",
    "    #   final_text.append('\\n')\n",
    "    #   final_text.append('(A) ')\n",
    "    #   for w in range(0, len(sentences[s])):\n",
    "    #     word = sentences[s][w]\n",
    "    #     tag = tags[s][w]\n",
    "\n",
    "    #     if tag !='O':\n",
    "    #       final_text.append('<{}>'.format(word))\n",
    "    #       if tag in ['I-POS','I-NEG', 'I-NEU']:\n",
    "    #         # color.append(color[-1])\n",
    "    #         color.append(pos_element)\n",
    "    #       else:\n",
    "    #         color.append ({'color':random.choice(['blue','green','red','magenta'])})\n",
    "    #     else:\n",
    "    #       final_text.append(word)\n",
    "    \n",
    "    final_text.append('\\n---------------\\n')\n",
    "\n",
    "\n",
    "    HighlightText(x=0, y=1,\n",
    "                s=' '.join(final_text),\n",
    "                highlight_textprops=color,\n",
    "                ax=ax)\n",
    "                \n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10\n",
    "integer = 285 #random.randint(0,500)\n",
    "tags = y_val[integer:integer+samples].array\n",
    "pred_tags = crf_model.predict(X_val[integer:integer+samples])\n",
    "sentences = [[x['word.lower()'] for x in sentence] for sentence in X_val[integer:integer+samples]]\n",
    "\n",
    "print(integer)\n",
    "vizualize_samples(sentences, pred_tags, tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "nth = integer + i\n",
    "print(nth)\n",
    "print(df_val.iloc[nth]['text'])\n",
    "\n",
    "print(df_val.iloc[nth]['aspects'])\n",
    "\n",
    "print(df_val.iloc[nth]['pairs'])\n",
    "\n",
    "print(pred_tags[i])\n",
    "print(tags[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with new data\n",
    "sample = pd.Series(['I was pleasantly surprised by the quality of this laptop for the money. '\n",
    "            ,'I am not super techy so it may be difficult for me to comment on the technical specifications of the laptop, but I was pleasantly surprised with the quality of the product especially at this price point.'])\n",
    "sample_text_token = [word_tokenize(sentence) for sentence in sample]\n",
    "X_sample = [sent2features(sentence, 5) for sentence in sample_text_token]\n",
    "\n",
    "predicted_labels = crf_model.predict(X_sample)\n",
    "\n",
    "vizualize_samples(sample_text_token, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "tags = y_train\n",
    "pred_tags = crf_model.predict(X_train)\n",
    "\n",
    "\n",
    "# Create a mapping of labels to indices\n",
    "labels = {\"B-POS\": 0, \"I-POS\": 1, \"B-NEU\": 2, \"I-NEU\": 3, \"B-NEG\": 4, \"I-NEG\": 5, \"O\":6}\n",
    "\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[x] for x in sum([tag for tag in pred_tags],[])])\n",
    "truths = np.array([labels[x] for x in sum([tag for tag in tags],[])])\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    target_names= [\"B-POS\", \"I-POS\", \"B-NEU\", \"I-NEU\", \"B-NEG\", \"I-NEG\", \"O\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tags = y_val\n",
    "pred_tags = crf_model.predict(X_val)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[x] for x in sum([tag for tag in pred_tags],[])])\n",
    "truths = np.array([labels[x] for x in sum([tag for tag in tags],[])])\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    target_names=[\"B-POS\", \"I-POS\", \"B-NEU\", \"I-NEU\", \"B-NEG\", \"I-NEG\", \"O\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Add more features:\n",
    "  - head words\n",
    "  - Google Word2Vec cluster_id\n",
    "  - Stemming / Lemming\n",
    "  - word index from beggining & ending of the sentence\n",
    "- Employ pre-trained word embeddings\n",
    "- Re-train model using rule-based aspect term extraction on larger dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainstation_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
