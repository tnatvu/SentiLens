{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org/api/nltk.tokenize.treebank.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Data preparation](#toc1_)    \n",
    "  - 1.1. [Tokenize sentence and aspect BIO encoding class](#toc1_1_)    \n",
    "- 2. [EDA](#toc2_)    \n",
    "  - 2.1. [Sentence length](#toc2_1_)    \n",
    "- 3. [CRF with FastText embeddings](#toc3_)    \n",
    "- 4. [Bi-LSTM](#toc4_)    \n",
    "- 5. [RNN](#toc5_)    \n",
    "- 6. [Simple RNN?!?! From study material](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # Replace 'path/to/word2vec/model.bin' with the path to your pretrained Word2Vec model\n",
    "# model_path = 'path/to/word2vec/model.bin'\n",
    "# word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# # Access the vector for a specific word\n",
    "# vector_for_word = word2vec_model['example']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from highlight_text import HighlightText, ax_text, fig_text\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# from sklearn_crfsuite import CRF\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Data preparation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Tokenize sentence and aspect BIO encoding class](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceToken:\n",
    "  '''\n",
    "    SentenceToken\n",
    "\n",
    "  '''\n",
    "  def __init__(self, sentence, aspect_type=None, aspects=None, sentence_id=None):\n",
    "    \n",
    "    if sentence_id is not None:\n",
    "      print(sentence_id)\n",
    "\n",
    "    self.sentence_id = sentence_id\n",
    "    self.sentence = sentence.replace(u\"\\u00A0\", \" \")\n",
    "                            \n",
    "    self.aspect_bio_tags = None\n",
    "    self.unified_aspect_bio_tags = None\n",
    "    self.token_span = None\n",
    "    self.space_pre_token = None\n",
    "\n",
    "    # Tokenize sentence\n",
    "    self.__tokenize_sentence(self.sentence)\n",
    "\n",
    "    if aspect_type == 'dict':\n",
    "      self.set_aspect_tagging_from_dict(aspects)\n",
    "    elif aspect_type == 'bio':\n",
    "      self.set_aspect_bio_tags(aspects)\n",
    "    elif aspect_type == 'unified bio':\n",
    "      self.set_aspect_unified_bio_tags(aspects)\n",
    "  \n",
    "  def __tokenize_sentence(self, sentence):\n",
    "    # self.sentence = sentence\n",
    "    \n",
    "    token_span = list(TreebankWordTokenizer().span_tokenize(sentence))\n",
    "\n",
    "    new_token_span = []\n",
    "    \n",
    "    for k in token_span:\n",
    "      token_start = k[0]\n",
    "      token_end = k[1]\n",
    "\n",
    "      token = sentence[token_start:token_end]\n",
    "      sub_tokens = re.split(r'([^\\w,\\d])', token)\n",
    "      \n",
    "      sub_token_start = token_start\n",
    "      for sub_token in sub_tokens:\n",
    "        if len(sub_token) != 0:\n",
    "          sub_token_end = sub_token_start + len(sub_token)\n",
    "          new_token_span.append((sub_token_start, sub_token_end))\n",
    "          sub_token_start = sub_token_end\n",
    "    \n",
    "    self.token_span = new_token_span\n",
    "    self.space_pre_token = [True if sentence[k[0]-1:k[0]] == ' ' else False for i,k in enumerate(new_token_span)]\n",
    "\n",
    "\n",
    "  def set_aspect_tagging_from_dict(self, aspects):\n",
    "    polarity_map = {'positive':'POS'\n",
    "              ,'negative': 'NEG'\n",
    "              ,'conflict': 'CON'\n",
    "              ,'neutral': 'NEU'}\n",
    "    \n",
    "    bio_tags = ['O'] * len(self.token_span)\n",
    "    unified_bio_tags = bio_tags\n",
    "\n",
    "    for x in aspects:\n",
    "      if x['term'] != '':\n",
    "        aspect_from = int(x['from'])\n",
    "        aspect_to = int(x['to'])\n",
    "        polarity = '-' + polarity_map[x['polarity']]\n",
    "        aspect_token_ids =  [i for i, v in enumerate(self.token_span) if (v[0] >= aspect_from) & (v[1] <= aspect_to)]\n",
    "        aspect_from_index = min(aspect_token_ids)\n",
    "        aspect_to_index = max(aspect_token_ids)\n",
    "        aspect_length = aspect_to_index - aspect_from_index\n",
    "        bio_tags = bio_tags[:aspect_from_index] + ['B'] + ['I'] * (aspect_length) + bio_tags[aspect_to_index+1:]\n",
    "        unified_bio_tags = unified_bio_tags[:aspect_from_index] + ['B' + polarity] + ['I'+ polarity] * (aspect_length) + unified_bio_tags[aspect_to_index+1:]\n",
    "    \n",
    "    self.set_aspect_bio_tags(bio_tags)\n",
    "    self.set_aspect_unified_bio_tags(unified_bio_tags)\n",
    "\n",
    "  def rebuild_sentence_from_token(self):\n",
    "    return ''.join([(' ' if self.space_pre_token[i] else '') + self.sentence[k[0]:k[1]] for i, k in enumerate(self.token_span)])\n",
    "  \n",
    "  def get_sentence_token_with_aspect_bio_tag(self, unified_bio_tag=False):\n",
    "    if (unified_bio_tag == False) & (self.aspect_bio_tags is None):\n",
    "      raise Exception('No BIO tags provided. Use \"SentenceToken.set_aspect_bio_tags()\" method to add bio_tags')\n",
    "    elif (unified_bio_tag == True) & (self.aspect_unified_bio_tags is None):\n",
    "      raise Exception('No Unified BIO tags provided. Use \"SentenceToken.set_aspect_unified_bio_tags()\" method to add unified_bio_tags')\n",
    "    else:\n",
    "      return [(self.sentence[k[0]:k[1]], self.aspect_unified_bio_tags[i] if unified_bio_tag else self.aspect_bio_tags[i]) for i, k in enumerate(self.token_span)]\n",
    "\n",
    "  def set_aspect_bio_tags(self, aspect_bio_tags):\n",
    "    self.aspect_bio_tags = aspect_bio_tags\n",
    "    self.aspect_unified_bio_tags = aspect_bio_tags\n",
    "\n",
    "  def set_aspect_unified_bio_tags(self, aspect_unified_bio_tags):\n",
    "    self.aspect_unified_bio_tags = aspect_unified_bio_tags\n",
    "    self.aspect_bio_tags = [k[0:1] for k in aspect_unified_bio_tags]\n",
    "\n",
    "  def check_rebuild_sentence_from_token(self):\n",
    "    return re.sub(r'\\s+', ' ',self.sentence.strip()) == self.rebuild_sentence_from_token().strip()\n",
    "  \n",
    "  def get_tokens(self):\n",
    "    return [self.sentence[k[0]:k[1]] for k in self.token_span]\n",
    "\n",
    "  def check_rebuild_aspect_terms(self, aspect_dict):\n",
    "    aspect_dict = sorted(aspect_dict, key=lambda d: int(d['from']))\n",
    "    aspect_input = ', '.join([k['term'] for k in aspect_dict])\n",
    "    aspect_computed = ''.join([(', ' if k == 'B' else '') + self.sentence[self.token_span[i][0]:self.token_span[i][1]] for i,k in enumerate(self.aspect_bio_tags) if k in ['B','I'] ])[2:]\n",
    "    \n",
    "    return (aspect_input == aspect_computed, aspect_input, aspect_computed)\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.rebuild_sentence_from_token()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape:  (3048, 3)\n",
      "df_val shape:  (800, 3)\n",
      "(3036, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_json('data/laptop/train.json')\n",
    "# df_train.set_index('id', inplace=True).reset_index()\n",
    "print('df_train shape: ', df_train.shape)\n",
    "\n",
    "df_val = pd.read_json('data/laptop/validate.json') # This will only be used for the very last step to evaluate how well the model is, but is input now for validating the BIO tagging to ensure the function works properly\n",
    "# df_val.set_index('id', inplace=True).reset_index()\n",
    "print('df_val shape: ', df_val.shape)\n",
    "\n",
    "# First, I will need to drop some duplicated data in our training dataset, as identified in the EDA process.\n",
    "df_train.drop_duplicates(subset='text', inplace=True)\n",
    "\n",
    "# We have removed 12 duplicated records in our training dataset\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sentence_token'] = df_train.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "df_train['sentence_check'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_sentence_from_token(), axis=1)\n",
    "df_train['aspect_check'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects']), axis=1)\n",
    "\n",
    "df_val['sentence_token'] = df_val.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "df_val['sentence_check'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_sentence_from_token(), axis=1)\n",
    "df_val['aspect_check'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_Data():\n",
    "  df_train = pd.read_json('data/laptop/train.json')\n",
    "  # df_train.set_index('id', inplace=True).reset_index()\n",
    "  print('df_train shape: ', df_train.shape)\n",
    "\n",
    "  df_val = pd.read_json('data/laptop/validate.json') # This will only be used for the very last step to evaluate how well the model is, but is input now for validating the BIO tagging to ensure the function works properly\n",
    "  # df_val.set_index('id', inplace=True).reset_index()\n",
    "  print('df_val shape: ', df_val.shape)\n",
    "\n",
    "  # First, I will need to drop some duplicated data in our training dataset, as identified in the EDA process.\n",
    "  df_train.drop_duplicates(subset='text', inplace=True)\n",
    "\n",
    "  # We have removed 12 duplicated records in our training dataset\n",
    "  print(df_train.shape)\n",
    "\n",
    "  df_train['sentence_token'] = df_train.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "  df_train['sentence_check'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_sentence_from_token(), axis=1)\n",
    "  df_train['aspect_check'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects']), axis=1)\n",
    "\n",
    "  df_val['sentence_token'] = df_val.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "  df_val['sentence_check'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_sentence_from_token(), axis=1)\n",
    "  df_val['aspect_check'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects']), axis=1)\n",
    "\n",
    "  return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of df_train records having tokenizing issues:  0\n",
      "# of df_train records having aspect bio tagging issues:  0\n",
      "# of df_val records having tokenizing issues:  0\n",
      "# of df_val records having  aspect bio tagging issues:  0\n"
     ]
    }
   ],
   "source": [
    "print('# of df_train records having tokenizing issues: ', len(df_train[df_train['sentence_check']==False]))\n",
    "print('# of df_train records having aspect bio tagging issues: ', len(df_train[df_train['aspect_check']==False]))\n",
    "print('# of df_val records having tokenizing issues: ', len(df_val[df_val['sentence_check']==False]))\n",
    "print('# of df_val records having  aspect bio tagging issues: ', len(df_val[df_val['aspect_check']==False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[EDA](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. <a id='toc2_1_'></a>[Sentence length](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sentence_length'] = df_train.apply(lambda x: len(x['sentence_token'].get_tokens()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.ecdf(df_train['sentence_length'], marginal=\"histogram\", title='90% reviews have <= 30 words')\n",
    "fig. update_layout(showlegend=False,\n",
    "                   xaxis_title=\"# of words in reviews\",\n",
    "                   yaxis_title=\"Review counts\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[CRF with FastText embeddings](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embeddings_to_dict (sentence_embeddings, window = 5):\n",
    "  sentence_features = []\n",
    "  for i in range(0, len(sentence_embeddings)):\n",
    "    word_features = {}\n",
    "    word_embeddings = sentence_embeddings[i]\n",
    "    word_features.update(word_embeddings_to_dict(word_embeddings,f'word'))\n",
    "    \n",
    "    if i > 0:\n",
    "      for k in range(1, min(window, i)+1):\n",
    "          # prev_word = tokens[i - k]\n",
    "          prev_word_embeddings = sentence_embeddings[i-k]\n",
    "          # print('prev_word: ', prev_word)\n",
    "          word_features.update(word_embeddings_to_dict(prev_word_embeddings,f'-{k}:word'))\n",
    "    else:\n",
    "        word_features['BOS'] = True  # Beginning of sentence\n",
    "\n",
    "    if i < len(sentence_embeddings) - 1:\n",
    "      for k in range(1, min(window, len(sentence_embeddings) - i - 1)+1):\n",
    "        # next_word = tokens[i + k]\n",
    "        next_word_embeddings = sentence_embeddings[i+k]\n",
    "        # print('next_word: ', next_word)\n",
    "        # print(next_word_embeddings)\n",
    "        word_features.update(word_embeddings_to_dict(next_word_embeddings,f'+{k}:word'))\n",
    "    else:\n",
    "        word_features['EOS'] = True  # End of sentence\n",
    "\n",
    "    sentence_features.append(word_features)\n",
    "    \n",
    "  return sentence_features\n",
    "\n",
    "def word_embeddings_to_dict(embeddings, feature_prefix='word'):\n",
    "  \n",
    "  \n",
    "  word_features = {}\n",
    "  for iv,value in enumerate(embeddings):\n",
    "    word_features[f'{feature_prefix}:v_{iv}'] = value\n",
    "  \n",
    "  return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.load_model('cc.en.300.bin')\n",
    "X_train_word_embeddings = [[fasttext_model.get_word_vector(token) for token in sentence_token.get_tokens()] for sentence_token in df_train['sentence_token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dict = [sentence_embeddings_to_dict(sentence_embeddings, 5) for sentence_embeddings in X_train_word_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['sentence_token'].apply(lambda x: x.aspect_unified_bio_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_crf, X_test_crf, y_train, y_test = train_test_split(X_train_dict, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train CRF model\n",
    "crf_model = CRF(algorithm='lbfgs',\n",
    "                max_iterations=100,\n",
    "                c1=0.5,\n",
    "                c2=0.05)\n",
    "\n",
    "# There is this error existing with this library: 'CRF' object has no attribute 'keep_tempfiles'\n",
    "# which has not been resolved and we can bypass it using this trick.\n",
    "try:\n",
    "  crf_model.fit(X_train_crf, y_train)\n",
    "except AttributeError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_crf_pred = crf_model.predict(X_train_crf)\n",
    "\n",
    "y_train_flat = [tag for sentence in y_train for tag in sentence]\n",
    "y_train_crf_pred_flat = [tag for sentence in y_train_crf_pred for tag in sentence]\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_train_flat, y_train_crf_pred_flat)\n",
    "classification_rep = classification_report(y_train_flat, y_train_crf_pred_flat)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_train_flat, y_train_crf_pred_flat, labels=crf_model.classes_)\n",
    "\n",
    "# Plot the confusion matrix with seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=crf_model.classes_, yticklabels=crf_model.classes_, vmax=100, vmin=1)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_crf_pred = crf_model.predict(X_test_crf)\n",
    "\n",
    "y_test_flat = [tag for sentence in y_test for tag in sentence]\n",
    "y_test_crf_pred_flat = [tag for sentence in y_test_crf_pred for tag in sentence]\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_flat, y_test_crf_pred_flat)\n",
    "classification_rep = classification_report(y_test_flat, y_test_crf_pred_flat)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_flat, y_test_crf_pred_flat, labels=crf_model.classes_)\n",
    "\n",
    "# Plot the confusion matrix with seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=crf_model.classes_, yticklabels=crf_model.classes_, vmax=100, vmin=1)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_words = set(token  for sentence_token in df_train['sentence_token'] for token in sentence_token.get_tokens() if token not in fasttext_model.words)\n",
    "len(missing_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[Bi-LSTM](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.datasets import imdb\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import keras\n",
    "# # from keras_contrib.layers import keras_CRF\n",
    "# # fix random seed for reproducibility\n",
    "# tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape:  (3048, 3)\n",
      "df_val shape:  (800, 3)\n",
      "(3036, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val = reload_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_words = df_train['sentence_token'].apply(lambda x: x.get_tokens())\n",
    "\n",
    "# Create a tokenizer\n",
    "word_tokenizer = Tokenizer(char_level=False, lower=True)\n",
    "# Fit on your list of words\n",
    "word_tokenizer.fit_on_texts(X_words)\n",
    "\n",
    "word2idx = word_tokenizer.word_index\n",
    "word2idx['ENDPAD'] = 0\n",
    "\n",
    "idx2word = word_tokenizer.index_word\n",
    "idx2word[0] = 'ENDPAD'\n",
    "\n",
    "n_words = len(word2idx.keys())\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'O': 1,\n",
       " 'B-POS': 2,\n",
       " 'B-NEG': 3,\n",
       " 'I-NEG': 4,\n",
       " 'B-NEU': 5,\n",
       " 'I-POS': 6,\n",
       " 'I-NEU': 7,\n",
       " 'B-CON': 8,\n",
       " 'I-CON': 9,\n",
       " 'UNK': 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_words = df_train['sentence_token'].apply(lambda x: x.aspect_unified_bio_tags)\n",
    "# Create a tokenizer\n",
    "tag_tokenizer = Tokenizer(char_level=False, lower=False)\n",
    "# Fit on your list of words\n",
    "tag_tokenizer.fit_on_texts(y_words)\n",
    "\n",
    "\n",
    "tag2idx = tag_tokenizer.word_index\n",
    "tag2idx['UNK'] = 0\n",
    "idx2tag = tag_tokenizer.index_word\n",
    "idx2tag[0] = 'UNK'\n",
    "\n",
    "n_tags = len(tag2idx.keys())\n",
    "print(n_tags)\n",
    "\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "X = word_tokenizer.texts_to_sequences(X_words)\n",
    "X = sequence.pad_sequences(maxlen=maxlen, sequences=X, padding=\"post\",value=n_words - 1)\n",
    "\n",
    "y = tag_tokenizer.texts_to_sequences(y_words)\n",
    "y = sequence.pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=n_tags - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "# y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "# y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tf2CRF import CRF\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, GRU, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tf2crf import CRF, ModelWithCRFLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 22:49:24.879836: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond/output/_106'\n",
      "2024-01-03 22:49:25.346404: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:933] Skipping loop optimization for Merge node with control input: cond_1/branch_executed/_135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214/1214 [==============================] - 21s 12ms/step - loss: 5.3956 - accuracy: 0.9562\n",
      "Epoch 2/2\n",
      "1214/1214 [==============================] - 15s 12ms/step - loss: 2.0928 - accuracy: 0.9751\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "INFO:tensorflow:Assets written to: tests/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tests/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(maxlen,), dtype='int32')\n",
    "output = Embedding(n_words, 40, trainable=True, mask_zero=True)(inputs)\n",
    "output = Bidirectional(GRU(64, return_sequences=True))(output)\n",
    "crf = CRF(units=n_tags)\n",
    "output = crf(output)\n",
    "base_model = Model(inputs, output)\n",
    "model = ModelWithCRFLoss(base_model, sparse_target=True)\n",
    "model.compile(optimizer='adam')\n",
    "\n",
    "# x = [[5, 2, 3] * 3] * 10\n",
    "# y = [[1, 2, 3] * 3] * 10\n",
    "\n",
    "model.fit(x=X_train, y=y_train, epochs=2, batch_size=2)\n",
    "model.save('tests/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(maxlen,))\n\u001b[1;32m      2\u001b[0m word_embedding_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mEmbedding(input_dim\u001b[38;5;241m=\u001b[39mn_words, output_dim\u001b[38;5;241m=\u001b[39mword_embedding_size, input_length\u001b[38;5;241m=\u001b[39mmaxlen)(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "input = layers.Input(shape=(maxlen,))\n",
    "word_embedding_size = 300\n",
    "model = layers.Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=maxlen)(input)\n",
    "model = layers.Bidirectional(layers.LSTM(units=word_embedding_size, \n",
    "                           return_sequences=True, \n",
    "                           dropout=0.5, \n",
    "                           recurrent_dropout=0.5, \n",
    "                           kernel_initializer=keras.initializers.he_normal()))(model)\n",
    "model = layers.LSTM(units=word_embedding_size * 2, \n",
    "             return_sequences=True, \n",
    "             dropout=0.5, \n",
    "             recurrent_dropout=0.5, \n",
    "             kernel_initializer=keras.initializers.he_normal())(model)\n",
    "model = layers.TimeDistributed(layers.Dense(n_tags, activation=\"relu\"))(model)  # previously softmax output layer\n",
    "\n",
    "crf = keras_CRF(n_tags)  # CRF layer\n",
    "out = crf(model)  # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = len(all_words) + 1 #Important adjustment\n",
    "number_of_tags = len(all_tags) + 1\n",
    "\n",
    "print(number_of_classes)\n",
    "RNN_wordlevel = Sequential([\n",
    "    layers.InputLayer(input_shape=( max_review_length,300,)),\n",
    "    # # embedding layer, 8-dimensional\n",
    "    # layers.Embedding(number_of_classes, 8),\n",
    "    layers.Bidirectional(layers.LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.15),\n",
    "    # # the intermediate recurrent layers should return full sequences\n",
    "    # layers.GRU(64, activation='relu', return_sequences=True),\n",
    "    # layers.BatchNormalization(),\n",
    "    # layers.Dropout(0.15),\n",
    "\n",
    "    # # the last recurrent layer only returns the final output\n",
    "    # layers.GRU(32, activation='relu', return_sequences=False),\n",
    "    # layers.BatchNormalization(),\n",
    "    # layers.Dropout(0.15),\n",
    "\n",
    "    # output layer\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.15),\n",
    "    layers.Dense(number_of_tags, activation='softmax')], name=\"RNN_wordlevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "RNN_wordlevel.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = RNN_wordlevel.fit(X_train, y_train,\n",
    "        batch_size=1024,\n",
    "        epochs=25,\n",
    "        validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 26)\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history[\"accuracy\"], label=\"training\", marker=\"o\")\n",
    "plt.plot(epochs, history.history[\"val_accuracy\"], label=\"validation\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs[::5])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history[\"loss\"], label=\"training\", marker=\"o\")\n",
    "plt.plot(epochs, history.history[\"val_loss\"], label=\"validation\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs[::5])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluate on test data\")\n",
    "# results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "# print(\"test loss: {} \".format(results[0]))\n",
    "# print(\"test accuracy: {} \".format(results[1]))\n",
    "\n",
    "i = np.random.randint(0, X_test.shape[0])\n",
    "print(\"This is sentence:\",i)\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "number_to_word[0] = ''\n",
    "number_to_tag[0] = ''\n",
    "\n",
    "print(\"{:15}{:5}\\t {}\\n\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(\"-\" *30)\n",
    "for w, true, pred in zip(X_test_words.iloc[i], y_test[i], p[0]):\n",
    "    print(\"{:15}{}\\t{}\".format(w, number_to_tag[true], number_to_tag[pred]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_raw = RNN_wordlevel.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred_raw, axis=-1)\n",
    "y_test_flat = [tag for sentence in y_test for tag in sentence]\n",
    "y_test_pred_flat = [tag for sentence in y_test_pred for tag in sentence]\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_flat, y_test_pred_flat)\n",
    "classification_rep = classification_report(y_test_flat, y_test_pred_flat)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_flat, y_test_pred_flat)\n",
    "conf_matrix\n",
    "\n",
    "# Plot the confusion matrix with seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['None'] + list(tag_to_number.keys()), yticklabels=['None']+list(tag_to_number.keys()), vmax=100, vmin=1)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tag_to_number.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[RNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "X_train_words = [sentence_token.get_tokens() for sentence_token in df_train['sentence_token']]\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(char_level=False, lower=True)\n",
    "# Fit on your list of words\n",
    "tokenizer.fit_on_texts(X_train_words)\n",
    "\n",
    "word_to_number = tokenizer.word_index\n",
    "number_to_word = tokenizer.index_word\n",
    "\n",
    "all_words = list(word_to_number.keys())\n",
    "\n",
    "print(f\"Vocabulary size: {len(all_words)}\")\n",
    "\n",
    "dataset = tokenizer.texts_to_sequences(X_train_words)\n",
    "\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = pd.DataFrame(data=tokenizer.word_counts.items(), columns=[\"word\", \"count\"])\n",
    "word_frequency = word_frequency.sort_values(\"count\", ascending=False)[:25]\n",
    "word_frequency.set_index(\"word\").plot(kind=\"bar\", rot=90, title=\"Word count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_words = df_train['sentence_token'].apply(lambda x: x.aspect_unified_bio_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a tokenizer\n",
    "y_tokenizer = Tokenizer(char_level=False, lower=False)\n",
    "# Fit on your list of words\n",
    "y_tokenizer.fit_on_texts(y_train_words)\n",
    "\n",
    "\n",
    "tag_to_number = y_tokenizer.word_index\n",
    "number_to_tag = y_tokenizer.index_word\n",
    "\n",
    "all_tags = list(tag_to_number.keys())\n",
    "\n",
    "print(f\"Tags size: {len(all_tags)}\")\n",
    "\n",
    "y_train = y_tokenizer.texts_to_sequences(y_train_words)\n",
    "\n",
    "\n",
    "word_frequency = pd.DataFrame(data=y_tokenizer.word_counts.items(), columns=[\"word\", \"count\"])\n",
    "word_frequency = word_frequency.sort_values(\"count\", ascending=False)[:25]\n",
    "word_frequency.set_index(\"word\").plot(kind=\"bar\", rot=90, title=\"Word count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[Simple RNN?!?! From study material](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### THIS NEEDS REVISE LOGIC - CURRENTLY IS JUST TO SAMPLE ########\n",
    "X = []\n",
    "y = []\n",
    "SEQUENCE_LENGTH = 5\n",
    "for nth_sentence, sentence in enumerate(dataset):\n",
    "    for window_start_idx in range(len(sentence)-SEQUENCE_LENGTH):\n",
    "        window_end_idx = window_start_idx + SEQUENCE_LENGTH\n",
    "        X.append(sentence[window_start_idx: window_end_idx])\n",
    "        y.append(y_train[nth_sentence][window_start_idx])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Let's look at the shapes\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(\"X:\", [number_to_word[num] for num in X[i]])\n",
    "    print(\"y:\", number_to_tag[y[i]])\n",
    "    print(\"*******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and validation set\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_validation.shape, y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GRU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = len(all_words) + 1 #Important adjustment\n",
    "number_of_tags = len(all_tags) + 1\n",
    "print(number_of_classes)\n",
    "RNN_wordlevel = Sequential([\n",
    "\n",
    "    # embedding layer, 8-dimensional\n",
    "    Embedding(number_of_classes, 8),\n",
    "\n",
    "    # the intermediate recurrent layers should return full sequences\n",
    "    GRU(64, activation='relu', return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "\n",
    "    # the last recurrent layer only returns the final output\n",
    "    GRU(32, activation='relu', return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "\n",
    "    # output layer\n",
    "    Dense(16, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "    Dense(number_of_tags, activation='softmax')], name=\"RNN_wordlevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "RNN_wordlevel.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = RNN_wordlevel.fit(X_train, y_train,\n",
    "        batch_size=1024,\n",
    "        epochs=25,\n",
    "        validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 26)\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history.history[\"accuracy\"], label=\"training\", marker=\"o\")\n",
    "plt.plot(epochs, history.history[\"val_accuracy\"], label=\"validation\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs[::5])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history.history[\"loss\"], label=\"training\", marker=\"o\")\n",
    "plt.plot(epochs, history.history[\"val_loss\"], label=\"validation\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs[::5])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = RNN_wordlevel.evaluate(X_train,  y_train, verbose=2)\n",
    "val_loss, val_acc = RNN_wordlevel.evaluate(X_validation,  y_validation, verbose=2)\n",
    "print('\\nTrain accuracy:', train_acc)\n",
    "print('\\nVal accuracy:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_phrase = ['I', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'cord', 'with', 'me', 'because', 'of', 'the'] # good battery life.'\n",
    "\n",
    "# process for the model\n",
    "processed_phrase = tokenizer.texts_to_sequences([input_phrase])[0]\n",
    "\n",
    "\n",
    "# extract last 5 words\n",
    "network_input = np.array(processed_phrase[-SEQUENCE_LENGTH:], dtype=np.float32)\n",
    "network_input = network_input.reshape((1, SEQUENCE_LENGTH)) # shape: 1 x 5\n",
    "\n",
    "# the RNN gives the probability of each word as the next one\n",
    "predict_proba = RNN_wordlevel.predict(network_input, verbose=0)[0] # shape (4855,)\n",
    "predict_label = number_to_tag[np.argmax(predict_proba)]\n",
    "# # sample one word using these chances\n",
    "# predicted_index = np.random.choice(number_of_classes, 1, p=predict_proba)[0]\n",
    "\n",
    "# # add new index at the end of our list\n",
    "# processed_phrase.append(predicted_index)\n",
    "\n",
    "# # progress indicator\n",
    "# print(i, end=\"\\r\")\n",
    "\n",
    "# indices mapped to words - the method expects a list of lists so we need the extra bracket\n",
    "output_phrase = tokenizer.sequences_to_texts([processed_phrase])[0]\n",
    "\n",
    "print(output_phrase)\n",
    "print(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_image(i, predictions_array, true_label, img):\n",
    "#   # true_label, img = true_label[i], img[i]\n",
    "#   # plt.grid(False)\n",
    "#   # plt.xticks([])\n",
    "#   # plt.yticks([])\n",
    "\n",
    "#   # plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "#   predicted_label = np.argmax(predictions_array)\n",
    "#   if predicted_label == true_label:\n",
    "#     color = 'blue'\n",
    "#   else:\n",
    "#     color = 'red'\n",
    "\n",
    "#   plt.xlabel(\"{} {:2.0f}% ({})\".format(number_to_tag[predicted_label],\n",
    "#                                 100*np.max(predictions_array),\n",
    "#                                 number_to_tag[true_label]),\n",
    "#                                 color=color)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test data\n",
    "score = RNN_wordlevel.evaluate(X_validation, y_validation, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentilens_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
