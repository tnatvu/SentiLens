{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size: 3em'>SentiLens - Uncover reviews' hidden emotion</div>\n",
    "\n",
    "__Prepared by:__ Tina Vu</br>\n",
    "__Date:__ 20231208</br>\n",
    "\n",
    "Employing aspect-based sentiment analysis (ABSA) to extract valuable feature insights from e-commerce product reviews, thereby empowering consumers to make more informed purchasing decisions and enhancing their overall user experience on the platform.\n",
    "\n",
    "Utilizing manually annotated reviews for aspect sentiment analysis to extract aspects and predict sentiments from reviews. This enables consumers to obtain a condensed overview of sentiments related to various product features, eliminating the need to delve into an extensive array of reviews. As a result, the decision-making process becomes more streamlined and user-friendly.\n",
    "\n",
    "__Phase:__\n",
    "1. Supervised ABSA (What, How): Aspect term extration and aspect term sentiment classification\n",
    "2. Unsupervised ABSA\n",
    "3. Add 'Why' into ABSA\n",
    "\n",
    "This notebook/ project may only aim to complete the first phase.\n",
    "\n",
    "<div style='font-size: 2em'>Phase 1 - Aspect Extration</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Import & prepare dataset](#toc1_)    \n",
    "  - 1.1. [Import data](#toc1_1_)    \n",
    "  - 1.2. [Preparing dataset for modelling](#toc1_2_)    \n",
    "    - 1.2.1. [BIO tagging encode](#toc1_2_1_)    \n",
    "    - 1.2.2. [Word features](#toc1_2_2_)    \n",
    "    - 1.2.3. [Define data processing functions](#toc1_2_3_)    \n",
    "    - 1.2.4. [Complete data preparation](#toc1_2_4_)    \n",
    "- 2. [Exploratory data analysis](#toc2_)    \n",
    "  - 2.1. [Flatten word features](#toc2_1_)    \n",
    "  - 2.2. [Aspect word distribution](#toc2_2_)    \n",
    "  - 2.3. [Part-of-speech](#toc2_3_)    \n",
    "  - 2.4. [Opinion Lexicon - Sentiment](#toc2_4_)    \n",
    "- 3. [Split data](#toc3_)    \n",
    "- 4. [Random forest](#toc4_)    \n",
    "  - 4.1. [Data prepration](#toc4_1_)    \n",
    "    - 4.1.1. [Flattening](#toc4_1_1_)    \n",
    "    - 4.1.2. [Oversampling](#toc4_1_2_)    \n",
    "    - 4.1.3. [Scaling](#toc4_1_3_)    \n",
    "  - 4.2. [Model](#toc4_2_)    \n",
    "  - 4.3. [Validation](#toc4_3_)    \n",
    "- 5. [CRF model](#toc5_)    \n",
    "  - 5.1. [Model](#toc5_1_)    \n",
    "- 6. [Model evaluation](#toc6_)    \n",
    "- 7. [Tuning](#toc7_)    \n",
    "  - 7.1. [Sample visualization](#toc7_1_)    \n",
    "- 8. [New data](#toc8_)    \n",
    "- 9. [Next steps](#toc9_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, opinion_lexicon\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('opinion_lexicon')\n",
    "\n",
    "# Load positive and negative words from the opinion lexicon\n",
    "POSITIVE_WORDS = set(opinion_lexicon.positive())\n",
    "NEGATIVE_WORDS = set(opinion_lexicon.negative())\n",
    "EN_STOP_WORDS = set(stopwords.words('english'))\n",
    "CONTEXT_SWITCHING_WORDS = ['but', 'yet', 'however', 'nevertheless', 'still', 'nonetheless', 'although', 'though', 'even though', 'while']\n",
    "# nltk.download()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Import & prepare dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Import data](#toc0_)\n",
    "\n",
    "We will load laptop reviews dataset with aspect term & sentiment annotations.\n",
    "\n",
    "The dataset comes in two parts:\n",
    "- train: 3,048 records\n",
    "- test: 800 records\n",
    "\n",
    "Each record is a sentence with zero, one or multiple aspect terms. Each aspect term has the following features:\n",
    "- start character index\n",
    "- end character index\n",
    "- sentiment/ polarity (positive or negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json('data/laptop/train.json')\n",
    "df_val = pd.read_json('data/laptop/validate.json') # This will only be used for the very last step to evaluate how well the model is, but is input now for validating the BIO tagging to ensure the function works properly\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Preparing dataset for modelling](#toc0_)\n",
    "\n",
    "The task we are solving is Named Entity Recognition (NER) which is a sequential labeling task, a.k.a we would like to predict whether a token (word) in each sentence is part of an aspect term or not.\n",
    "\n",
    "In order to prepare the data for NER task, we need to label our tokens. Here, I implemented a unified BIO tagging technique which combines aspect boundaries and aspect sentiment.\n",
    "\n",
    "Word boundaries:\n",
    "- B: indicates the 1st word in the aspect term\n",
    "- I: indicates the subsequent word in the aspect term\n",
    "- O: indicates words that are not part of any aspect term\n",
    "\n",
    "Aspect sentiment:\n",
    "- POS: positive\n",
    "- NEU: neutral\n",
    "- NEG: conflict\n",
    "\n",
    "This BIO label technique is more effective in recognizing unigram and n-gram aspect terms comparing to a binary classification (whether a token is part of an aspect). By using a unified a approach, we can combine two tasks: aspect extraction and sentiment classification into one task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I will need to drop some duplicated data in our training dataset, as identified in the EDA process.\n",
    "df_train.drop_duplicates(subset='text', inplace=True)\n",
    "\n",
    "# We have removed 12 duplicated records in our training dataset\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. <a id='toc1_2_1_'></a>[BIO tagging encode](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I defined a function to encode our sentences' aspects using a unified BIO tagging technique (<a href='https://arxiv.org/pdf/1811.05082.pdf'>reference</a>) that combines aspect boundaries and aspect sentiment in a single label.\n",
    "\n",
    "Word boundaries:\n",
    "- B: indicates the 1st word in the aspect term\n",
    "- I: indicates the subsequent word in the aspect term\n",
    "- O: indicates words that are not part of any aspect term\n",
    "\n",
    "Aspect sentiment:\n",
    "- POS: positive\n",
    "- NEU: neutral\n",
    "- NEG: conflict\n",
    "\n",
    "Unified BIO tagging will be like: B-NEU, I-NEU\n",
    "\n",
    "For example:\n",
    "['I', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'cord', 'with', 'me', 'because', 'of', 'the', 'good', 'battery', 'life', '.']\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEU', 'I-NEU', 'O', 'O', 'O', 'O', 'O', 'B-POS', 'I-POS', 'I-POS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset provides a full sentence and annotates aspect term using character index, we cannot perform word_tokenize directly on the raw text due to:\n",
    "  1. Word_tokenize separates punctuation as a single normal token which makes it difficult to re-string (combine) word tokens (using ' '.join(tokens)) for character index, as it adds extra spaces between word and punctuations, thus invalidate character index for aspect terms. This makes it very difficult to align the aspect term character index and word index accurately.</br>\n",
    "  E.g. \"I love pizza, cheese.\", term indexes are (7,12),(16,22), re-string tokens from word_tokenize (' '.join(tokens)) can turn the sentence into \"I love pizza__\\<extra space\\>__, cheese__\\<extra space\\>__.\". The char index of the aspect terms now become (7,12), `(17,23)`.\n",
    "  \n",
    "  2. Word_tokenize tends to not separate words that have special characters between them (other than space and common punctuations like <,.:;>), while some terms treated those chunks as separated terms. \n",
    "  E.g. \"size/screen\" is a single token based on word_tokenize, while terms defined this as two separate tokens. \n",
    "\n",
    "Therefore, I applied the below approach to acoomodate the above short commings:\n",
    "  1. Add aspect_prefix & aspect_suffix (with additional spaces in order to overcome issue #2) to the start & end of each aspect term (to overcome issue #1) in the sentence using from, to char index as supplied by the dataset\n",
    "  2. Perform word_tokenize on the new aspect_annotated_sentence\n",
    "  3. Perform BIO tagging on the sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_unified_BIO (x, sentiment_tag=False):\n",
    "  '''  This function puts aspect's details into a dictionary, and multiple aspect as an array\n",
    "  \n",
    "  Parameter:\n",
    "  - ASPECTS: dictionary array\n",
    "    dictionary of\n",
    "    - term\n",
    "    - polarity\n",
    "    - term_start\n",
    "    - term_end\n",
    "\n",
    "    For example:\n",
    "    [\n",
    "      {'term':'cord', 'polarity':'neutral', 'from': 41, 'to': 45},\n",
    "      {'term':'battery life', 'polarity':'positive', 'from': 74, 'to': 86}\n",
    "    ]\n",
    "  - SENTIMENT_TAG: boolean\n",
    "    True: if we want to return sentiment polarity with BIO tagging (unified BIO)\n",
    "    False: if we do not want to return sentiment polarity with BIO tagging (just pure BIO)\n",
    "    \n",
    "  Output: \n",
    "  - TEXT_TOKENS: array of string\n",
    "    Sentence tokens\n",
    "    E.g. ['Boot', 'time', 'is', 'super', 'fast', ',', 'around', 'anywhere', 'from', '35', 'seconds', 'to', '1', 'minute', '.']\n",
    "    \n",
    "  - TAGS: array of string\n",
    "    Unified BIO tags (if sentiment_tag paratement is set to True) or BIO tags (if sentiment_tag is set to False)\n",
    "      Aspect boundaries:\n",
    "        B: beginning of aspect term\n",
    "        I: subsequent words of aspect term\n",
    "        O: outside of aspect term\n",
    "      Sentiment:\n",
    "        POS: positive\n",
    "        NEG: negative\n",
    "        NEU: neutral\n",
    "        CON: conflict\n",
    "    E.g. ['B-POS', 'I-POS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "  - ASPECT_COMPUTE_PAIRS: tuple of two strings\n",
    "    1st String combining of all aspect terms as provided by the dataset\n",
    "    2nd String combining of all aspect terms from annotated BIO tagged using text_token\n",
    "    E.g.(' Boot time', ' Boot time')\n",
    "\n",
    "  - IS_INCORRECT_TAGGING: boolean\n",
    "    True: when ASPECT_COMPUTE_PAIRS is different from each other\n",
    "    False: when ASPECT_COMPUTE_PAIRS is exactly like each other\n",
    "    This may return False, when there is some special characters in aspect term that causing them to be splitted into a different token.\n",
    "    E.g. aspect_term `15\" TV`, computed_term can return `15 \" TV`. Thus, they are not exactly matching each other, but they should be okay\n",
    "  '''\n",
    "\n",
    "  aspects = x['aspects']\n",
    "  aspects = sorted(aspects, key=lambda d: int(d['from']))  # sort aspects based on from, some aspects are not sorted: later terms in the sentence sometimes are placed before terms appear earlier. \n",
    "  \n",
    "  text = x['text']\n",
    "\n",
    "  sentiment_tag_map = {'neutral': '-NEU'\n",
    "                       ,'conflict':'-CON'\n",
    "                       ,'positive':'-POS'\n",
    "                       ,'negative':'-NEG'}\n",
    "  \n",
    "  aspect_prefix = ' XXATBXX' # add leading space to break words if they are in the same chunk. E.g. \"size/window\" --> \"size / window\"\n",
    "  aspect_suffix = 'XXATEXX ' # add trailing space\n",
    "\n",
    "  # these are for validation to ensure the BIO tagging is accurate\n",
    "  aspect_terms = ''\n",
    "  aspect_terms_compute = ''\n",
    "\n",
    "  # we cannot perform word_tokenize directly on the raw text due to:\n",
    "  # 1. there is no space between punctuation and word, which makes it difficult to calculate word index from char index when concatenating word tokens for word index search\n",
    "  # 2. terms can be partial of a word token, e.g. \"size/screen\" is a single token based on word_token, while terms defined this as two separate terms. \n",
    "  # Therefore, I applied the below approach:\n",
    "  # 1. Add aspect_prefix & aspect_suffix to the start & end of each aspect term in the sentence using from, to char index as supplied by the dataset\n",
    "  # 2. Perform word_tokenize on the new aspect_annotated_sentence\n",
    "  # 3. Perform BIO tagging on the sentence token\n",
    "\n",
    "  # 1. Add aspect prefix & suffix to the sentence\n",
    "  aspect_annotated_sentence = text\n",
    "\n",
    "  for i, k in enumerate(aspects):\n",
    "    term = k['term']\n",
    "    \n",
    "    if k['term'] != '': # there are empty aspects but still have an empty dict structure, so we only perform tagging for those that has `term` != ''\n",
    "      # this is for validation purposes only\n",
    "      aspect_terms += ' ' + term \n",
    "\n",
    "      if k['polarity'] == '':\n",
    "        print(k['id'])\n",
    "\n",
    "      polarity = sentiment_tag_map[k['polarity']] if sentiment_tag == True else '' # get polarity encode\n",
    "  \n",
    "      i_from = int(k['from']) + i * (len(aspect_prefix) + len(aspect_suffix) + len(polarity)) # re-calculate from & by shifting them by the length of additional character added for aspect prefix & suffix\n",
    "      i_to = int(k['to']) + i * (len(aspect_prefix) + len(aspect_suffix) + len(polarity))\n",
    "      \n",
    "      aspect_annotated_sentence = aspect_annotated_sentence[:i_from] + aspect_prefix+ polarity + aspect_annotated_sentence[i_from:i_to] + aspect_suffix + aspect_annotated_sentence[i_to:]\n",
    "  \n",
    "  # Tokenize aspect annotated sentence\n",
    "  text_tokens = word_tokenize(aspect_annotated_sentence)\n",
    "\n",
    "  # Perfom BIO tagging\n",
    "  aspect_start = False\n",
    "  polarity = ''\n",
    "  token_BIO_pairs = []\n",
    "\n",
    "  for i,k in enumerate(text_tokens):\n",
    "    tag = 'O' # default token tag as 'O' outside of aspect term\n",
    "\n",
    "    if k[:7] == aspect_prefix.strip(): # if we see aspect prefix in a term, update tag as 'B' & set aspect_start as True\n",
    "      aspect_start = True\n",
    "      polarity = k[7:11] # extract polarity for next following tokens if there is\n",
    "      tag = 'B' + polarity\n",
    "      \n",
    "    elif aspect_start == True: # if token does not have aspect_prefx, set tag to I if aspect_start is still True\n",
    "      tag = 'I' + polarity\n",
    "      \n",
    "    else: # if aspect_start is False or if there is no aspect prefix\n",
    "      tag = 'O'\n",
    "\n",
    "    text_tokens[i] = re.sub(aspect_prefix.strip() + '.{4}', '',text_tokens[i]).replace(aspect_suffix.strip(),'') # clean text token by removing aspect prefix, polarity & aspect suffix if any\n",
    "\n",
    "    token_BIO_pairs.append((text_tokens[i], tag))\n",
    "    \n",
    "    if k[-7:] == aspect_suffix.strip(): # If token contains aspect_suffix, restart aspect_start as False and polarity as empty\n",
    "      aspect_start = False\n",
    "      polarity = ''\n",
    "    \n",
    "  # This is for validation purposes only\n",
    "  for i, k in enumerate(token_BIO_pairs):\n",
    "    if k[1] != 'O':\n",
    "      aspect_terms_compute += ' ' + k[0]\n",
    "  \n",
    "  # This is for validation purposes only\n",
    "  aspect_compute_pairs = (aspect_terms, aspect_terms_compute)\n",
    "  is_incorrect_tagging = True if aspect_terms != aspect_terms_compute  else False\n",
    "\n",
    "  # Cleaning punctuation or special characters\n",
    "  token_BIO_pairs_clean = list()\n",
    "  shift_b = False\n",
    "  for i,k in enumerate(token_BIO_pairs):\n",
    "    if (k[0].lower() in string.punctuation + '\\'\\'') == False:\n",
    "      BIO_tag = k[1]\n",
    "      if (shift_b == True) & (k[1][:1] == 'I'):\n",
    "        BIO_tag = 'B' + k[1][1:]\n",
    "        shift_b = False\n",
    "      token_BIO_pairs_clean.append((k[0], BIO_tag, i)) # i is the original token idx, this will help remap the predicted tag to the orginal sentence for highlighting in viz, or application\n",
    "    \n",
    "    elif k[1][:1] == 'B':\n",
    "      shift_b = True\n",
    "\n",
    "\n",
    "  return pd.Series([token_BIO_pairs, token_BIO_pairs_clean, aspect_compute_pairs, is_incorrect_tagging])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['token_BIO_pairs','token_BIO_pairs_clean', 'aspect_compute_pairs','is_incorrect_tagging']] = df_train.apply(lambda x: encode_unified_BIO(x, True), axis=1)\n",
    "df_val[['token_BIO_pairs','token_BIO_pairs_clean', 'aspect_compute_pairs','is_incorrect_tagging']] = df_val.apply(lambda x: encode_unified_BIO(x, True), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing BIO encode, we want to check if there is any misclassification using is_incorrect_tagging.\n",
    "\n",
    "There are only 20 possible inccorect BIO labels in df_train and 10 for df_val.\n",
    "\n",
    "Looking through a few examples, it seems like the issues are mainly around special characters inside aspect terms that cause the comparision strings to be different, however, the BIO labels are still accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# of possible incorrect labels in df_train: ', df_train['is_incorrect_tagging'].sum())\n",
    "print('# of possible incorrect labels in df_train: ', df_val['is_incorrect_tagging'].sum())\n",
    "print('\\n\\n Some examples')\n",
    "\n",
    "n_samples = 2\n",
    "sample = df_train[df_train['is_incorrect_tagging']].copy().reset_index()\n",
    "\n",
    "for i in range(0, n_samples):\n",
    "  s = sample.iloc[i]\n",
    "  print(s['text'])\n",
    "  print(s['aspects'])\n",
    "  print(s['aspect_compute_pairs'])\n",
    "  print(s['token_BIO_pairs'])\n",
    "  print(s['token_BIO_pairs_clean'])\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. <a id='toc1_2_2_'></a>[Word features](#toc0_)\n",
    "Here we will populate some word features for each token (word) in the sentence, such as:\n",
    "- word\n",
    "- stemming / lemming versions of word\n",
    "- part of speech (POS) of word\n",
    "- words sentiment lexicon\n",
    "- context words within a pre-defined window (5 words surrounding the token)\n",
    "- context words stemming/ lemming\n",
    "- context words POS\n",
    "- context words sentiment lexicon\n",
    "- ...\n",
    "\n",
    "The list is not exhaustive, and is an iterative process as we perform EDA and go back and refining/ adding more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sentences into features\n",
    "def word2features(sent, i, window_size=5): \n",
    "    word = sent[i][0]\n",
    "\n",
    "    _, pos = zip(*nltk.pos_tag([x[0] for x in sent]))\n",
    "\n",
    "    window_size = int((window_size - 1)/ 2 if (window_size % 2) == 1 else window_size / 2)\n",
    "    tag_sentiment = lambda word: 'POS' if word in POSITIVE_WORDS else 'NEG' if word in NEGATIVE_WORDS else 'NEU'\n",
    "    features = {\n",
    "        'word.lower()': word.lower(), # word\n",
    "        'word.index()': i,\n",
    "        'word.reverseindex()': len(sent) - 1 - i, # reverse index - nth word from end of sentence\n",
    "        'word.pos': pos[i],\n",
    "        'word.opinionlexicon': tag_sentiment(word.lower()),\n",
    "        'word.contextswitching': word.lower() in CONTEXT_SWITCHING_WORDS,\n",
    "        'word.isstopword()': word.lower() in EN_STOP_WORDS,\n",
    "        'word[-3:]': word[-3:], # last 4 char\n",
    "        'word[-2:]': word[-2:], # last 3 char - in case of -ing, -ion, etc.\n",
    "        'word.isupper()': word.isupper(), # is the word in upper case\n",
    "        'word.istitle()': word.istitle(), # is the first letter of the word in upper case\n",
    "        'word.isdigit()': word.isdigit(), # is the word full of digit\n",
    "        # 'word.isspecialchar()': re.sub('[^\\w,\\d,\\s]', '', word.lower()) == '', # is punctuation/ special characters\n",
    "    }\n",
    "    if i > 0:\n",
    "        for k in range(1, min(window_size, i)+1):\n",
    "            prev_word = sent[i - k][0]\n",
    "            prev_pos = pos[i - k]\n",
    "            \n",
    "            features.update({\n",
    "                f'-{k}:word.lower()': prev_word.lower(),\n",
    "                f'-{k}:word.pos': prev_pos,\n",
    "                f'-{k}:word.opinionlexicon': tag_sentiment(word.lower()),\n",
    "                f'-{k}:word.contextswitching': prev_word.lower() in CONTEXT_SWITCHING_WORDS,\n",
    "                f'-{k}:word.isstopword()': prev_word in EN_STOP_WORDS,\n",
    "                f'-{k}:word.istitle()': prev_word.istitle(),\n",
    "                f'-{k}:word.isupper()': prev_word.isupper(),\n",
    "                # f'-{k}:word.isspecialchar()': re.sub('[^\\w,\\d,\\s]', '', prev_word.lower()) == '', # is punctuation/ special characters\n",
    "            })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sentence\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        for k in range(1, min(window_size, len(sent) - i - 1)+1):\n",
    "            next_word = sent[i + k][0]\n",
    "            next_pos = pos[i + k]\n",
    "\n",
    "            features.update({\n",
    "                f'+{k}:word.lower()': next_word.lower(),\n",
    "                f'+{k}:word.pos': next_pos,\n",
    "                f'+{k}:word.opinionlexicon': tag_sentiment(word.lower()),\n",
    "                f'+{k}:word.contextswitching': next_word.lower() in CONTEXT_SWITCHING_WORDS,\n",
    "                f'+{k}:word.isstopword()': next_word in EN_STOP_WORDS,\n",
    "                f'+{k}:word.istitle()': next_word.istitle(),\n",
    "                f'+{k}:word.isupper()': next_word.isupper(),\n",
    "                # f'+{k}:word.isspecialchar()': re.sub('[^\\w,\\d,\\s]', '', next_word.lower()) == '', # is punctuation/ special characters\n",
    "            })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to convert sentences into feature sequences\n",
    "def sent2features(sent, window_size=5):\n",
    "    # print(i, sent)\n",
    "    return [word2features(sent, i, window_size) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. <a id='toc1_2_3_'></a>[Define data processing functions](#toc0_)\n",
    "These functions help us streamline and ensure consistencies in our data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df_input, window_size=5, show_BIO_errors=False):\n",
    "  df = df_input.copy()\n",
    "  df[['token_BIO_pairs','token_BIO_pairs_clean','aspect_compute_pairs', 'is_incorrect_tagging']] = df.apply(lambda x: encode_unified_BIO(x, True), axis=1)\n",
    "\n",
    "  X = [sent2features(sentence, window_size) for sentence in df['token_BIO_pairs_clean']]\n",
    "  y = [[word[1] for word in sentence] for sentence in df['token_BIO_pairs_clean']] #[x for x in df['token_BIO_pairs_clean']]\n",
    "\n",
    "\n",
    "  # for i, x in enumerate(df['token_BIO_pairs_clean']):\n",
    "  #   print(i, x)\n",
    "  #   y.append(x[1])\n",
    "    \n",
    "  # These are for showing examples if there are any issues during BIO tagging\n",
    "  errors = df['is_incorrect_tagging'].sum()\n",
    "  print('# of possible incorrect labels in df: ', errors, f'({errors/len(df)*100:.0f}% total records)')\n",
    "  print('\\n\\n Some incorrect examples')\n",
    "\n",
    "  if (errors > 0) & (show_BIO_errors == True):\n",
    "    n_samples = 3\n",
    "    sample = df[df['is_incorrect_tagging']].copy().reset_index()\n",
    "\n",
    "    for i in range(0, n_samples):\n",
    "      s = sample.iloc[i]\n",
    "      print('Text: ', s['token_BIO_pairs'])\n",
    "      print('Dataset provided aspects - original form: ', s['aspects'])\n",
    "      print('Dataset provided aspects - Computed aspects: ', s['aspect_compute_pairs'])\n",
    "      print('Unified BIO tagging: ', s['token_BIO_pairs'])\n",
    "      print('Unified BIO tagging clean: ', s['token_BIO_pairs_clean'])\n",
    "      print('Total text tokens: ', len(s['token_BIO_pairs_clean']))\n",
    "      print('\\n')\n",
    "\n",
    "  return pd.Series([X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_features(string_sentences, window_size=5):\n",
    "  sentences_token = [word_tokenize(sentence) for sentence in string_sentences]\n",
    "  X =  [sent2features(sentence, window_size) for sentence in sentences_token]\n",
    "  return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. <a id='toc1_2_4_'></a>[Complete data preparation](#toc0_)\n",
    "\n",
    "df_train looks ok with the errors we have found previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data to make sure we have the clean data\n",
    "df_train = pd.read_json('data/laptop/train.json')\n",
    "\n",
    "# Drop duplicates as we identified and inspected previously\n",
    "df_train.drop_duplicates(subset='text',inplace=True)\n",
    "\n",
    "# Prepare the data: BIO tagging & get word features\n",
    "df_train[['features', 'unified_BIO_tag']] = prepare_df(df_train, 9, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[Exploratory data analysis](#toc0_)\n",
    "\n",
    "1. POS:\n",
    "  - aspect - sentiment - neither\n",
    "  - context\n",
    "2. Word form (compact Xx)\n",
    "3. Word index\n",
    "4. Sentiment terms around aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. <a id='toc2_1_'></a>[Flatten word features](#toc0_)\n",
    "\n",
    "We are going to flatten our word features into a pandas dataframe for easier exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features2df(sentence_index, X):\n",
    "  '''\n",
    "  This function takes in array of (array of dictionary) and flattens it to a pandas dataframe with all features as columns.\n",
    "  Each element in an array is a sentence which is an array of dictionaries of each word and its features\n",
    "  Missing features will have a null value.\n",
    "\n",
    "  INPUT: \n",
    "  -----------------------\n",
    "  SENTENCE INDEX: int\n",
    "  This give the index of the sentence\n",
    "\n",
    "  '''\n",
    "  df = pd.DataFrame(X)\n",
    "  df['nth_sentence'] = sentence_index\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word features into data frame\n",
    "df_features = pd.concat([features2df(df_train.iloc[i]['id'], df_train.iloc[i]['features']) for i in range(0, len(df_train))], ignore_index=True)\n",
    "df_features['unified_BIO_tag'] = [tag for sentence in df_train['unified_BIO_tag'] for tag in sentence]\n",
    "\n",
    "# Generalized aspect tag\n",
    "df_features['is_aspect'] = df_features['unified_BIO_tag'].str[:1].apply(lambda x: 'aspect' if x in ['B','I'] else 'non-aspect')\n",
    "df_features['sentiment'] = df_features['unified_BIO_tag'].str[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. <a id='toc2_2_'></a>[Aspect word distribution](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to look into the dataset to understand what are the distributions of all the word tags. \n",
    "\n",
    "- \"O\" (non-aspect term) words are dominant in the dataset, while, conflict aspect terms are very few, which will be a challenge in training and evaluating the model.\n",
    "- \"B\" tags are more common than \"I\" tags which indicates that the dataset has a lot of singular aspect terms comparing to multi-word aspect terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_features['unified_BIO_tag'].sort_values())\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Very few \"conflict\" aspect words, while dominant by non-aspect words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. <a id='toc2_3_'></a>[Part-of-speech](#toc0_)\n",
    "\n",
    "Aspect terms are often nouns and are verb objectives or subject. Let's inspect their part of speech to confirm this.\n",
    "\n",
    "Same definition of POS tags:\n",
    "- __CD__: Cardinal number (e.g., \"one\", \"two\", \"3\")\n",
    "- __DT__: Determiner (e.g., \"the\", \"a\", \"this\")\n",
    "- __IN__: Preposition or subordinating conjunction (e.g., \"in\", \"on\", \"at\")\n",
    "- __IN__: Preposition or subordinating conjunction (e.g., \"in\", \"on\", \"at\")\n",
    "- __JJ__: Adjective (e.g., \"big\", \"happy\", \"green\")\n",
    "- __NN__: Noun, singular or mass (e.g., \"dog\", \"city\", \"love\")\n",
    "- __NNP__: Proper noun, singular (e.g., \"John\", \"Paris\", \"December\")\n",
    "- __NNS__: Noun, plural (e.g., \"dogs\", \"cities\", \"loves\")\n",
    "- __PRP__: Personal pronoun (e.g., \"I\", \"you\", \"he\")\n",
    "- __RB__: Adverb (e.g., \"quickly\", \"very\", \"well\")\n",
    "- __VB__: Verb, base form (e.g., \"eat\", \"run\", \"play\")\n",
    "\n",
    "\\>80% aspect terms are nouns, while non-aspect words have very uniform distribution for all part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count words per each POS per each tag\n",
    "pos_counts = df_features.groupby(['is_aspect', 'word.pos']).agg(word_count=pd.NamedAgg(column='is_aspect', aggfunc='count')).reset_index()\n",
    "\n",
    "# Separate data to aspect & non-aspect words\n",
    "aspect_pos_counts = pos_counts[pos_counts['is_aspect'] == 'aspect']\n",
    "non_aspect_pos_counts = pos_counts[pos_counts['is_aspect'] == 'non-aspect']\n",
    "\n",
    "# Plot POS per word type\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\n",
    "# Aspects\n",
    "fig.add_trace(go.Pie(labels=aspect_pos_counts['word.pos'], values=aspect_pos_counts['word_count'], name=\"aspects\"),\n",
    "              1, 1)\n",
    "# Non-aspects\n",
    "fig.add_trace(go.Pie(labels=non_aspect_pos_counts['word.pos'], values=non_aspect_pos_counts['word_count'], name=\"non aspects\"),\n",
    "              1, 2)\n",
    "\n",
    "fig.update_traces(hole=.4, direction='clockwise', textinfo='label+percent', textposition='inside',showlegend = False)\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    title_text=\"Part-of-speech per word tag\",\n",
    "    annotations=[dict(text='Aspect', x=0.188, y=0.5, font_size=16, showarrow=False),\n",
    "                 dict(text='Non-aspect', x=0.83, y=0.5, font_size=16, showarrow=False)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. <a id='toc2_4_'></a>[Opinion Lexicon - Sentiment](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment of an aspect term in a sentence can be extract from their context words. \n",
    "\n",
    "Here I built the feature set using 4 preceding and 4 succeeding words and their sentiment lexicon. I also filtered out only words that are tagged as \"B\" for this exploration, as it is easier to understand the relationshop with n-words from the starting of a term that give us the sentiment for the aspect term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opnion_lexicon_columns = [x for x in list(df_features.columns) if re.match('.*opinionlexicon.*', x)]\n",
    "opnion_lexicon_columns.remove('word.opinionlexicon')\n",
    "\n",
    "df_aspects = df_features[df_features['unified_BIO_tag'].str.match(r'B.*')].copy()\n",
    "df_opnions = df_aspects[opnion_lexicon_columns]\n",
    "df_aspects['POS_word_counts'] = df_opnions.map(lambda cell: bool(re.match('POS', str(cell)))).max(axis=1)\n",
    "df_aspects['NEG_word_counts'] = df_opnions.map(lambda cell: bool(re.match('NEG', str(cell)))).max(axis=1)\n",
    "df_aspects['NEU_word_counts'] = df_opnions.map(lambda cell: bool(re.match('NEU', str(cell)))).max(axis=1)\n",
    "\n",
    "df_aspects[df_aspects['is_aspect']=='aspect'].groupby('sentiment')[['POS_word_counts','NEG_word_counts','NEU_word_counts']].agg('mean').sort_values(by=['POS_word_counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table, I computed the % of aspect term's head word whether they have any positve/ negative/ neutral words within 4 words around them.\n",
    "- We noticed that \"negative\" aspect terms has the highest rate of having at least one negative words around it comparing to others.\n",
    "- While, positive aspect terms have much higher rate of positive words comparing to negative words around them\n",
    "- Neutral aspect terms have event higher rate of having negative context words (ignoring that the sample size is really small), while their positive context word seems to be lower comparing to negative aspect terms\n",
    "- What about conflict aspect term? Why does its chances of having positive context words is almost zero?\n",
    "\n",
    "Let's look into a few conflict aspect term examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in [1054, 2390, 2001, 599]:\n",
    "  print(df_train[df_train['id']==id]['text'].to_list())\n",
    "  print(df_train[df_train['id']==id]['aspects'].to_list())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing I noticed from these samples is that all four samples have a \"context switching\" conjunction: \"but\" which makes a lot of sense. Therefore, I added a new feature into our word feature set: `is_context_switching` words by matching it with a list of words like: but, however, although, nevertheless,....\n",
    "\n",
    "Re-run the above analysis with context switching conjunctions, we saw that ~35% of conflict aspect terms have context switching words within 4 words from the head word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_switching_columns = [x for x in list(df_features.columns) if re.match('.*contextswitching.*', x)]\n",
    "\n",
    "df_aspects['contextswitching_word_counts'] = df_aspects[context_switching_columns].max(axis=1)\n",
    "sentiment_context_summary = df_aspects[df_features['unified_BIO_tag'].str.match(r'B.*')].groupby('sentiment')[['POS_word_counts','NEG_word_counts','contextswitching_word_counts']].agg('mean').sort_values(by=['POS_word_counts'])\n",
    "\n",
    "fig = px.bar( sentiment_context_summary.astype(float), x=sentiment_context_summary.index, y=['POS_word_counts', 'NEG_word_counts', 'contextswitching_word_counts'],\n",
    "             title='% aspect words contains positive, negative, and context switching context words by Sentiment',\n",
    "             labels={'value': 'Mean Word Count', 'variable': 'Word Count Type', 'sentiment': 'Sentiment'},\n",
    "             barmode='group')\n",
    "\n",
    "sentiment_context_summary\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Split data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have learned and confirmed some of our hypothesis on the influences of aspect terms and their sentiments. We will now load the full dataset and prepare it again for our modelling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data to make sure we have the clean data\n",
    "df_train = pd.read_json('data/laptop/train.json')\n",
    "\n",
    "# Drop duplicates as we identified and inspected previously\n",
    "df_train.drop_duplicates(subset='text',inplace=True)\n",
    "\n",
    "# Prepare the data: BIO tagging & get word features\n",
    "df_train[['features', 'unified_BIO_tag']] = prepare_df(df_train, 9, True)\n",
    "\n",
    "X = df_train[['id','features']]\n",
    "y = df_train['unified_BIO_tag']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[Random forest](#toc0_)\n",
    "\n",
    "First, we can try applying basic ML algorithm to get a sense of the data. I am selecting Random Forest as it is easy to setup & understand and is also quite powerful in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. <a id='toc4_1_'></a>[Data prepration](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform random forest, we need to flatten our word feature dataset so that it can be ussed for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. <a id='toc4_1_1_'></a>[Flattening](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the feature set involves a lot of categorical data like: word, its context words, its endings, ..., the flatten dataset can be very wide if we need to perform one-hot-encoding all the data. \n",
    "For simplicity and also from our EDA, we noticed that word's POS is very important, and its context' sentiment lexicon. \n",
    "\n",
    "I will perform one-hot-encoding on the words, and retaining POS and sentiment lexicon features, while dropping all other categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word features into data frame\n",
    "def data_prep_rf(X, y, one_hot_encoder=None):\n",
    "  X_rf = pd.concat([features2df(X.iloc[i]['id'], X.iloc[i]['features']) for i in range(0, len(X))], ignore_index=True)\n",
    "  y_rf = [tag for sentence in y for tag in sentence]\n",
    "\n",
    "  X_rf.replace(True,1, inplace=True)\n",
    "  X_rf.replace(False,0, inplace=True)\n",
    "\n",
    "  # Fill NaN values in object-type columns with a 'missing'\n",
    "  object_columns = X_rf.select_dtypes(include=['object']).columns\n",
    "  X_rf[object_columns] = X_rf[object_columns].fillna(value='missing')\n",
    "\n",
    "  # Fill the rest NaN with -1, since most of missing features are boolean\n",
    "  X_rf = X_rf.fillna(-1)\n",
    "  \n",
    "  # We renove all categorical columns that are not POS, or sentiment lexicon + nth_sentence which the number of the sentence we we use to string back the data at the end, but is not needed for training\n",
    "  drop_cols = [x for x in list(X_rf.columns) if re.match('(.*word\\.lower\\(\\))|(.*word\\[-\\d\\:])', x)] + ['nth_sentence'] # word columns\n",
    "  X_rf = X_rf.drop(columns=drop_cols, axis=1)\n",
    "\n",
    "  # Perform one-hot-encoder on the reamining columns\n",
    "  if one_hot_encoder is None: # This is to make sure we use the same one-hot-encoder for both train & test split (avoiding data leakage)\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False, drop='first')  # 'drop' parameter is optional, set to 'first' to avoid multicollinearity\n",
    "    encoded_data = one_hot_encoder.fit_transform(X_rf.select_dtypes(include=['object']))\n",
    "\n",
    "  else: # when we perform data prep for test data, we can reuse the one-hot-encoder used during training data preparation\n",
    "    encoded_data = one_hot_encoder.transform(X_rf.select_dtypes(include=['object']))\n",
    "\n",
    "  # Re-added one-hot-encoding data back to the main dataframe\n",
    "  df_encoded = pd.DataFrame(encoded_data, columns=one_hot_encoder.get_feature_names_out(X_rf.select_dtypes(include=['object']).columns))\n",
    "  X_rf = pd.concat([X_rf, df_encoded], axis=1)\n",
    "\n",
    "  # Dropped all the categorical features that have already been one-hot-encoded\n",
    "  X_rf.drop(columns=X_rf.select_dtypes(include=['object']).columns, inplace=True)\n",
    "\n",
    "  return X_rf, y_rf, one_hot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf, y_train_rf, one_hot_encoder = data_prep_rf(X_train, y_train)\n",
    "X_test_rf, y_test_rf, _ = data_prep_rf(X_test, y_test, one_hot_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. <a id='toc4_1_2_'></a>[Oversampling](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noticed in our EDA process, our data is very skewed with lots of \"O\" (non-aspect) words and very few \"conflict\" aspect terms. \n",
    "So, I am applying oversampling with a hope to improve the model performance a bit.\n",
    "We can see that after applying SMOTE over sampling, we now have a uniform distribution of all aspect tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display class distribution before oversampling\n",
    "print(\"Class distribution before oversampling:\", Counter(y_train_rf))\n",
    "\n",
    "# Apply SMOTE to upsample the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_rf_resampled, y_train_rf_resampled = smote.fit_resample(X_train_rf, y_train_rf)\n",
    "\n",
    "# Display class distribution after oversampling\n",
    "print(\"Class distribution after oversampling:\", Counter(y_train_rf_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. <a id='toc4_1_3_'></a>[Scaling](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than binary features, we still have a few features around word index within a sentence, therefore, scaling can help bring all features to similar scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "X_train_rf_scaled = scaler.fit_transform(X_train_rf_resampled)\n",
    "X_test_rf_scaled = scaler.transform(X_test_rf) # Double check why we are seeing more columns in test????\n",
    "\n",
    "print(X_train_rf_scaled.shape, len(y_train_rf_resampled))\n",
    "print(X_test_rf_scaled.shape, len(y_test_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. <a id='toc4_2_'></a>[Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=5, random_state=42)\n",
    "rf_classifier.fit(X_train_rf_scaled, y_train_rf_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_train_rf_pred = rf_classifier.predict(X_train_rf_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_train_rf_resampled, y_train_rf_pred)\n",
    "classification_rep = classification_report(y_train_rf_resampled, y_train_rf_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_train_rf_resampled, y_train_rf_pred)\n",
    "\n",
    "# Plot the confusion matrix with seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance looks very good on all metrics and all aspect tags. Can the model perform close to that on test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. <a id='toc4_3_'></a>[Test](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_classifier.predict(X_test_rf_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_rf, y_pred_rf)\n",
    "classification_rep = classification_report(y_test_rf, y_pred_rf)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_rf, y_pred_rf)\n",
    "\n",
    "# Plot the confusion matrix with seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_, vmax=100, vmin=1)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of the very skewed dataset identified earlier, the model could not make any prediction on \"I-CON\" which is expected as the chances of having those tags in the results is also very rare. \n",
    "\n",
    "However, we can see that the model predict most words to be \"O\", and for those that it was able to predict not \"O\", it oftens confused between \"B\" & \"I\" and would almost predicting all results evenly between different sentiments.\n",
    "\n",
    "In the end, the model performance has dropped significantly comparing to training set. It looks like the model could not learn anything from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[CRF model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Random Field (CRF) model is a very famous algorithm for NER classification task. It is a type of discriminative undirected probabilistic graphical model.\n",
    "\n",
    "Let's apply this model on our dataset and see how it will perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. <a id='toc5_1_'></a>[Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train CRF model\n",
    "crf_model = CRF(algorithm='lbfgs',\n",
    "                max_iterations=100,\n",
    "                c1=0.5,\n",
    "                c2=0.05)\n",
    "\n",
    "# There is this error existing with this library: 'CRF' object has no attribute 'keep_tempfiles'\n",
    "# which has not been resolved and we can bypass it using this trick.\n",
    "try:\n",
    "  crf_model.fit(X_train['features'], y_train)\n",
    "except AttributeError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_crf_pred = crf_model.predict(X_train['features'])\n",
    "\n",
    "y_train_flat = [tag for sentence in y_train for tag in sentence]\n",
    "y_train_crf_pred_flat = [tag for sentence in y_train_crf_pred for tag in sentence]\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_train_flat, y_train_crf_pred_flat)\n",
    "classification_rep = classification_report(y_train_flat, y_train_crf_pred_flat)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_train_flat, y_train_crf_pred_flat)\n",
    "\n",
    "# Plot the confusion matrix with seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=crf_model.classes_, yticklabels=crf_model.classes_, vmax=100, vmin=1)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CRF model also performs very well on our training data like Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_crf_pred = crf_model.predict(X_test['features'])\n",
    "\n",
    "y_test_flat = [tag for sentence in y_test for tag in sentence]\n",
    "y_test_crf_pred_flat = [tag for sentence in y_test_crf_pred for tag in sentence]\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_flat, y_test_crf_pred_flat)\n",
    "classification_rep = classification_report(y_test_flat, y_test_crf_pred_flat)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_flat, y_test_crf_pred_flat)\n",
    "\n",
    "# Plot the confusion matrix with seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=crf_model.classes_, yticklabels=crf_model.classes_, vmax=100, vmin=1)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to Random Forest mode, CRF performance has also dropped a lot, but the model would now classify most words to be \"I-CON\" which is the least common tags in our dataset, maybe if we were to balance our training data, we can improve this error.\n",
    "\n",
    "The model also could not distinguish the aspect sentiments very well. However, we can see that fewer \"I\" words are being predicted as \"B\" and vice versa.\n",
    "\n",
    "So in general, CRF does have a slightly better improvement comparing to Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[Model evaluation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since NER is a multi-class classification task, we cannot look into each individual tag's metrics to evaluate different models. We need a single metric that can unified other individual tag's performance.\n",
    "\n",
    "Here I would use __macro average f1-score__ for our model evaluation and selection:\n",
    "- f1-score is a harmonic metric that gives us a balance view on our imbalanced dataset\n",
    "- Also a macro average is selected in order to combine all idividual tag metrics together. I selected \"macro\" instead of \"micro\" average as it gives more balanced evaluation regardless of the imbalanced distribution of the tags in the dataset & the likelyhood of correctly predicting the dominant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('Macro avg f1-score:')\n",
    "print('Random Forest: ', f1_score(y_test_rf, y_pred_rf, average='macro'))\n",
    "print('CRF          : ', f1_score(y_test_flat, y_test_crf_pred_flat, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this metric, we can see that a default CRF model has already improved the performance by almost 1.5x times comparing to Random Forest. \n",
    "\n",
    "If we were to provided better word embeddings (features) to the model, and tuning the context windows, and other hyper-parameters, I believe the model performance will improve significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random\n",
    "from highlight_text import HighlightText, ax_text, fig_text\n",
    "\n",
    "def vizualize_samples (sentences, pred_tags, tags = None):\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(30,10))\n",
    "  font = {'family' : 'arial',\n",
    "          'size'   : 16}\n",
    "  matplotlib.rc('font', **font)\n",
    "  final_text = []\n",
    "  color = []\n",
    "  pos_element = {\"bbox\": {\"edgecolor\": \"Green\", \"facecolor\": \"#99FF00\", \"linewidth\": 1.5, \"pad\": 1}} \n",
    "  neu_element = {\"bbox\": {\"edgecolor\": \"Orange\", \"facecolor\": \"Yellow\", \"linewidth\": 1.5, \"pad\": 1.5}} \n",
    "  neg_element = {\"bbox\": {\"edgecolor\": \"Red\", \"facecolor\": \"#FF99CC\", \"linewidth\": 1.5, \"pad\": 1}}\n",
    "  con_element = {\"bbox\": {\"edgecolor\": \"#6600FF\", \"facecolor\": \"#99FFFF\", \"linewidth\": 1.5, \"pad\": 1}}\n",
    "  \n",
    "  color_map = {'POS': pos_element, 'NEU': neu_element, 'NEG':neg_element, 'CON': con_element}\n",
    "\n",
    "  final_text.append('\\n---------------\\n')\n",
    "  final_text.append(f'  <positive>  ')\n",
    "  color.append(pos_element)\n",
    "  final_text.append(f'  <negative>  ')\n",
    "  color.append(neg_element)\n",
    "  final_text.append(f'  <neutral>  ')\n",
    "  color.append(neu_element)\n",
    "  final_text.append(f'  <conflict>  ')\n",
    "  color.append(con_element)\n",
    "  final_text.append('\\n---------------\\n\\n\\n')\n",
    "\n",
    "  for s in range(0, len(sentences)):\n",
    "    final_text.append('(P) ')\n",
    "    chunk = []\n",
    "    next_tag = ''\n",
    "    \n",
    "    for w in range(0, len(sentences[s])):\n",
    "      # print(w)\n",
    "      word = sentences[s][w]\n",
    "      tag = pred_tags[s][w]\n",
    "      # print(tag[2:])  \n",
    "      if w + 1 < len(sentences[s]):\n",
    "        next_tag = pred_tags[s][w+1]\n",
    "      else:\n",
    "        next_tag = 'O'\n",
    "      \n",
    "      # print(word, tag, next_tag)\n",
    "\n",
    "      if tag == 'O':\n",
    "        final_text.append(word)\n",
    "      elif (tag in ['I-POS','I-NEG', 'I-NEU','I-CON','B-POS','B-NEG', 'B-NEU','B-CON']):\n",
    "        chunk.append(word)\n",
    "\n",
    "      if (next_tag in ['B-POS','B-NEG', 'B-NEU','B-CON', 'O']) & (len(chunk) > 0):\n",
    "        final_text.append(f'  <{\" \".join(chunk)}>  ')\n",
    "        color.append(color_map[tag[2:]])\n",
    "        chunk = []\n",
    "\n",
    "    if tags is not None:\n",
    "      final_text.append('\\n')\n",
    "      final_text.append('(A) ')\n",
    "      for w in range(0, len(sentences[s])):\n",
    "        # print(w)\n",
    "        word = sentences[s][w]\n",
    "        tag = tags[s][w]\n",
    "        if w + 1 < len(sentences[s]):\n",
    "          next_tag = tags[s][w+1]\n",
    "        else:\n",
    "          next_tag = 'O'\n",
    "          \n",
    "        if tag == 'O':\n",
    "          final_text.append(word)\n",
    "        elif (tag in ['I-POS','I-NEG', 'I-NEU','I-CON','B-POS','B-NEG', 'B-NEU','B-CON']):\n",
    "          chunk.append(word)\n",
    "\n",
    "        if (next_tag in ['B-POS','B-NEG', 'B-NEU','B-CON', 'O']) & (len(chunk) > 0):\n",
    "          final_text.append(f'  <{\" \".join(chunk)}>  ')\n",
    "          color.append(color_map[tag[2:]])\n",
    "          chunk = []\n",
    "  \n",
    "    \n",
    "    final_text.append('\\n---------------\\n')\n",
    "\n",
    "\n",
    "    HighlightText(x=0, y=1,\n",
    "                s=' '.join(final_text),\n",
    "                highlight_textprops=color,\n",
    "                ax=ax)\n",
    "                \n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random\n",
    "from highlight_text import HighlightText, ax_text, fig_text\n",
    "\n",
    "def vizualize_samples (sentences, tag_dict):\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(30,10))\n",
    "  font = {'family' : 'arial',\n",
    "          'size'   : 16}\n",
    "  matplotlib.rc('font', **font)\n",
    "  final_text = []\n",
    "  color = []\n",
    "  pos_element = {\"bbox\": {\"edgecolor\": \"Green\", \"facecolor\": \"#99FF00\", \"linewidth\": 1.5, \"pad\": 1}} \n",
    "  neu_element = {\"bbox\": {\"edgecolor\": \"Orange\", \"facecolor\": \"Yellow\", \"linewidth\": 1.5, \"pad\": 1.5}} \n",
    "  neg_element = {\"bbox\": {\"edgecolor\": \"Red\", \"facecolor\": \"#FF99CC\", \"linewidth\": 1.5, \"pad\": 1}}\n",
    "  con_element = {\"bbox\": {\"edgecolor\": \"#6600FF\", \"facecolor\": \"#99FFFF\", \"linewidth\": 1.5, \"pad\": 1}}\n",
    "  \n",
    "  color_map = {'POS': pos_element, 'NEU': neu_element, 'NEG':neg_element, 'CON': con_element}\n",
    "\n",
    "  final_text.append('\\n---------------\\n')\n",
    "  final_text.append(f'  <positive>  ')\n",
    "  color.append(pos_element)\n",
    "  final_text.append(f'  <negative>  ')\n",
    "  color.append(neg_element)\n",
    "  final_text.append(f'  <neutral>  ')\n",
    "  color.append(neu_element)\n",
    "  final_text.append(f'  <conflict>  ')\n",
    "  color.append(con_element)\n",
    "  final_text.append('\\n---------------\\n\\n\\n')\n",
    "\n",
    "  for s in range(0, len(sentences)):\n",
    "    \n",
    "    chunk = []\n",
    "    next_tag = ''\n",
    "    \n",
    "    for k in tag_dict:\n",
    "      # print(k)\n",
    "      tags = tag_dict[k]\n",
    "      final_text.append(f'\\n({k}) ')\n",
    "      for w in range(0, len(sentences[s])):\n",
    "        word = sentences[s][w]\n",
    "        tag = tags[s][w]\n",
    "\n",
    "        if w + 1 < len(sentences[s]):\n",
    "          next_tag = tags[s][w+1]\n",
    "        else:\n",
    "          next_tag = 'O'\n",
    "\n",
    "\n",
    "        if tag == 'O':\n",
    "          final_text.append(word)\n",
    "        elif (tag[:1] in ['B','I']):\n",
    "          chunk.append(word)\n",
    "\n",
    "        if (next_tag[:1] in ['B','O']) & (len(chunk) > 0):\n",
    "          final_text.append(f'  <{\" \".join(chunk)}>  ')\n",
    "          color.append(color_map[tag[2:]])\n",
    "          chunk = []\n",
    "    \n",
    "    final_text.append('\\n---------------\\n')\n",
    "\n",
    "\n",
    "    HighlightText(x=0, y=1,\n",
    "                s=' '.join(final_text),\n",
    "                highlight_textprops=color,\n",
    "                ax=ax)\n",
    "                \n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 5\n",
    "integer = random.randint(0,len(X_test) - samples)\n",
    "tags = y_test[integer:integer+samples].array\n",
    "pred_tags = crf_model.predict(X_test['features'][integer:integer+samples])\n",
    "sentences = [[x['word.lower()'] for x in sentence] for sentence in X_test['features'][integer:integer+samples]]\n",
    "\n",
    "print(integer)\n",
    "vizualize_samples(sentences, {'CRF  ':pred_tags, 'Actual': tags})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. <a id='toc9_'></a>[Next steps](#toc0_)\n",
    "1. Add more features:\n",
    "  - head words\n",
    "  - Google Word2Vec cluster_id\n",
    "\n",
    "2. Employ pre-trained word embeddings\n",
    "3. Hyper-parameter tuning\n",
    "4. Re-train model using rule-based aspect term extraction on larger dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainstation_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
