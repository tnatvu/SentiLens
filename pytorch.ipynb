{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Data preparation](#toc1_)    \n",
    "  - 1.1. [Tokenize sentence and aspect BIO encoding class](#toc1_1_)    \n",
    "  - 1.2. [Load data](#toc1_2_)    \n",
    "  - 1.3. [Inspect tagging issues](#toc1_3_)    \n",
    "- 2. [EDA](#toc2_)    \n",
    "  - 2.1. [Transform \"conflict\" aspect to \"negative\"](#toc2_1_)    \n",
    "- 3. [BERT](#toc3_)    \n",
    "  - 3.1. [Convert df to HuggingFace datasets](#toc3_1_)    \n",
    "  - 3.2. [Define model](#toc3_2_)    \n",
    "    - 3.2.1. [Define tokenizer](#toc3_2_1_)    \n",
    "    - 3.2.2. [Define token classification model](#toc3_2_2_)    \n",
    "  - 3.3. [Data preparation](#toc3_3_)    \n",
    "  - 3.4. [Performance metrics](#toc3_4_)    \n",
    "  - 3.5. [Training](#toc3_5_)    \n",
    "    - 3.5.1. [Define trainer](#toc3_5_1_)    \n",
    "    - 3.5.2. [Training results](#toc3_5_2_)    \n",
    "  - 3.6. [Error analysis](#toc3_6_)    \n",
    "    - 3.6.1. [Model performance](#toc3_6_1_)    \n",
    "    - 3.6.2. [Group by word token](#toc3_6_2_)    \n",
    "    - 3.6.3. [Group by Tag ID](#toc3_6_3_)    \n",
    "- 4. [Save model](#toc4_)    \n",
    "- 5. [Load saved model](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0' # this setting is needed to run NN on my Mac\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from highlight_text import HighlightText, ax_text, fig_text\n",
    "\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "# from seqeval.metrics import f1_score\n",
    "# pip install torch==2.2.0 torchtext --index-url https://download.pytorch.org/whl/test/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Data preparation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Tokenize sentence and aspect BIO encoding class](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceToken:\n",
    "  '''\n",
    "    SentenceToken\n",
    "\n",
    "  '''\n",
    "  def __init__(self, sentence, aspect_type=None, aspects=None, sentence_id=None):\n",
    "    \n",
    "    if sentence_id is not None:\n",
    "      print(sentence_id)\n",
    "\n",
    "    self.sentence_id = sentence_id\n",
    "    self.sentence = sentence.replace(u\"\\u00A0\", \" \").replace(u'\\xa0',' ') # replace unicode space character\n",
    "                            \n",
    "    self.aspect_bio_tags = None\n",
    "    self.unified_aspect_bio_tags = None\n",
    "    self.token_span = None\n",
    "    self.space_pre_token = None\n",
    "\n",
    "    # Tokenize sentence\n",
    "    self.__tokenize_sentence(self.sentence)\n",
    "\n",
    "    if aspect_type == 'dict':\n",
    "      self.set_aspect_tagging_from_dict(aspects)\n",
    "    elif aspect_type == 'bio':\n",
    "      self.set_aspect_bio_tags(aspects)\n",
    "    elif aspect_type == 'unified bio':\n",
    "      self.set_aspect_unified_bio_tags(aspects)\n",
    "  \n",
    "  def __tokenize_sentence(self, sentence):\n",
    "    token_span = list(TreebankWordTokenizer().span_tokenize(sentence))\n",
    "    new_token_span = token_span #[] # if want to break down word followed by special character using the commented code\n",
    "    \n",
    "    # for k in token_span:\n",
    "    #   token_start = k[0]\n",
    "    #   token_end = k[1]\n",
    "\n",
    "    #   token = sentence[token_start:token_end]\n",
    "    #   sub_tokens = re.split(r'([^\\w,\\d])', token)\n",
    "      \n",
    "    #   sub_token_start = token_start\n",
    "    #   for sub_token in sub_tokens:\n",
    "    #     if len(sub_token) != 0:\n",
    "    #       sub_token_end = sub_token_start + len(sub_token)\n",
    "    #       new_token_span.append((sub_token_start, sub_token_end))\n",
    "    #       sub_token_start = sub_token_end\n",
    "    \n",
    "    self.token_span = new_token_span\n",
    "    self.space_pre_token = [True if sentence[k[0]-1:k[0]] == ' ' else False for i,k in enumerate(new_token_span)]\n",
    "\n",
    "  def set_aspect_tagging_from_dict(self, aspects):\n",
    "    polarity_map = {'positive':'POS'\n",
    "              ,'negative': 'NEG'\n",
    "              ,'conflict': 'CON'\n",
    "              ,'neutral': 'NEU'}\n",
    "    \n",
    "    bio_tags = ['O'] * len(self.token_span)\n",
    "    unified_bio_tags = bio_tags\n",
    "\n",
    "    for x in aspects:\n",
    "      if x['term'] != '':\n",
    "        aspect_from = int(x['from'])\n",
    "        aspect_to = int(x['to'])\n",
    "        polarity = '-' + polarity_map[x['polarity']]\n",
    "\n",
    "        # aspect_token_ids =  [i for i, v in enumerate(self.token_span) if (v[0] >= aspect_from) & (v[1] <= aspect_to)]\n",
    "        # if aspect_token_ids != []:\n",
    "        #   aspect_from_index = min(aspect_token_ids)\n",
    "        #   aspect_to_index = max(aspect_token_ids)\n",
    "        #   aspect_from = int(x['from'])\n",
    "        #   aspect_length = aspect_to_index - aspect_from_index\n",
    "        #   bio_tags = bio_tags[:aspect_from_index] + ['B'] + ['I'] * (aspect_length) + bio_tags[aspect_to_index+1:]\n",
    "        #   unified_bio_tags = unified_bio_tags[:aspect_from_index] + ['B' + polarity] + ['I'+ polarity] * (aspect_length) + unified_bio_tags[aspect_to_index+1:]\n",
    "        \n",
    "        aspect_from_index = [i for i, v in enumerate(self.token_span) if (v[0] <= aspect_from) & (v[1] >= aspect_from)][0]\n",
    "        aspect_to_index = [i for i, v in enumerate(self.token_span) if (v[0] <= aspect_to) & (v[1] >= aspect_to)][0]\n",
    "      \n",
    "        aspect_length = aspect_to_index - aspect_from_index\n",
    "        bio_tags = bio_tags[:aspect_from_index] + ['B'] + ['I'] * (aspect_length) + bio_tags[aspect_to_index+1:]\n",
    "        unified_bio_tags = unified_bio_tags[:aspect_from_index] + ['B' + polarity] + ['I'+ polarity] * (aspect_length) + unified_bio_tags[aspect_to_index+1:]\n",
    "        \n",
    "\n",
    "    self.set_aspect_bio_tags(bio_tags)\n",
    "    self.set_aspect_unified_bio_tags(unified_bio_tags)\n",
    "\n",
    "  def rebuild_sentence_from_token(self):\n",
    "    return ''.join([(' ' if self.space_pre_token[i] else '') + self.sentence[k[0]:k[1]] for i, k in enumerate(self.token_span)])\n",
    "  \n",
    "  def get_sentence_token_with_aspect_bio_tag(self, unified_bio_tag=False):\n",
    "    if (unified_bio_tag == False) & (self.aspect_bio_tags is None):\n",
    "      raise Exception('No BIO tags provided. Use \"SentenceToken.set_aspect_bio_tags()\" method to add bio_tags')\n",
    "    elif (unified_bio_tag == True) & (self.aspect_unified_bio_tags is None):\n",
    "      raise Exception('No Unified BIO tags provided. Use \"SentenceToken.set_aspect_unified_bio_tags()\" method to add unified_bio_tags')\n",
    "    else:\n",
    "      return [(self.sentence[k[0]:k[1]], self.aspect_unified_bio_tags[i] if unified_bio_tag else self.aspect_bio_tags[i]) for i, k in enumerate(self.token_span)]\n",
    "\n",
    "  def set_aspect_bio_tags(self, aspect_bio_tags):\n",
    "    self.aspect_bio_tags = aspect_bio_tags\n",
    "    self.aspect_unified_bio_tags = aspect_bio_tags\n",
    "\n",
    "  def set_aspect_unified_bio_tags(self, aspect_unified_bio_tags):\n",
    "    self.aspect_unified_bio_tags = aspect_unified_bio_tags\n",
    "    self.aspect_bio_tags = [k[0:1] for k in aspect_unified_bio_tags]\n",
    "\n",
    "  def get_tokens(self):\n",
    "    '''\n",
    "    get_tokens()\n",
    "    Return an array of sentence word tokens\n",
    "    '''\n",
    "    return [self.sentence[k[0]:k[1]] for k in self.token_span]\n",
    "  \n",
    "  def check_rebuild_sentence_from_token(self):\n",
    "    '''\n",
    "    check_rebuild_sentence_from_token()\n",
    "\n",
    "    This is a test / debugger function.\n",
    "    This help validating if we have computed the sentence to token properly and whether we can re-compute the exact sentence from information stored.\n",
    "    '''\n",
    "    return re.sub(r'\\s+', ' ',self.sentence.strip()) == self.rebuild_sentence_from_token().strip()\n",
    "  \n",
    "  def check_rebuild_aspect_terms(self, aspect_dict):\n",
    "    '''\n",
    "    check_rebuild_aspect_terms(aspect_dict)\n",
    "\n",
    "    This is a test / debugger fucntion. \n",
    "    This help validate if we have compute the correct aspect terms as given by the aspect dict\n",
    "\n",
    "    INPUT:\n",
    "    aspect dict: array of aspect dictionaries in the following format\n",
    "      [{'term': 'storage', \n",
    "       'polarity': 'positive', \n",
    "       'from': '14', \n",
    "       'to': '21'}]\n",
    "    '''\n",
    "    aspect_dict = sorted(aspect_dict, key=lambda d: int(d['from']))\n",
    "    aspect_input = [k['term'].replace(u\"\\u00A0\", \" \").replace(u'\\xa0',' ') for k in aspect_dict if k['term'] != '' ]\n",
    "    aspect_computed = []\n",
    "    aspect = ''\n",
    "    \n",
    "    for i,k in enumerate(self.aspect_bio_tags):\n",
    "      token = self.sentence[self.token_span[i][0]:self.token_span[i][1]]\n",
    "      \n",
    "      if k == 'B':\n",
    "        if (self.aspect_bio_tags[i-1] == 'B' if i > 0 else False):\n",
    "          aspect_computed.append(aspect)\n",
    "        aspect = token\n",
    "      elif k == 'I':\n",
    "        aspect += ' ' * ((self.token_span[i][0] -  self.token_span[i-1][1]) if i > 0 else 0) + token\n",
    "      \n",
    "      if (aspect != '') & ((k == 'O') or (i == (len(self.aspect_bio_tags) - 1))):\n",
    "          aspect_computed.append(aspect)\n",
    "          aspect = ''\n",
    "\n",
    "    return [aspect_input == aspect_computed, aspect_input, aspect_computed]\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.rebuild_sentence_from_token()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Load data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_Data():\n",
    "  df_train = pd.read_json('data/laptop/train.json')\n",
    "  # First, I will need to drop some duplicated data in our training dataset, as identified in the EDA process.\n",
    "  # We have removed 12 duplicated records in our training dataset\n",
    "  df_train.drop_duplicates(subset='text', inplace=True)\n",
    "  print('df_train shape: ', df_train.shape)\n",
    "\n",
    "  df_val = pd.read_json('data/laptop/validate.json') # This will only be used for the very last step to evaluate how well the model is, but is input now for validating the BIO tagging to ensure the function works properly\n",
    "  print('df_val shape: ', df_val.shape)\n",
    "\n",
    "  df_train['sentence_token'] = df_train.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "  df_train['sentence_check'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_sentence_from_token(), axis=1)\n",
    "  df_train['aspect_check'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects']), axis=1)\n",
    "  df_train['aspect_check_TF'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects'])[0], axis=1)\n",
    "  df_train['tokens'] = df_train.apply(lambda x: x['sentence_token'].get_tokens(), axis=1)\n",
    "  df_train['tags'] = df_train.apply(lambda x: x['sentence_token'].aspect_unified_bio_tags, axis=1)\n",
    "\n",
    "  df_val['sentence_token'] = df_val.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "  df_val['sentence_check'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_sentence_from_token(), axis=1)\n",
    "  df_val['aspect_check'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects']), axis=1)\n",
    "  df_val['aspect_check_TF'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects'])[0], axis=1)\n",
    "  df_val['tokens'] = df_val.apply(lambda x: x['sentence_token'].get_tokens(), axis=1)\n",
    "  df_val['tags'] = df_val.apply(lambda x: x['sentence_token'].aspect_unified_bio_tags, axis=1)\n",
    "  return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape:  (3036, 3)\n",
      "df_val shape:  (800, 3)\n",
      "# of df_train records having tokenizing issues:  0\n",
      "# of df_train records having aspect bio tagging issues:  36\n",
      "# of df_test records having tokenizing issues:  0\n",
      "# of df_test records having aspect bio tagging issues:  9\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = reload_Data()\n",
    "print('# of df_train records having tokenizing issues: ', len(df_train[df_train['sentence_check']==False]))\n",
    "print('# of df_train records having aspect bio tagging issues: ', len(df_train[df_train['aspect_check_TF']==False]))\n",
    "print('# of df_test records having tokenizing issues: ', len(df_test[df_test['sentence_check']==False]))\n",
    "print('# of df_test records having aspect bio tagging issues: ', len(df_test[df_test['aspect_check_TF']==False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. <a id='toc1_3_'></a>[Inspect tagging issues](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([ 125,  140,  220,  293,  374,  375,  431,  612,  656,  834,  922,  924,\n",
      "        953,  999, 1031, 1374, 1456, 1502, 1631, 1716, 1936, 1958, 2113, 2160,\n",
      "       2244, 2392, 2502, 2533, 2587, 2606, 2783, 2831, 2842, 2876, 2930, 2940],\n",
      "      dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(df_train[df_train['aspect_check_TF']==False].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'term': 'delivery service', 'polarity': 'negative', 'from': '59', 'to': '75'}]\n",
      "After way too many times sending the thing in for repairs (delivery service was slow, and without the laptop I had no access to the internet, and thus no way of tracking it to find out when I might hope to see my computer again), it finally kicked the bucket after just over 2 years.\n",
      "[False, ['delivery service'], ['(delivery service']]\n",
      "['After', 'way', 'too', 'many', 'times', 'sending', 'the', 'thing', 'in', 'for', 'repairs', '(', 'delivery', 'service', 'was', 'slow', ',', 'and', 'without', 'the', 'laptop', 'I', 'had', 'no', 'access', 'to', 'the', 'internet', ',', 'and', 'thus', 'no', 'way', 'of', 'tracking', 'it', 'to', 'find', 'out', 'when', 'I', 'might', 'hope', 'to', 'see', 'my', 'computer', 'again', ')', ',', 'it', 'finally', 'kicked', 'the', 'bucket', 'after', 'just', 'over', '2', 'years', '.']\n"
     ]
    }
   ],
   "source": [
    "num = 2606\n",
    "\n",
    "print(df_train.loc[num]['aspects'])\n",
    "print(df_train.loc[num]['text'])\n",
    "print(df_train.loc[num]['aspect_check'])\n",
    "print(df_train.loc[num]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the tagging issues due to word that are not separated properly from special characters/ punctuations. The issue is unavoidable in practice as reviews may not adherent to perfect grammar.\n",
    "\n",
    "I have tried to fix this issues to have 100% accuracy with further token breakdown to match the specified aspect tokens, however, this can break some of the standard logics for word tokenizer and further modelling. \n",
    "\n",
    "Therefore, I decided to include a whole token where the aspect may start or end, even if the index is in the middle of token, which may results with aspect tokens that could include extra characters than planned. This is the risk we will accept for this approach, and we can perform a cleaning process to remove these extra characters during implementation with actual use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[EDA](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. <a id='toc2_1_'></a>[Transform \"conflict\" aspect to \"negative\"](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tina.vu/work/fun/sentilens_env_wipp/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/tina.vu/work/fun/sentilens_env_wipp/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-01-27 20:20:54.287142: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, ClassLabel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from transformers import AutoConfig, XLMRobertaConfig, AutoTokenizer, TrainingArguments, DataCollatorForTokenClassification, Trainer\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "\n",
    "device = torch.device('mps') # This is required for Mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[Convert df to HuggingFace datasets](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert aspect tag to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of unique aspect tags\n",
    "tags = list(set(sum(df_train['tags'],[])))\n",
    "tags.sort()\n",
    "tag2idx = {k:i for i,k in enumerate(tags)}\n",
    "\n",
    "# Convert aspect tag text to ids\n",
    "df_train['tags_idx'] = df_train['tags'].apply(lambda x: [tag2idx[k] for k in x])\n",
    "df_test['tags_idx'] = df_test['tags'].apply(lambda x: [tag2idx[k] for k in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pandas to HuggingFace datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags_idx'],\n",
      "        num_rows: 2125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags_idx'],\n",
      "        num_rows: 911\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags_idx'],\n",
      "        num_rows: 800\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split train into train & validation set\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.3, random_state=42,)\n",
    "\n",
    "# Define dataset features\n",
    "features = Features({'tokens': Sequence(Value(dtype='string', id=None)),\n",
    "                    'tags_idx': Sequence(ClassLabel(names=tags))\n",
    "                    })\n",
    "\n",
    "tds = Dataset.from_pandas(df_train[['tokens','tags_idx']], features=features, preserve_index=False)\n",
    "vds = Dataset.from_pandas(df_val[['tokens','tags_idx']], features=features,  preserve_index=False)\n",
    "tsds = Dataset.from_pandas(df_test[['tokens','tags_idx']], features=features, preserve_index=False)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = tds\n",
    "ds['validation'] = vds\n",
    "ds['test'] = tsds\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for total counts per aspect type in each data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39baf5444d9e4408bd8e41f6adb58c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b96d5f8e0a4119a641525c20e24715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa80b6ccb0e349818990acdb80cfb98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert aspect idx to aspect aspect text\n",
    "def create_tag_names(batch):\n",
    "    return {\"tags\": [ds[\"train\"].features[\"tags_idx\"].feature.int2str(idx) for idx in batch[\"tags_idx\"]]}\n",
    "\n",
    "ds = ds.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEG</th>\n",
       "      <th>POS</th>\n",
       "      <th>NEU</th>\n",
       "      <th>CON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>552</td>\n",
       "      <td>675</td>\n",
       "      <td>304</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>310</td>\n",
       "      <td>312</td>\n",
       "      <td>157</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>128</td>\n",
       "      <td>340</td>\n",
       "      <td>169</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NEG  POS  NEU  CON\n",
       "train       552  675  304   30\n",
       "validation  310  312  157   15\n",
       "test        128  340  169   16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform aspect type counts per each dataset split\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in ds.items():\n",
    "    for row in dataset[\"tags\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "                \n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[Define model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_model_name = \"xlm-roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. <a id='toc3_2_1_'></a>[Define tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. <a id='toc3_2_2_'></a>[Define token classification model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['B-CON', 'B-NEG', 'B-NEU', 'B-POS', 'I-CON', 'I-NEG', 'I-NEU', 'I-POS', 'O'], id=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ds['train'].features['tags_idx'].feature\n",
    "\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, \n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaForABSA(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Roberta body\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "\n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Load and initialize weights from pretrained\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                labels=None, **kwargs):\n",
    "        \n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        # Apply classifier to encoder representation (model head)\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, \n",
    "                                     hidden_states=outputs.hidden_states, \n",
    "                                     attentions=outputs.attentions)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. <a id='toc3_3_'></a>[Data preparation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, \n",
    "                                      is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"tags_idx\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_align_labels(examples):\n",
    "#     tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, \n",
    "#                                       is_split_into_words=True)\n",
    "#     labels = []\n",
    "#     for idx, label in enumerate(examples[\"tags_idx\"]):\n",
    "#         word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "#         previous_word_idx = None\n",
    "#         label_ids = []\n",
    "#         for word_idx in word_ids:\n",
    "#             if word_idx is None or word_idx == previous_word_idx:\n",
    "#                 label_ids.append(-100)\n",
    "#             else:\n",
    "#                 label_ids.append(tag2index[label[word_idx]])\n",
    "#             previous_word_idx = word_idx\n",
    "#         labels.append(label_ids)\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True, \n",
    "                      remove_columns=['tags_idx', 'tokens','tags']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d711292c3f35420aadc765c89d394314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d31fcf66084b7da89511b352f715e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d79ad3184c444a8c8925649a32af96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_output\n",
    "ds_encoded = encode_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. <a id='toc3_4_'></a>[Performance metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                index2tag = xlmr_config.id2label\n",
    "                # example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                # example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "                labels_list.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                preds_list.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        # labels_list.append(example_labels)\n",
    "        # preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, \n",
    "                                       eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred, average='macro')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. <a id='toc3_5_'></a>[Training](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1. <a id='toc3_5_1_'></a>[Define trainer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "num_epochs = 6\n",
    "batch_size = 24\n",
    "logging_steps = len(ds[\"train\"]) // batch_size\n",
    "model_name = f\"{xlmr_model_name}-absa\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model/\" + model_name, log_level=\"error\", num_train_epochs=num_epochs, \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\", \n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False, \n",
    "    logging_steps=logging_steps, push_to_hub=False)\n",
    "\n",
    "# Define data collator for data batching \n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
    "     \n",
    "# Model init\n",
    "def model_init():\n",
    "    return (XLMRobertaForABSA\n",
    "            .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "            .to(device))\n",
    "     \n",
    "trainer = Trainer(model_init=model_init, args=training_args, \n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                  train_dataset=ds_encoded[\"train\"],\n",
    "                  eval_dataset=ds_encoded[\"validation\"], \n",
    "                  tokenizer=xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042b3486400943578d475bd1b453a790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3275, 'learning_rate': 4.176029962546817e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357a15fc8f054552a1e7d7fe0f874b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21636153757572174, 'eval_f1': 0.21988777107667387, 'eval_runtime': 10.6484, 'eval_samples_per_second': 85.553, 'eval_steps_per_second': 3.569, 'epoch': 1.0}\n",
      "{'loss': 0.1372, 'learning_rate': 3.352059925093633e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602b5d4d6a2448b9b20425428d14d430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15352432429790497, 'eval_f1': 0.43114449823158857, 'eval_runtime': 10.3004, 'eval_samples_per_second': 88.443, 'eval_steps_per_second': 3.689, 'epoch': 2.0}\n",
      "{'loss': 0.0923, 'learning_rate': 2.5280898876404497e-05, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4545548b230a4aefb32ab53ac77b1653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14529673755168915, 'eval_f1': 0.48605968244677494, 'eval_runtime': 10.2501, 'eval_samples_per_second': 88.878, 'eval_steps_per_second': 3.707, 'epoch': 3.0}\n",
      "{'loss': 0.061, 'learning_rate': 1.704119850187266e-05, 'epoch': 3.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26efb7f73d342beb5755aa34394c629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1534760296344757, 'eval_f1': 0.4998138172090995, 'eval_runtime': 9.49, 'eval_samples_per_second': 95.996, 'eval_steps_per_second': 4.004, 'epoch': 4.0}\n",
      "{'loss': 0.0437, 'learning_rate': 8.801498127340826e-06, 'epoch': 4.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a43f8b3d9f4d5fbdf7f1b25c408876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15812097489833832, 'eval_f1': 0.514965632517541, 'eval_runtime': 9.8346, 'eval_samples_per_second': 92.632, 'eval_steps_per_second': 3.864, 'epoch': 5.0}\n",
      "{'loss': 0.0315, 'learning_rate': 5.617977528089887e-07, 'epoch': 5.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb1bbf281fe42c5b7c7362b56627f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1625165492296219, 'eval_f1': 0.5191766911763827, 'eval_runtime': 8.2257, 'eval_samples_per_second': 110.75, 'eval_steps_per_second': 4.62, 'epoch': 6.0}\n",
      "{'train_runtime': 3148.0777, 'train_samples_per_second': 4.05, 'train_steps_per_second': 0.17, 'train_loss': 0.11458374970079807, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.11458374970079807, metrics={'train_runtime': 3148.0777, 'train_samples_per_second': 4.05, 'train_steps_per_second': 0.17, 'train_loss': 0.11458374970079807, 'epoch': 6.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2. <a id='toc3_5_2_'></a>[Training results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3275</td>\n",
       "      <td>0.216362</td>\n",
       "      <td>0.219888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>0.153524</td>\n",
       "      <td>0.431144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>0.145297</td>\n",
       "      <td>0.486060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0610</td>\n",
       "      <td>0.153476</td>\n",
       "      <td>0.499814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.158121</td>\n",
       "      <td>0.514966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.162517</td>\n",
       "      <td>0.519177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch  Training Loss  Validation Loss        F1\n",
       "0       1         0.3275         0.216362  0.219888\n",
       "2       2         0.1372         0.153524  0.431144\n",
       "4       3         0.0923         0.145297  0.486060\n",
       "6       4         0.0610         0.153476  0.499814\n",
       "8       5         0.0437         0.158121  0.514966\n",
       "10      6         0.0315         0.162517  0.519177"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/tnatvu/xlm-roberta-base-finetuned-panx-de/commit/392fb1439a34c700ee40c2b53da7b342f86aea5e\n",
    "# CommitInfo(commit_url='https://huggingface.co/tnatvu/xlm-roberta-base-absa/commit/287181a14c225d7508e274c215c266865d0fe071', commit_message='Training completed!', commit_description='', oid='287181a14c225d7508e274c215c266865d0fe071', pr_url=None, pr_revision=None, pr_num=None)\n",
    "\n",
    "df = pd.DataFrame(trainer.state.log_history)[['epoch','loss' ,'eval_loss','eval_f1']]\n",
    "df = df.rename(columns={\"epoch\":\"Epoch\",\"loss\": \"Training Loss\", \"eval_loss\": \"Validation Loss\", 'eval_f1':'F1'})\n",
    "df['Epoch'] = df[\"Epoch\"].apply(lambda x: round(x))\n",
    "df['Training Loss'] = df[\"Training Loss\"].ffill()\n",
    "df[['Validation Loss', 'F1']] = df[['Validation Loss', 'F1']].bfill().ffill()\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch==2.3.0.dev20240121 # this does not work\n",
    "\n",
    "# pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu # run this in CLI before running the notebook\n",
    "\n",
    "# import torch\n",
    "# if torch.backends.mps.is_available():\n",
    "#     mps_device = torch.device(\"mps\")\n",
    "#     x = torch.ones(1, device=mps_device)\n",
    "#     print (x)\n",
    "# else:\n",
    "#     print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. <a id='toc3_6_'></a>[Error analysis](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_with_label(batch):\n",
    "    # Convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # Pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        # Pass data through model  \n",
    "        output = trainer.model(input_ids, attention_mask)\n",
    "\n",
    "        # Logit.size: [batch_size, sequence_length, classes]\n",
    "        # Predict class with largest logit value on classes axis\n",
    "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "    # Calculate loss per token after flattening batch dimension with view\n",
    "    loss = cross_entropy(output.logits.view(-1, tags.num_classes), \n",
    "                         labels.view(-1), reduction=\"none\")\n",
    "    # Unflatten batch dimension and convert to numpy array\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "\n",
    "    return {\"loss\":loss, \"predicted_label\": predicted_label}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7cf22a0eb24a31ad1976211b21dd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_set = ds_encoded[\"validation\"]\n",
    "validation_set = validation_set.map(forward_pass_with_label, batched=True, batch_size=32)\n",
    "df_validation = validation_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = trainer.model.config.id2label\n",
    "index2tag[-100] = \"IGN\"\n",
    "df_validation[\"input_tokens\"] = df_validation[\"input_ids\"].apply(\n",
    "    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\n",
    "df_validation[\"predicted_label\"] = df_validation[\"predicted_label\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "df_validation[\"labels\"] = df_validation[\"labels\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "df_validation['loss'] = df_validation.apply(\n",
    "    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\n",
    "df_validation['predicted_label'] = df_validation.apply(\n",
    "    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\n",
    "\n",
    "df_tokens = df_validation.apply(pd.Series.explode)\n",
    "df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
    "df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\n",
    "# df_tokens.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1. <a id='toc3_6_1_'></a>[Model performance](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_validation_metrics = df_validation.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁The', '▁processo', 'r', '▁went', '▁on', '▁me', '▁', ',', '▁the', '▁fan', '▁went', '▁and', '▁the', '▁mother', 'board', '▁went', '▁', '.', '</s>']\n",
      "['IGN', 'O', 'B-NEG', 'IGN', 'O', 'O', 'O', 'O', 'IGN', 'O', 'B-NEG', 'O', 'O', 'O', 'B-NEG', 'IGN', 'O', 'O', 'IGN', 'IGN']\n",
      "['O', 'O', 'B-NEU', 'I-NEU', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEU', 'O', 'O', 'O', 'B-NEU', 'I-NEU', 'O', 'O', 'O', 'O']\n",
      "20\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "num =800\n",
    "print(df_validation_metrics.iloc[num]['input_tokens'])\n",
    "print(df_validation_metrics.iloc[num]['labels'])\n",
    "print(df_validation_metrics.iloc[num]['predicted_label'])\n",
    "print(len(df_validation_metrics.iloc[num]['input_tokens']))\n",
    "print(len(df_validation_metrics.iloc[num]['labels']))\n",
    "print(len(df_validation_metrics.iloc[num]['predicted_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁The', '▁processo', 'r', '▁went', '▁on', '▁me', '▁', ',', '▁the', '▁fan', '▁went', '▁and', '▁the', '▁mother', 'board', '▁went', '▁', '.', '</s>']\n",
      "['O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'B-NEG', 'O', 'O']\n",
      "['O', 'B-NEU', 'O', 'O', 'O', 'O', 'O', 'B-NEU', 'O', 'O', 'O', 'B-NEU', 'O', 'O']\n",
      "20\n",
      "14\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "df_validation_metrics['predicted_label'] = df_validation_metrics.apply(lambda x: [x['predicted_label'][i] for i,k in enumerate(x['labels']) if k != 'IGN' ], axis=1)\n",
    "df_validation_metrics['labels'] = df_validation_metrics.apply(lambda x: [k for i,k in enumerate(x['labels']) if k != 'IGN' ], axis=1)\n",
    "\n",
    "\n",
    "num =800\n",
    "print(df_validation_metrics.iloc[num]['input_tokens'])\n",
    "print(df_validation_metrics.iloc[num]['labels'])\n",
    "print(df_validation_metrics.iloc[num]['predicted_label'])\n",
    "print(len(df_validation_metrics.iloc[num]['input_tokens']))\n",
    "print(len(df_validation_metrics.iloc[num]['labels']))\n",
    "print(len(df_validation_metrics.iloc[num]['predicted_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score \n",
    "\n",
    "y_true = [['O', 'B-NEG', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'O', 'O', 'O', 'B-NEG', 'O', 'O']]\n",
    "y_pred = [['O', 'O', 'B-NEU', 'I-NEU', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEU', 'O', 'O', 'O']]\n",
    "\n",
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-CON       0.00      0.00      0.00        15\n",
      "       B-NEG       0.69      0.69      0.69       310\n",
      "       B-NEU       0.45      0.47      0.46       157\n",
      "       B-POS       0.71      0.76      0.73       312\n",
      "       I-CON       0.00      0.00      0.00         9\n",
      "       I-NEG       0.68      0.63      0.66       158\n",
      "       I-NEU       0.50      0.57      0.54       122\n",
      "       I-POS       0.71      0.53      0.61       158\n",
      "           O       0.99      0.99      0.99     14503\n",
      "\n",
      "    accuracy                           0.96     15744\n",
      "   macro avg       0.53      0.52      0.52     15744\n",
      "weighted avg       0.96      0.96      0.96     15744\n",
      "\n",
      "0.5191766911763827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tina.vu/work/fun/sentilens_env_wipp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/tina.vu/work/fun/sentilens_env_wipp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/tina.vu/work/fun/sentilens_env_wipp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "macro_f1 = f1_score(df_tokens['labels'], df_tokens['predicted_label'], average='macro')\n",
    "print(classification_report(df_tokens['labels'], df_tokens['predicted_label']))\n",
    "print(macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CON       0.00      0.00      0.00        15\n",
      "         NEG       0.64      0.66      0.65       310\n",
      "         NEU       0.39      0.45      0.42       157\n",
      "         POS       0.65      0.72      0.68       312\n",
      "\n",
      "   micro avg       0.59      0.63      0.61       794\n",
      "   macro avg       0.42      0.46      0.44       794\n",
      "weighted avg       0.58      0.63      0.60       794\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tina.vu/work/fun/sentilens_env_wipp/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report(df_validation_metrics['labels'], df_validation_metrics['predicted_label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2. <a id='toc3_6_2_'></a>[Group by word token](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.3. <a id='toc3_6_3_'></a>[Group by Tag ID](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"labels\")[[\"loss\"]] \n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)\n",
    "    .sort_values(by=\"mean\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n",
    "                      list(tag2index.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(df):\n",
    "    for _, row in df.iterrows():\n",
    "        labels, preds, tokens, losses = [], [], [], []\n",
    "        for i, mask in enumerate(row[\"attention_mask\"]):\n",
    "            if i not in {0, len(row[\"attention_mask\"])}:\n",
    "                labels.append(row[\"labels\"][i])\n",
    "                preds.append(row[\"predicted_label\"][i])\n",
    "                tokens.append(row[\"input_tokens\"][i])\n",
    "                losses.append(f\"{row['loss'][i]:.2f}\")\n",
    "        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n",
    "                               \"preds\": preds, \"losses\": losses}).T\n",
    "        yield df_tmp\n",
    "\n",
    "df_validation[\"total_loss\"] = df_validation[\"loss\"].apply(sum)\n",
    "df_tmp = df_validation.sort_values(by=\"total_loss\", ascending=False).head(3)\n",
    "\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample.T)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors could be from human / annotation errors: United Nations is ORG, not PER, similar to Central African Republic. This can happen as data was annotated using rule based, it is better with human annotations, but mistakes can always occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hide_output\n",
    "df_tmp = df_validation.loc[df_validation[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[Save model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_tokenizer.save_pretrained('tokenizer/xlmr_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained('model/xlm-roberta-base-absa_manual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[Load saved model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# Reload the model\n",
    "loaded_model = AutoModelForTokenClassification.from_pretrained('model/xlm-roberta-base-absa_manual', ignore_mismatched_sizes=True)#.to(device) #output_model_dir\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained('tokenizer/xlmr_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have some input data\n",
    "input_data = [\"I love a \\\"pc\\\" but I was ready for a change and tired of the windows system.\"]\n",
    "# input_token_span = [list(TreebankWordTokenizer().span_tokenize(x)) for x in input_data]\n",
    "# input_tokens = [[input_data[r][k[0]:k[1]] for k in row] for r, row in enumerate(input_token_span)]\n",
    "\n",
    "# Tokenize and get predictions\n",
    "inputs = loaded_tokenizer(input_data, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "# features = [dict(zip(inputs, t)) for t in zip(*inputs.values())]\n",
    "\n",
    "\n",
    "# input_ids = features[\"input_ids\"].to(device)\n",
    "# attention_mask = features[\"attention_mask\"].to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "  outputs = loaded_model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "predicted_label_idx = torch.argmax(outputs.logits, axis=-1).cpu().numpy()\n",
    "df_res = pd.DataFrame({'predicted_label': predicted_label_idx.tolist(), \n",
    "                      'input_ids': inputs['input_ids'].numpy().tolist()}\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = loaded_model.config.id2label\n",
    "index2tag[-100] = \"IGN\"\n",
    "df_res[\"input_tokens\"] = df_res[\"input_ids\"].apply(\n",
    "    lambda x: loaded_tokenizer.convert_ids_to_tokens(x))\n",
    "df_res[\"predicted_label_text\"] = df_res[\"predicted_label\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "df_res['predicted_label'] = df_res.apply(\n",
    "    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\n",
    "df_res['predicted_label_text'] = df_res.apply(\n",
    "    lambda x: x['predicted_label_text'][:len(x['input_ids'])], axis=1)\n",
    "\n",
    "df_res_tokens = df_res.apply(pd.Series.explode)\n",
    "# df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
    "# df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\n",
    "# # df_tokens.head(7)\n",
    "df_res_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=loaded_model, tokenizer=loaded_tokenizer, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"I love a \\\"pc\\\" but I was ready for a change and tired of the windows system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentilens_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
