{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size: 3em'>SentiLens - Uncover reviews' hidden emotion</div>\n",
    "\n",
    "__Prepared by:__ Tina Vu</br>\n",
    "__Date:__ 20231208</br>\n",
    "\n",
    "Employing aspect-based sentiment analysis (ABSA) to extract valuable feature insights from e-commerce product reviews, thereby empowering consumers to make more informed purchasing decisions and enhancing their overall user experience on the platform.\n",
    "\n",
    "Utilizing manually annotated reviews for aspect sentiment analysis to extract aspects and predict sentiments from reviews. This enables consumers to obtain a condensed overview of sentiments related to various product features, eliminating the need to delve into an extensive array of reviews. As a result, the decision-making process becomes more streamlined and user-friendly.\n",
    "\n",
    "__Approach:__\n",
    "\n",
    "ABSA\n",
    "\n",
    "__Phase:__\n",
    "1. Supervised ABSA (What, How)\n",
    "2. Unsupervised ABSA\n",
    "3. Add 'Why' into ABSA\n",
    "\n",
    "<div style='font-size: 2em'>Phase 1 - Aspect Extration</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Import & prepare dataset](#toc1_)    \n",
    "  - 1.1. [Import data](#toc1_1_)    \n",
    "  - 1.2. [Preparing dataset for modelling](#toc1_2_)    \n",
    "    - 1.2.1. [Unified BIO tagging encode](#toc1_2_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tina.vu/work/fun/brainstation_capstone_env/brainstation_capstone/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "en_stop_words = set(stopwords.words('english'))\n",
    "# nltk.download()\n",
    "\n",
    "from sklearn_crfsuite import CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Import & prepare dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Import data](#toc0_)\n",
    "\n",
    "We will load laptop reviews dataset with aspect term & sentiment annotations.\n",
    "\n",
    "The dataset comes in two parts:\n",
    "- train: 3,048 records\n",
    "- test: 800 records\n",
    "\n",
    "Each record is a sentence with zero, one or multiple aspect terms. Each aspect term has the following features:\n",
    "- start character index\n",
    "- end character index\n",
    "- sentiment/ polarity (positive or negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3048, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_json('data/laptop/train.json')\n",
    "df_val = pd.read_json('data/laptop/validate.json')\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[{'term': 'cord', 'polarity': 'neutral', 'from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>812</td>\n",
       "      <td>I bought a HP Pavilion DV4-1222nr laptop and h...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>[{'term': 'service center', 'polarity': 'negat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2328</td>\n",
       "      <td>I investigated netbooks and saw the Toshiba NB...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2193</td>\n",
       "      <td>The other day I had a presentation to do for a...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  \\\n",
       "0  2339  I charge it at night and skip taking the cord ...   \n",
       "1   812  I bought a HP Pavilion DV4-1222nr laptop and h...   \n",
       "2  1316  The tech guy then said the service center does...   \n",
       "3  2328  I investigated netbooks and saw the Toshiba NB...   \n",
       "4  2193  The other day I had a presentation to do for a...   \n",
       "\n",
       "                                             aspects  \n",
       "0  [{'term': 'cord', 'polarity': 'neutral', 'from...  \n",
       "1  [{'term': '', 'polarity': '', 'from': 0, 'to':...  \n",
       "2  [{'term': 'service center', 'polarity': 'negat...  \n",
       "3  [{'term': '', 'polarity': '', 'from': 0, 'to':...  \n",
       "4  [{'term': '', 'polarity': '', 'from': 0, 'to':...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Preparing dataset for modelling](#toc0_)\n",
    "\n",
    "The task we are solving is Named Entity Recognition (NER) which is a sequential labeling task, a.k.a we would like to predict whether a token (word) in each sentence is part of an aspect term or not.\n",
    "\n",
    "In order to prepare the data for NER task, we need to label our tokens. Here, I implemented a unified BIO tagging technique which combines aspect boundaries and aspect sentiment.\n",
    "\n",
    "Word boundaries:\n",
    "- B: indicates the 1st word in the aspect term\n",
    "- I: indicates the subsequent word in the aspect term\n",
    "- O: indicates words that are not part of any aspect term\n",
    "\n",
    "Aspect sentiment:\n",
    "- POS: positive\n",
    "- NEU: neutral\n",
    "- NEG: conflict\n",
    "\n",
    "This BIO label technique is more effective in recognizing unigram and n-gram aspect terms comparing to a binary classification (whether a token is part of an aspect). By using a unified a approach, we can combine two tasks: aspect extraction and sentiment classification into one task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3036, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, I will need to drop some duplicated data in our training dataset, as identified in the EDA process.\n",
    "df_train.drop_duplicates(subset='text', inplace=True)\n",
    "\n",
    "# We have removed 12 duplicated records in our training dataset\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. <a id='toc1_2_1_'></a>[BIO tagging encode](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I defined a function to encode our sentences' aspects using a unified BIO tagging technique (<a href='https://arxiv.org/pdf/1811.05082.pdf'>reference</a>) that combines aspect boundaries and aspect sentiment in a single label.\n",
    "\n",
    "Word boundaries:\n",
    "- B: indicates the 1st word in the aspect term\n",
    "- I: indicates the subsequent word in the aspect term\n",
    "- O: indicates words that are not part of any aspect term\n",
    "\n",
    "Aspect sentiment:\n",
    "- POS: positive\n",
    "- NEU: neutral\n",
    "- NEG: conflict\n",
    "\n",
    "Unified BIO tagging will be like: B-NEU, I-NEU\n",
    "\n",
    "For example:\n",
    "['I', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'cord', 'with', 'me', 'because', 'of', 'the', 'good', 'battery', 'life', '.']\n",
    "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEU', 'I-NEU', 'O', 'O', 'O', 'O', 'O', 'B-POS', 'I-POS', 'I-POS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find word index by character index\n",
    "Since aspect terms are denoted using character index in a sentence, I need to figure out the word index in the sentence which is the unit I am going to work with for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_index(sentence, char_index):\n",
    "    ''''\n",
    "    Find word index in a sentence base on the character index in a sentence\n",
    "\n",
    "    -------------------------------\n",
    "    Parameters:\n",
    "    -------------------------------\n",
    "    sentence: str\n",
    "      a sentence in string format\n",
    "      e.g. 'I love pizza'\n",
    "\n",
    "    char_index: int\n",
    "      index of the character which can be a beginning, mid, or end of a word that you are searching for\n",
    "\n",
    "    -------------------------------\n",
    "    Return:\n",
    "    -------------------------------\n",
    "    word_index: int\n",
    "      index of the word which contains the character of char_index in the sentence\n",
    "    '''\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # for i, _ in enumerate(words): # Loop through all words in a sentence\n",
    "    #   total_chars = -1\n",
    "    #   for w in words[:i+1]:# for each word from beginning of the sentence to word ith\n",
    "    #     total_chars += len(w) + 1\n",
    "    #     if char_index <= total_chars:\n",
    "    #       return i\n",
    "    # raise Exception(f'char_index({char_index}) > sentence length {len(sentence)} in sentence: \"{sentence}\"')\n",
    "       \n",
    "\n",
    "    return next((i for i, word in enumerate(words) if (char_index - sum(len(w) + 1 for w in words[:i])) < len(word)), len(words) - 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect term unified BIO encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_unified_BIO (x, sentiment_tag=False):\n",
    "  '''  This function puts aspect's details into a dictionary, and multiple aspect as an array\n",
    "  \n",
    "  Parameter:\n",
    "  - ASPECTS: dict array\n",
    "      term: string array\n",
    "      polarity: string array\n",
    "      from: integer array\n",
    "      to: integer array\n",
    "\n",
    "    For example:\n",
    "    [\n",
    "      {'term':'cord', 'polarity':'neutral', 'from': 41, 'to': 45},\n",
    "      {'term':'battery life', 'polarity':'positive', 'from': 74, 'to': 86}\n",
    "    ]\n",
    "      \n",
    "  Output:\n",
    "  - PAIRS: dictionary array\n",
    "    dictionary of\n",
    "    - term\n",
    "    - polarity\n",
    "    - term_start\n",
    "    - term_end\n",
    "\n",
    "    For example:\n",
    "    [\n",
    "      {'term':'cord', 'polarity':'neutral', 'from': 41, 'to': 45},\n",
    "      {'term':'battery life', 'polarity':'positive', 'from': 74, 'to': 86}\n",
    "    ]\n",
    "  '''\n",
    "\n",
    "  pairs=x['aspects']\n",
    "\n",
    "  text = x['text']\n",
    "  text_token = wordpunct_tokenize(text)\n",
    "\n",
    "  \n",
    "  aspect_encode = ['O'] * len(text_token)\n",
    "  check_raw = []\n",
    "\n",
    "  check_count = 0\n",
    "  list_of_terms = ''\n",
    "\n",
    "  sentiment_tag_map = {'neutral': '-NEU','conflict':'-NEU', 'positive':'-POS', 'negative':'-NEG', '':''}\n",
    "\n",
    "  pairs = sorted(pairs, key=lambda d: int(d['from'])) \n",
    "\n",
    "  for i, k in enumerate(pairs):\n",
    "    list_of_terms += ' ' + k['term']\n",
    "    if k['term'] != '':\n",
    "      polarity = sentiment_tag_map[k['polarity']] if sentiment_tag == True else '' \n",
    "      i_from = int(k['from']) + i*20\n",
    "      i_to = int(k['to']) + i*20\n",
    "      text = text[:i_from] + ' XXATBXX'+ polarity + text[i_from:i_to] + 'XXATEXX ' + text[i_to:]\n",
    "\n",
    "\n",
    "  clean_text = re.sub('XXATBXX-\\\\w\\\\w\\\\w', '',text).replace('XXATEXX','')\n",
    "  text_token = word_tokenize(clean_text )\n",
    "  \n",
    "  text_term_token = word_tokenize(text)\n",
    "\n",
    "  tags = []\n",
    "  start_tag = False\n",
    "\n",
    "  polarity = ''\n",
    "  for k in text_term_token:\n",
    "    tag = 'O'\n",
    "    \n",
    "    \n",
    "    if k[:7] == 'XXATBXX':\n",
    "      start_tag = True\n",
    "      \n",
    "      polarity = k[7:11]\n",
    "      tag = 'B' + polarity\n",
    "      \n",
    "    elif start_tag == True:\n",
    "      \n",
    "      tag = 'I' + polarity\n",
    "      \n",
    "    else:\n",
    "      tag = 'O'\n",
    "\n",
    "\n",
    "    tags.append(tag)\n",
    "    \n",
    "    if k[-7:] == 'XXATEXX':\n",
    "      start_tag = False\n",
    "      polarity = ''\n",
    "    \n",
    "\n",
    "  list_of_terms_compute = ''\n",
    "  for i, k in enumerate(tags):\n",
    "    if k in ('B-POS','B-NEU','B-NEG','I-POS','I-NEU','I-NEG'):\n",
    "      list_of_terms_compute += ' ' + re.sub('XXATBXX-\\\\w\\\\w\\\\w', '', text_term_token[i]).replace('XXATEXX','')\n",
    "    \n",
    "  pairs = x['aspects']\n",
    "  \n",
    "  aspect_encode = tags\n",
    "  check_raw = (list_of_terms, list_of_terms_compute)\n",
    "  check_count = 1 if (list_of_terms != list_of_terms_compute) & (not (list_of_terms in ['',' '])) else 0\n",
    "  return pd.Series([pairs, text_token, aspect_encode, check_raw, check_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'admakfmdsk'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('XXATBXX-\\\\w\\\\w\\\\w', '', 'XXATBXX-NEUadmakfmdsk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = df_train.copy()\n",
    "# df_test = df_train.loc[df_train['id']==2339].copy()\n",
    "# df_test[['pairs','text_token','aspect_encode','check_raw','check_count']] = df_test.apply(lambda x: encode_unified_BIO(x, True), axis=1)\n",
    "df_train[['pairs','text_token','aspect_encode','check_raw','check_count']] = df_train.apply(lambda x: encode_unified_BIO(x, True), axis=1)\n",
    "df_val[['pairs','text_token','aspect_encode','check_raw','check_count']] = df_val.apply(lambda x: encode_unified_BIO(x, True), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print((df_train['check_count']>0).sum())\n",
    "print((df_val['check_count']>0).sum())\n",
    "# df_test[df_test['check_count']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>pairs</th>\n",
       "      <th>text_token</th>\n",
       "      <th>aspect_encode</th>\n",
       "      <th>check_raw</th>\n",
       "      <th>check_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2339</td>\n",
       "      <td>I charge it at night and skip taking the cord ...</td>\n",
       "      <td>[{'term': 'cord', 'polarity': 'neutral', 'from...</td>\n",
       "      <td>[{'term': 'cord', 'polarity': 'neutral', 'from...</td>\n",
       "      <td>[I, charge, it, at, night, and, skip, taking, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NEU, O, O, O, O,...</td>\n",
       "      <td>( cord battery life,  cord battery life)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>812</td>\n",
       "      <td>I bought a HP Pavilion DV4-1222nr laptop and h...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "      <td>[I, bought, a, HP, Pavilion, DV4-1222nr, lapto...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>( , )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1316</td>\n",
       "      <td>The tech guy then said the service center does...</td>\n",
       "      <td>[{'term': 'service center', 'polarity': 'negat...</td>\n",
       "      <td>[{'term': 'service center', 'polarity': 'negat...</td>\n",
       "      <td>[The, tech, guy, then, said, the, service, cen...</td>\n",
       "      <td>[O, B-NEU, I-NEU, O, O, O, B-NEG, I-NEG, O, O,...</td>\n",
       "      <td>( tech guy service center \"sales\" team,  tech ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2328</td>\n",
       "      <td>I investigated netbooks and saw the Toshiba NB...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "      <td>[I, investigated, netbooks, and, saw, the, Tos...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>( , )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2193</td>\n",
       "      <td>The other day I had a presentation to do for a...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "      <td>[{'term': '', 'polarity': '', 'from': 0, 'to':...</td>\n",
       "      <td>[The, other, day, I, had, a, presentation, to,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>( , )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  \\\n",
       "0  2339  I charge it at night and skip taking the cord ...   \n",
       "1   812  I bought a HP Pavilion DV4-1222nr laptop and h...   \n",
       "2  1316  The tech guy then said the service center does...   \n",
       "3  2328  I investigated netbooks and saw the Toshiba NB...   \n",
       "4  2193  The other day I had a presentation to do for a...   \n",
       "\n",
       "                                             aspects  \\\n",
       "0  [{'term': 'cord', 'polarity': 'neutral', 'from...   \n",
       "1  [{'term': '', 'polarity': '', 'from': 0, 'to':...   \n",
       "2  [{'term': 'service center', 'polarity': 'negat...   \n",
       "3  [{'term': '', 'polarity': '', 'from': 0, 'to':...   \n",
       "4  [{'term': '', 'polarity': '', 'from': 0, 'to':...   \n",
       "\n",
       "                                               pairs  \\\n",
       "0  [{'term': 'cord', 'polarity': 'neutral', 'from...   \n",
       "1  [{'term': '', 'polarity': '', 'from': 0, 'to':...   \n",
       "2  [{'term': 'service center', 'polarity': 'negat...   \n",
       "3  [{'term': '', 'polarity': '', 'from': 0, 'to':...   \n",
       "4  [{'term': '', 'polarity': '', 'from': 0, 'to':...   \n",
       "\n",
       "                                          text_token  \\\n",
       "0  [I, charge, it, at, night, and, skip, taking, ...   \n",
       "1  [I, bought, a, HP, Pavilion, DV4-1222nr, lapto...   \n",
       "2  [The, tech, guy, then, said, the, service, cen...   \n",
       "3  [I, investigated, netbooks, and, saw, the, Tos...   \n",
       "4  [The, other, day, I, had, a, presentation, to,...   \n",
       "\n",
       "                                       aspect_encode  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, B-NEU, O, O, O, O,...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, B-NEU, I-NEU, O, O, O, B-NEG, I-NEG, O, O,...   \n",
       "3                        [O, O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                           check_raw  check_count  \n",
       "0           ( cord battery life,  cord battery life)            0  \n",
       "1                                              ( , )            0  \n",
       "2  ( tech guy service center \"sales\" team,  tech ...            1  \n",
       "3                                              ( , )            0  \n",
       "4                                              ( , )            0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The tech guy then said the service center does not do 1-to-1 exchange and I have to direct my concern to the \"sales\" team, which is the retail shop which I bought my netbook from.']\n",
      "[['The', 'tech', 'guy', 'then', 'said', 'the', 'service', 'center', 'does', 'not', 'do', '1-to-1', 'exchange', 'and', 'I', 'have', 'to', 'direct', 'my', 'concern', 'to', 'the', '``', 'sales', \"''\", 'team', ',', 'which', 'is', 'the', 'retail', 'shop', 'which', 'I', 'bought', 'my', 'netbook', 'from', '.']]\n",
      "[[{'term': 'service center', 'polarity': 'negative', 'from': '27', 'to': '41'}, {'term': '\"sales\" team', 'polarity': 'negative', 'from': '109', 'to': '121'}, {'term': 'tech guy', 'polarity': 'neutral', 'from': '4', 'to': '12'}]]\n",
      "[(' tech guy service center \"sales\" team', \" tech guy service center  '' sales '' team\")]\n",
      "[['O', 'B-NEU', 'I-NEU', 'O', 'O', 'O', 'B-NEG', 'I-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NEG', 'I-NEG', 'I-NEG', 'I-NEG', 'I-NEG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "n = 1316\n",
    "\n",
    "\n",
    "print(df_train.loc[df_train['id']==n]['text'].tolist())\n",
    "print(df_train.loc[df_train['id']==n]['text_token'].tolist())\n",
    "print(df_train.loc[df_train['id']==n]['aspects'].tolist())\n",
    "print(df_train.loc[df_train['id']==n]['check_raw'].tolist())\n",
    "print(df_train.loc[df_train['id']==n]['aspect_encode'].tolist())\n",
    "# print(df_val.iloc[1668]['aspects'])\n",
    "# print(df_val.iloc[285]['pairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sentences into features\n",
    "def word2features(sent, i, window_size=5): \n",
    "    word = sent[i]\n",
    "\n",
    "    _, pos = zip(*nltk.pos_tag(sent))\n",
    "\n",
    "    window_size = int((window_size - 1)/ 2 if (window_size % 2) == 1 else window_size / 2)\n",
    "    \n",
    "    features = {\n",
    "        'word.lower()': word.lower(), # word\n",
    "        'word.index()': i,\n",
    "        'word.reverseindex()': len(sent) - 1 - i, # reverse index - nth word from end of sentence\n",
    "        'word.pos': pos[i],\n",
    "        'word.isstopword()': word in en_stop_words,\n",
    "        'word[-3:]': word[-3:], # last 4 char\n",
    "        'word[-2:]': word[-2:], # last 3 char - in case of -ing, -ion, etc.\n",
    "        'word.isupper()': word.isupper(), # is the word in upper case\n",
    "        'word.istitle()': word.istitle(), # is the first letter of the word in upper case\n",
    "        'word.isdigit()': word.isdigit(), # is the word full of digit\n",
    "        'word.isspecialchar()': re.sub('[^\\w,\\d,\\s]', '', word.lower()) == '', # is punctuation/ special characters\n",
    "    }\n",
    "    if i > 0:\n",
    "        for k in range(1, min(window_size, i)+1):\n",
    "            prev_word = sent[i - k]\n",
    "            prev_pos = pos[i - k]\n",
    "            \n",
    "            features.update({\n",
    "                f'-{k}:word.lower()': prev_word.lower(),\n",
    "                f'-{k}:word.pos': prev_pos,\n",
    "                f'-{k}:word.isstopword()': prev_word in en_stop_words,\n",
    "                f'-{k}:word.istitle()': prev_word.istitle(),\n",
    "                f'-{k}:word.isupper()': prev_word.isupper(),\n",
    "                f'-{k}:word.isspecialchar()': re.sub('[^\\w,\\d,\\s]', '', prev_word.lower()) == '', # is punctuation/ special characters\n",
    "            })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sentence\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        for k in range(1, min(window_size, len(sent) - i - 1)+1):\n",
    "            next_word = sent[i + k]\n",
    "            next_pos = pos[i + k]\n",
    "\n",
    "            features.update({\n",
    "                f'+{k}:word.lower()': next_word.lower(),\n",
    "                f'+{k}:word.pos': next_pos,\n",
    "                f'+{k}:word.isstopword()': next_word in en_stop_words,\n",
    "                f'+{k}:word.istitle()': next_word.istitle(),\n",
    "                f'+{k}:word.isupper()': next_word.isupper(),\n",
    "                f'-{k}:word.isspecialchar()': re.sub('[^\\w,\\d,\\s]', '', next_word.lower()) == '', # is punctuation/ special characters\n",
    "            })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to convert sentences into feature sequences\n",
    "def sent2features(sent, window_size=5):\n",
    "    return [word2features(sent, i, window_size) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = [sent2features(sentence, 5) for sentence in df_train['text_token']]\n",
    "y_train = df_train['aspect_encode']\n",
    "\n",
    "X_val = [sent2features(sentence,5) for sentence in df_val['text_token']]\n",
    "y_val = df_val['aspect_encode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = df_train[:1].copy()\n",
    "# test[['pairs','text_token','aspect_encode']] = test.apply(lambda x: encode_BIO(x, True), axis=1)\n",
    "# print(test.iloc[0]['pairs'])\n",
    "# print(test.iloc[0]['text_token'])\n",
    "# print(test.iloc[0]['aspect_encode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. POS:\n",
    "  - aspect - sentiment - neither\n",
    "  - context\n",
    "2. Word form (compact Xx)\n",
    "3. Word index\n",
    "4. Sentiment terms around aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "- Scale the data\n",
    "- Add regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert X_train to tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word.lower()</th>\n",
       "      <th>word.index()</th>\n",
       "      <th>word.reverseindex()</th>\n",
       "      <th>word.pos</th>\n",
       "      <th>word.isstopword()</th>\n",
       "      <th>word[-3:]</th>\n",
       "      <th>word[-2:]</th>\n",
       "      <th>word.isupper()</th>\n",
       "      <th>word.istitle()</th>\n",
       "      <th>word.isdigit()</th>\n",
       "      <th>...</th>\n",
       "      <th>-1:word.isstopword()</th>\n",
       "      <th>-1:word.istitle()</th>\n",
       "      <th>-1:word.isupper()</th>\n",
       "      <th>-2:word.lower()</th>\n",
       "      <th>-2:word.pos</th>\n",
       "      <th>-2:word.isstopword()</th>\n",
       "      <th>-2:word.istitle()</th>\n",
       "      <th>-2:word.isupper()</th>\n",
       "      <th>EOS</th>\n",
       "      <th>nth_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>PRP</td>\n",
       "      <td>False</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>charge</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>VBP</td>\n",
       "      <td>False</td>\n",
       "      <td>rge</td>\n",
       "      <td>ge</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>PRP</td>\n",
       "      <td>True</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>i</td>\n",
       "      <td>PRP</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>IN</td>\n",
       "      <td>True</td>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>charge</td>\n",
       "      <td>VBP</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>night</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "      <td>ght</td>\n",
       "      <td>ht</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>it</td>\n",
       "      <td>PRP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  word.lower()  word.index()  word.reverseindex() word.pos  word.isstopword()  \\\n",
       "0            i             0                   18      PRP              False   \n",
       "1       charge             1                   17      VBP              False   \n",
       "2           it             2                   16      PRP               True   \n",
       "3           at             3                   15       IN               True   \n",
       "4        night             4                   14       NN              False   \n",
       "\n",
       "  word[-3:] word[-2:]  word.isupper()  word.istitle()  word.isdigit()  ...  \\\n",
       "0         I         I            True            True           False  ...   \n",
       "1       rge        ge           False           False           False  ...   \n",
       "2        it        it           False           False           False  ...   \n",
       "3        at        at           False           False           False  ...   \n",
       "4       ght        ht           False           False           False  ...   \n",
       "\n",
       "   -1:word.isstopword() -1:word.istitle() -1:word.isupper() -2:word.lower()  \\\n",
       "0                   NaN               NaN               NaN             NaN   \n",
       "1                 False              True              True             NaN   \n",
       "2                 False             False             False               i   \n",
       "3                  True             False             False          charge   \n",
       "4                  True             False             False              it   \n",
       "\n",
       "  -2:word.pos -2:word.isstopword() -2:word.istitle()  -2:word.isupper()  EOS  \\\n",
       "0         NaN                  NaN               NaN                NaN  NaN   \n",
       "1         NaN                  NaN               NaN                NaN  NaN   \n",
       "2         PRP                False              True               True  NaN   \n",
       "3         VBP                False             False              False  NaN   \n",
       "4         PRP                 True             False              False  NaN   \n",
       "\n",
       "  nth_sentence  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rf = pd.DataFrame()\n",
    "\n",
    "def features2df(sentence_index, X):\n",
    "  df = pd.DataFrame(X)\n",
    "  df['nth_sentence'] = sentence_index\n",
    "  return df\n",
    "\n",
    "# Assuming X_train is a list of DataFrames\n",
    "X_train_rf = pd.concat([features2df(i,X) for i,X in enumerate(X_train)], ignore_index=True)\n",
    "X_train_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51138 entries, 0 to 51137\n",
      "Data columns (total 36 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   word.lower()             51138 non-null  object \n",
      " 1   word.index()             51138 non-null  int64  \n",
      " 2   word.reverseindex()      51138 non-null  int64  \n",
      " 3   word.pos                 51138 non-null  object \n",
      " 4   word.isstopword()        51138 non-null  int64  \n",
      " 5   word[-3:]                51138 non-null  object \n",
      " 6   word[-2:]                51138 non-null  object \n",
      " 7   word.isupper()           51138 non-null  int64  \n",
      " 8   word.istitle()           51138 non-null  int64  \n",
      " 9   word.isdigit()           51138 non-null  int64  \n",
      " 10  word.isspecialchar()     51138 non-null  int64  \n",
      " 11  BOS                      51138 non-null  float64\n",
      " 12  +1:word.lower()          51138 non-null  object \n",
      " 13  +1:word.pos              51138 non-null  object \n",
      " 14  +1:word.isstopword()     51138 non-null  float64\n",
      " 15  +1:word.istitle()        51138 non-null  float64\n",
      " 16  +1:word.isupper()        51138 non-null  float64\n",
      " 17  -1:word.isspecialchar()  51138 non-null  int64  \n",
      " 18  +2:word.lower()          51138 non-null  object \n",
      " 19  +2:word.pos              51138 non-null  object \n",
      " 20  +2:word.isstopword()     51138 non-null  float64\n",
      " 21  +2:word.istitle()        51138 non-null  float64\n",
      " 22  +2:word.isupper()        51138 non-null  float64\n",
      " 23  -2:word.isspecialchar()  51138 non-null  float64\n",
      " 24  -1:word.lower()          51138 non-null  object \n",
      " 25  -1:word.pos              51138 non-null  object \n",
      " 26  -1:word.isstopword()     51138 non-null  float64\n",
      " 27  -1:word.istitle()        51138 non-null  float64\n",
      " 28  -1:word.isupper()        51138 non-null  float64\n",
      " 29  -2:word.lower()          51138 non-null  object \n",
      " 30  -2:word.pos              51138 non-null  object \n",
      " 31  -2:word.isstopword()     51138 non-null  float64\n",
      " 32  -2:word.istitle()        51138 non-null  float64\n",
      " 33  -2:word.isupper()        51138 non-null  float64\n",
      " 34  EOS                      51138 non-null  float64\n",
      " 35  nth_sentence             51138 non-null  int64  \n",
      "dtypes: float64(15), int64(9), object(12)\n",
      "memory usage: 14.0+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train_rf.replace(True,1, inplace=True)\n",
    "X_train_rf.replace(False,0, inplace=True)\n",
    "X_train_rf.fillna(-1, inplace=True)\n",
    "X_train_rf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        word.lower()\n",
       "12    +1:word.lower()\n",
       "18    +2:word.lower()\n",
       "24    -1:word.lower()\n",
       "29    -2:word.lower()\n",
       "5           word[-3:]\n",
       "6           word[-2:]\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = pd.Series(X_train_rf.columns)\n",
    "drop_cols = cols[cols.str.contains('.*word\\.lower\\(\\)', regex=True)]\n",
    "drop_cols = pd.concat([drop_cols, cols[cols.str.contains('.*word\\[-\\d\\:]', regex=True)]])\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51138 entries, 0 to 51137\n",
      "Data columns (total 29 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   word.index()             51138 non-null  int64  \n",
      " 1   word.reverseindex()      51138 non-null  int64  \n",
      " 2   word.pos                 51138 non-null  object \n",
      " 3   word.isstopword()        51138 non-null  int64  \n",
      " 4   word.isupper()           51138 non-null  int64  \n",
      " 5   word.istitle()           51138 non-null  int64  \n",
      " 6   word.isdigit()           51138 non-null  int64  \n",
      " 7   word.isspecialchar()     51138 non-null  int64  \n",
      " 8   BOS                      51138 non-null  float64\n",
      " 9   +1:word.pos              51138 non-null  object \n",
      " 10  +1:word.isstopword()     51138 non-null  float64\n",
      " 11  +1:word.istitle()        51138 non-null  float64\n",
      " 12  +1:word.isupper()        51138 non-null  float64\n",
      " 13  -1:word.isspecialchar()  51138 non-null  int64  \n",
      " 14  +2:word.pos              51138 non-null  object \n",
      " 15  +2:word.isstopword()     51138 non-null  float64\n",
      " 16  +2:word.istitle()        51138 non-null  float64\n",
      " 17  +2:word.isupper()        51138 non-null  float64\n",
      " 18  -2:word.isspecialchar()  51138 non-null  float64\n",
      " 19  -1:word.pos              51138 non-null  object \n",
      " 20  -1:word.isstopword()     51138 non-null  float64\n",
      " 21  -1:word.istitle()        51138 non-null  float64\n",
      " 22  -1:word.isupper()        51138 non-null  float64\n",
      " 23  -2:word.pos              51138 non-null  object \n",
      " 24  -2:word.isstopword()     51138 non-null  float64\n",
      " 25  -2:word.istitle()        51138 non-null  float64\n",
      " 26  -2:word.isupper()        51138 non-null  float64\n",
      " 27  EOS                      51138 non-null  float64\n",
      " 28  nth_sentence             51138 non-null  int64  \n",
      "dtypes: float64(15), int64(9), object(5)\n",
      "memory usage: 11.3+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train_rf.drop(columns=drop_cols, inplace=True)\n",
    "X_train_rf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51138, 236)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rf = pd.get_dummies(X_train_rf, drop_first=True)\n",
    "X_train_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'B'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m labels \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-POS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI-POS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-NEU\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI-NEU\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-NEG\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI-NEG\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m6\u001b[39m}\n\u001b[0;32m----> 2\u001b[0m y_train_rf \u001b[38;5;241m=\u001b[39m [labels[y] \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m y_train \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m sentence]\n\u001b[1;32m      3\u001b[0m y_train_rf[:\u001b[38;5;241m20\u001b[39m]\n",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m labels \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-POS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI-POS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-NEU\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI-NEU\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-NEG\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI-NEG\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m6\u001b[39m}\n\u001b[0;32m----> 2\u001b[0m y_train_rf \u001b[38;5;241m=\u001b[39m [\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m y_train \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m sentence]\n\u001b[1;32m      3\u001b[0m y_train_rf[:\u001b[38;5;241m20\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'B'"
     ]
    }
   ],
   "source": [
    "labels = {\"B-POS\": 0, \"I-POS\": 1, \"B-NEU\": 2, \"I-NEU\": 3, \"B-NEG\": 4, \"I-NEG\": 5, \"O\":6}\n",
    "y_train_rf = [labels[y] for sentence in y_train for y in sentence]\n",
    "y_train_rf[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pred = cross_val_predict(RandomForestClassifier(n_estimators=20),X=X_train_rf.drop(columns='nth_sentence'), y=y_train_rf, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_pred=pred, y_true=y_train_rf)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train CRF model\n",
    "crf_model = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100)\n",
    "try:\n",
    "  crf_model.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def post_process_predictions(predictions):\n",
    "#   for i in range(1, len(predictions)):\n",
    "#       if predictions[i] == 'I' and predictions[i - 1] == 'O':\n",
    "#           predictions[i] = 'O'  # Change 'I' to 'O' if not preceded by 'B'\n",
    "#   return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random\n",
    "from highlight_text import HighlightText, ax_text, fig_text\n",
    "\n",
    "def vizualize_samples (sentences, pred_tags, tags = None):\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(30,10))\n",
    "  font = {'family' : 'arial',\n",
    "          'size'   : 16}\n",
    "  matplotlib.rc('font', **font)\n",
    "  final_text = []\n",
    "  color = []\n",
    "  pos_element = {\"bbox\": {\"edgecolor\": \"Green\", \"facecolor\": \"#99FF00\", \"linewidth\": 1.5, \"pad\": 1}} \n",
    "  neu_element = {\"bbox\": {\"edgecolor\": \"Orange\", \"facecolor\": \"Yellow\", \"linewidth\": 1.5, \"pad\": 1.5}} \n",
    "  neg_element = {\"bbox\": {\"edgecolor\": \"Red\", \"facecolor\": \"#FF99CC\", \"linewidth\": 1.5, \"pad\": 1}}\n",
    "  \n",
    "  color_map = {'POS': pos_element, 'NEU': neu_element, 'NEG':neg_element}\n",
    "\n",
    "  for s in range(0, len(sentences)):\n",
    "    final_text.append('(P) ')\n",
    "    chunk = []\n",
    "    next_tag = ''\n",
    "    \n",
    "    for w in range(0, len(sentences[s])):\n",
    "      # print(w)\n",
    "      word = sentences[s][w]\n",
    "      tag = pred_tags[s][w]\n",
    "      # print(tag[2:])  \n",
    "      if w + 1 < len(sentences[s]):\n",
    "        next_tag = pred_tags[s][w+1]\n",
    "      else:\n",
    "        next_tag = 'O'\n",
    "      \n",
    "      # print(word, tag, next_tag)\n",
    "\n",
    "      if tag == 'O':\n",
    "        final_text.append(word)\n",
    "      elif (tag in ['I-POS','I-NEG', 'I-NEU','B-POS','B-NEG', 'B-NEU']):\n",
    "        chunk.append(word)\n",
    "\n",
    "      if (next_tag in ['B-POS','B-NEG', 'B-NEU', 'O']) & (len(chunk) > 0):\n",
    "        final_text.append(f'  <{\" \".join(chunk)}>  ')\n",
    "        color.append(color_map[tag[2:]])\n",
    "        chunk = []\n",
    "\n",
    "    if tags is not None:\n",
    "      final_text.append('\\n')\n",
    "      final_text.append('(A) ')\n",
    "      for w in range(0, len(sentences[s])):\n",
    "        # print(w)\n",
    "        word = sentences[s][w]\n",
    "        tag = tags[s][w]\n",
    "        if w + 1 < len(sentences[s]):\n",
    "          next_tag = tags[s][w+1]\n",
    "        else:\n",
    "          next_tag = 'O'\n",
    "          \n",
    "        if tag == 'O':\n",
    "          final_text.append(word)\n",
    "        elif (tag in ['I-POS','I-NEG', 'I-NEU','B-POS','B-NEG', 'B-NEU']):\n",
    "          chunk.append(word)\n",
    "\n",
    "        if (next_tag in ['B-POS','B-NEG', 'B-NEU', 'O']) & (len(chunk) > 0):\n",
    "          final_text.append(f'  <{\" \".join(chunk)}>  ')\n",
    "          color.append(pos_element)\n",
    "          chunk = []\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # if tags is not None:\n",
    "    #   final_text.append('\\n')\n",
    "    #   final_text.append('(A) ')\n",
    "    #   for w in range(0, len(sentences[s])):\n",
    "    #     word = sentences[s][w]\n",
    "    #     tag = tags[s][w]\n",
    "\n",
    "    #     if tag !='O':\n",
    "    #       final_text.append('<{}>'.format(word))\n",
    "    #       if tag in ['I-POS','I-NEG', 'I-NEU']:\n",
    "    #         # color.append(color[-1])\n",
    "    #         color.append(pos_element)\n",
    "    #       else:\n",
    "    #         color.append ({'color':random.choice(['blue','green','red','magenta'])})\n",
    "    #     else:\n",
    "    #       final_text.append(word)\n",
    "    \n",
    "    final_text.append('\\n---------------\\n')\n",
    "\n",
    "\n",
    "    HighlightText(x=0, y=1,\n",
    "                s=' '.join(final_text),\n",
    "                highlight_textprops=color,\n",
    "                ax=ax)\n",
    "                \n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10\n",
    "integer = 285 #random.randint(0,500)\n",
    "tags = y_val[integer:integer+samples].array\n",
    "pred_tags = crf_model.predict(X_val[integer:integer+samples])\n",
    "sentences = [[x['word.lower()'] for x in sentence] for sentence in X_val[integer:integer+samples]]\n",
    "\n",
    "print(integer)\n",
    "vizualize_samples(sentences, pred_tags, tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "nth = integer + i\n",
    "print(nth)\n",
    "print(df_val.iloc[nth]['text'])\n",
    "\n",
    "print(df_val.iloc[nth]['aspects'])\n",
    "\n",
    "print(df_val.iloc[nth]['pairs'])\n",
    "\n",
    "print(pred_tags[i])\n",
    "print(tags[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with new data\n",
    "sample = pd.Series(['I was pleasantly surprised by the quality of this laptop for the money. '\n",
    "            ,'I am not super techy so it may be difficult for me to comment on the technical specifications of the laptop, but I was pleasantly surprised with the quality of the product especially at this price point.'])\n",
    "sample_text_token = [word_tokenize(sentence) for sentence in sample]\n",
    "X_sample = [sent2features(sentence, 5) for sentence in sample_text_token]\n",
    "\n",
    "predicted_labels = crf_model.predict(X_sample)\n",
    "\n",
    "vizualize_samples(sample_text_token, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "tags = y_train\n",
    "pred_tags = crf_model.predict(X_train)\n",
    "\n",
    "\n",
    "# Create a mapping of labels to indices\n",
    "labels = {\"B-POS\": 0, \"I-POS\": 1, \"B-NEU\": 2, \"I-NEU\": 3, \"B-NEG\": 4, \"I-NEG\": 5, \"O\":6}\n",
    "\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[x] for x in sum([tag for tag in pred_tags],[])])\n",
    "truths = np.array([labels[x] for x in sum([tag for tag in tags],[])])\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    target_names= [\"B-POS\", \"I-POS\", \"B-NEU\", \"I-NEU\", \"B-NEG\", \"I-NEG\", \"O\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tags = y_val\n",
    "pred_tags = crf_model.predict(X_val)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[x] for x in sum([tag for tag in pred_tags],[])])\n",
    "truths = np.array([labels[x] for x in sum([tag for tag in tags],[])])\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    target_names=[\"B-POS\", \"I-POS\", \"B-NEU\", \"I-NEU\", \"B-NEG\", \"I-NEG\", \"O\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Add more features:\n",
    "  - head words\n",
    "  - Google Word2Vec cluster_id\n",
    "  - Stemming / Lemming\n",
    "  - word index from beggining & ending of the sentence\n",
    "- Employ pre-trained word embeddings\n",
    "- Re-train model using rule-based aspect term extraction on larger dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainstation_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
