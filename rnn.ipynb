{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org/api/nltk.tokenize.treebank.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # Replace 'path/to/word2vec/model.bin' with the path to your pretrained Word2Vec model\n",
    "# model_path = 'path/to/word2vec/model.bin'\n",
    "# word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# # Access the vector for a specific word\n",
    "# vector_for_word = word2vec_model['example']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceToken:\n",
    "  '''\n",
    "    SentenceToken\n",
    "\n",
    "  '''\n",
    "  def __init__(self, sentence, aspect_type=None, aspects=None, sentence_id=None):\n",
    "    \n",
    "    if sentence_id is not None:\n",
    "      print(sentence_id)\n",
    "\n",
    "    self.sentence_id = sentence_id\n",
    "    self.sentence = sentence.replace(u\"\\u00A0\", \" \")\n",
    "                            \n",
    "    self.aspect_bio_tags = None\n",
    "    self.unified_aspect_bio_tags = None\n",
    "    self.token_span = None\n",
    "    self.space_pre_token = None\n",
    "\n",
    "    # Tokenize sentence\n",
    "    self.__tokenize_sentence(self.sentence)\n",
    "\n",
    "    if aspect_type == 'dict':\n",
    "      self.set_aspect_tagging_from_dict(aspects)\n",
    "    elif aspect_type == 'bio':\n",
    "      self.set_aspect_bio_tags(aspects)\n",
    "    elif aspect_type == 'unified bio':\n",
    "      self.set_aspect_unified_bio_tags(aspects)\n",
    "  \n",
    "  def __tokenize_sentence(self, sentence):\n",
    "    # self.sentence = sentence\n",
    "    \n",
    "    token_span = list(TreebankWordTokenizer().span_tokenize(sentence))\n",
    "\n",
    "    new_token_span = []\n",
    "    \n",
    "    for k in token_span:\n",
    "      token_start = k[0]\n",
    "      token_end = k[1]\n",
    "\n",
    "      token = sentence[token_start:token_end]\n",
    "      sub_tokens = re.split(r'([^\\w,\\d])', token)\n",
    "      \n",
    "      sub_token_start = token_start\n",
    "      for sub_token in sub_tokens:\n",
    "        if len(sub_token) != 0:\n",
    "          sub_token_end = sub_token_start + len(sub_token)\n",
    "          new_token_span.append((sub_token_start, sub_token_end))\n",
    "          sub_token_start = sub_token_end\n",
    "    \n",
    "    self.token_span = new_token_span\n",
    "    self.space_pre_token = [True if sentence[k[0]-1:k[0]] == ' ' else False for i,k in enumerate(new_token_span)]\n",
    "\n",
    "\n",
    "  def set_aspect_tagging_from_dict(self, aspects):\n",
    "    polarity_map = {'positive':'POS'\n",
    "              ,'negative': 'NEG'\n",
    "              ,'conflict': 'CON'\n",
    "              ,'neutral': 'NEU'}\n",
    "    \n",
    "    bio_tags = ['O'] * len(self.token_span)\n",
    "    unified_bio_tags = bio_tags\n",
    "\n",
    "    for x in aspects:\n",
    "      if x['term'] != '':\n",
    "        aspect_from = int(x['from'])\n",
    "        aspect_to = int(x['to'])\n",
    "        polarity = '-' + polarity_map[x['polarity']]\n",
    "        aspect_token_ids =  [i for i, v in enumerate(self.token_span) if (v[0] >= aspect_from) & (v[1] <= aspect_to)]\n",
    "        aspect_from_index = min(aspect_token_ids)\n",
    "        aspect_to_index = max(aspect_token_ids)\n",
    "        aspect_length = aspect_to_index - aspect_from_index\n",
    "        bio_tags = bio_tags[:aspect_from_index] + ['B'] + ['I'] * (aspect_length) + bio_tags[aspect_to_index+1:]\n",
    "        unified_bio_tags = unified_bio_tags[:aspect_from_index] + ['B' + polarity] + ['I'+ polarity] * (aspect_length) + unified_bio_tags[aspect_to_index+1:]\n",
    "    \n",
    "    self.set_aspect_bio_tags(bio_tags)\n",
    "    self.set_aspect_unified_bio_tags(unified_bio_tags)\n",
    "\n",
    "  def rebuild_sentence_from_token(self):\n",
    "    return ''.join([(' ' if self.space_pre_token[i] else '') + self.sentence[k[0]:k[1]] for i, k in enumerate(self.token_span)])\n",
    "  \n",
    "  def get_sentence_token_with_aspect_bio_tag(self, unified_bio_tag=False):\n",
    "    if (unified_bio_tag == False) & (self.aspect_bio_tags is None):\n",
    "      raise Exception('No BIO tags provided. Use \"SentenceToken.set_aspect_bio_tags()\" method to add bio_tags')\n",
    "    elif (unified_bio_tag == True) & (self.aspect_unified_bio_tags is None):\n",
    "      raise Exception('No Unified BIO tags provided. Use \"SentenceToken.set_aspect_unified_bio_tags()\" method to add unified_bio_tags')\n",
    "    else:\n",
    "      return [(self.sentence[k[0]:k[1]], self.aspect_unified_bio_tags[i] if unified_bio_tag else self.aspect_bio_tags[i]) for i, k in enumerate(self.token_span)]\n",
    "\n",
    "  def set_aspect_bio_tags(self, aspect_bio_tags):\n",
    "    self.aspect_bio_tags = aspect_bio_tags\n",
    "    self.aspect_unified_bio_tags = aspect_bio_tags\n",
    "\n",
    "  def set_aspect_unified_bio_tags(self, aspect_unified_bio_tags):\n",
    "    self.aspect_unified_bio_tags = aspect_unified_bio_tags\n",
    "    self.set_aspect_bio_tags([k[0:1] for k in aspect_unified_bio_tags])\n",
    "\n",
    "  def check_rebuild_sentence_from_token(self):\n",
    "    return re.sub(r'\\s+', ' ',self.sentence.strip()) == self.rebuild_sentence_from_token().strip()\n",
    "  \n",
    "  def get_tokens(self):\n",
    "    return [self.sentence[k[0]:k[1]] for k in self.token_span]\n",
    "\n",
    "  def check_rebuild_aspect_terms(self, aspect_dict):\n",
    "    aspect_dict = sorted(aspect_dict, key=lambda d: int(d['from']))\n",
    "    aspect_input = ', '.join([k['term'] for k in aspect_dict])\n",
    "    aspect_computed = ''.join([(', ' if k == 'B' else '') + self.sentence[self.token_span[i][0]:self.token_span[i][1]] for i,k in enumerate(self.aspect_bio_tags) if k in ['B','I'] ])[2:]\n",
    "    \n",
    "    return (aspect_input == aspect_computed, aspect_input, aspect_computed)\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.rebuild_sentence_from_token()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bio( sentence, aspects):\n",
    "\n",
    "\n",
    "  sentence_token = SentenceToken(sentence, 'dict', aspects)\n",
    "  setence_input = sentence_token.sentence\n",
    "  return pd.Series([setence_input, sentence_token.check_rebuild_sentence_from_token(), sentence_token.check_rebuild_aspect_terms(aspects), sentence_token])\n",
    "\n",
    "# check, token = encode_bio(df_train['text'][612], df_train['aspects'][612])\n",
    "# print(token)\n",
    "# print(token.get_sentence_token_with_aspect_bio_tag())\n",
    "# print(token.get_sentence_token_with_aspect_bio_tag(True))\n",
    "# token.check_rebuild_aspect_terms(df_train['aspects'][612])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape:  (3048, 3)\n",
      "df_val shape:  (800, 3)\n",
      "(3036, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_json('data/laptop/train.json')\n",
    "# df_train.set_index('id', inplace=True).reset_index()\n",
    "print('df_train shape: ', df_train.shape)\n",
    "\n",
    "df_val = pd.read_json('data/laptop/validate.json') # This will only be used for the very last step to evaluate how well the model is, but is input now for validating the BIO tagging to ensure the function works properly\n",
    "# df_val.set_index('id', inplace=True).reset_index()\n",
    "print('df_val shape: ', df_val.shape)\n",
    "\n",
    "# First, I will need to drop some duplicated data in our training dataset, as identified in the EDA process.\n",
    "df_train.drop_duplicates(subset='text', inplace=True)\n",
    "\n",
    "# We have removed 12 duplicated records in our training dataset\n",
    "print(df_train.shape)\n",
    "\n",
    "# df_train = df_train[~df_train['id'].isin([1650,1050])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['sentence_token'] = df_train.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "# df_val['sentence_token'] = df_val.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "\n",
    "df_train[['setence_input','sentence_check','aspect_check','sentence_token']] = df_train.apply(lambda x: encode_bio(x['text'], x['aspects']), axis=1)\n",
    "df_val[['setence_input','sentence_check','aspect_check','sentence_token']] = df_val.apply(lambda x: encode_bio(x['text'], x['aspects']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>setence_input</th>\n",
       "      <th>sentence_check</th>\n",
       "      <th>aspect_check</th>\n",
       "      <th>sentence_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, text, aspects, setence_input, sentence_check, aspect_check, sentence_token]\n",
       "Index: []"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[df_val['sentence_check']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HP Pavilion DV9000 Notebook PC      When I first got this computer, it really rocked.\n",
      "HP Pavilion DV9000 Notebook PC      When I first got this computer, it really rocked.\n",
      "HP Pavilion DV9000 Notebook PC When I first got this computer, it really rocked.\n",
      "[False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False]\n",
      "[(0, 2), (3, 11), (12, 18), (19, 27), (28, 30), (36, 40), (41, 42), (43, 48), (49, 52), (53, 57), (58, 66), (66, 67), (68, 70), (71, 77), (78, 84), (84, 85)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1260\n",
    "print(df_train[df_train['id']==n]['text'].values[0])\n",
    "print(df_train[df_train['id']==n]['setence_input'].values[0])\n",
    "print(df_train[df_train['id']==n]['sentence_token'].values[0])\n",
    "print(df_train[df_train['id']==n]['sentence_token'].values[0].space_pre_token)\n",
    "print(df_train[df_train['id']==n]['sentence_token'].values[0].token_span)\n",
    "sentence = ' One night I turned the freaking thing off after using it, the next day I turn it on, no GUI, screen all dark, power light steady, hard drive light steady and not flashing as it usually does.'\n",
    "s = True if sentence[1-1:1] == ' ' else False \n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_train['sentence_token'][15])\n",
    "# print(df_train['sentence_token'][15].get_sentence_token_with_aspect_bio_tag())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_token_span = []\n",
    "# sentence = 'Without a doubt, the **design* of this laptop is fantastic.'\n",
    "# token_span  = [(21,30)]\n",
    "# for k in token_span:\n",
    "#   token_start = k[0]\n",
    "#   token_end = k[1]\n",
    "\n",
    "#   token = sentence[token_start:token_end]\n",
    "#   sub_tokens = re.split(r'([^\\w,\\d])', token)\n",
    "  \n",
    "#   sub_token_start = token_start\n",
    "#   for sub_token in sub_tokens:\n",
    "#     if len(sub_token) != 0:\n",
    "#       sub_token_end = sub_token_start + len(sub_token)\n",
    "#       new_token_span.append((sub_token_start, sub_token_end))\n",
    "#       sub_token_start = sub_token_end\n",
    "\n",
    "#   # new_token_span.append((sub_token_start, token_end))\n",
    "\n",
    "# print(new_token_span)\n",
    "# print([sentence[k[0]:k[1]] for k in new_token_span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = s['text'].values[0]\n",
    "# t[24:25]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentilens_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
