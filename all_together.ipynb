{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Data preparation](#toc1_)    \n",
    "  - 1.1. [Tokenize sentence and aspect BIO encoding class](#toc1_1_)    \n",
    "  - 1.2. [Load data](#toc1_2_)    \n",
    "  - 1.3. [Inspect tagging issues](#toc1_3_)    \n",
    "  - 1.4. [Merge conflict to negative & remove neutral](#toc1_4_)    \n",
    "  - 1.5. [Convert df to HuggingFace datasets](#toc1_5_)    \n",
    "  - 1.6. [Word features](#toc1_6_)    \n",
    "- 2. [EDA](#toc2_)    \n",
    "- 3. [Model performance class](#toc3_)    \n",
    "- 4. [Random forest](#toc4_)    \n",
    "- 5. [CRF](#toc5_)    \n",
    "- 6. [Bi-LSTM](#toc6_)    \n",
    "- 7. [BERT](#toc7_)    \n",
    "  - 7.1. [Model](#toc7_1_)    \n",
    "  - 7.2. [Data preparation](#toc7_2_)    \n",
    "  - 7.3. [Upsampling / downsampling](#toc7_3_)    \n",
    "  - 7.4. [Model tuning](#toc7_4_)    \n",
    "  - 7.5. [Error analysis](#toc7_5_)    \n",
    "    - 7.5.1. [Group by word token](#toc7_5_1_)    \n",
    "    - 7.5.2. [Group by Tag ID](#toc7_5_2_)    \n",
    "  - 7.6. [Load saved model](#toc7_6_)    \n",
    "    - 7.6.1. [Load model manually](#toc7_6_1_)    \n",
    "    - 7.6.2. [Pipeline](#toc7_6_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0' # this setting is needed to run NN on my Mac\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from highlight_text import HighlightText, ax_text, fig_text\n",
    "\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "from sklearn.metrics import f1_score as sklearn_f1_score\n",
    "\n",
    "# pip install torch==2.2.0 torchtext --index-url https://download.pytorch.org/whl/test/cpu\n",
    "# pip install torch==2.3.0.dev20240121 # this does not work\n",
    "\n",
    "# pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu # run this in CLI before running the notebook\n",
    "\n",
    "# import torch\n",
    "# if torch.backends.mps.is_available():\n",
    "#     mps_device = torch.device(\"mps\")\n",
    "#     x = torch.ones(1, device=mps_device)\n",
    "#     print (x)\n",
    "# else:\n",
    "#     print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Data preparation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Tokenize sentence and aspect BIO encoding class](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceToken:\n",
    "  '''\n",
    "    SentenceToken\n",
    "\n",
    "    This class takes care of word tokenize and tagging aspect entities\n",
    "  '''\n",
    "  def __init__(self, sentence, aspect_type=None, aspects=None, sentence_id=None):\n",
    "    \n",
    "    if sentence_id is not None:\n",
    "      print(sentence_id)\n",
    "\n",
    "    self.sentence_id = sentence_id\n",
    "    self.sentence = sentence.replace(u\"\\u00A0\", \" \").replace(u'\\xa0',' ') # replace unicode space character\n",
    "                            \n",
    "    self.aspect_bio_tags = None\n",
    "    self.unified_aspect_bio_tags = None\n",
    "    self.token_span = None\n",
    "    self.space_pre_token = None\n",
    "\n",
    "    # Tokenize sentence\n",
    "    self.__tokenize_sentence(self.sentence)\n",
    "\n",
    "    if aspect_type == 'dict':\n",
    "      self.set_aspect_tagging_from_dict(aspects)\n",
    "    elif aspect_type == 'bio':\n",
    "      self.set_aspect_bio_tags(aspects)\n",
    "    elif aspect_type == 'unified bio':\n",
    "      self.set_aspect_unified_bio_tags(aspects)\n",
    "  \n",
    "  def __tokenize_sentence(self, sentence):\n",
    "    '''\n",
    "    __tokenize_sentence\n",
    "\n",
    "    Break sentence into word token span\n",
    "    '''\n",
    "    token_span = list(TreebankWordTokenizer().span_tokenize(sentence))\n",
    "    \n",
    "    self.token_span = token_span\n",
    "    self.space_pre_token = [True if sentence[k[0]-1:k[0]] == ' ' else False for i,k in enumerate(token_span)]\n",
    "\n",
    "  def set_aspect_tagging_from_dict(self, aspects):\n",
    "    '''\n",
    "    set_aspect_tagging_from_dict\n",
    "\n",
    "    Calculate & assign aspect entities to token given an array of aspects (term, start_index, to_index, and polarity)\n",
    "    '''\n",
    "    polarity_map = {'positive':'POS'\n",
    "              ,'negative': 'NEG'\n",
    "              ,'conflict': 'CON'\n",
    "              ,'neutral': 'NEU'}\n",
    "    \n",
    "    bio_tags = ['O'] * len(self.token_span)\n",
    "    unified_bio_tags = bio_tags\n",
    "\n",
    "    for x in aspects:\n",
    "      if x['term'] != '':\n",
    "        aspect_from = int(x['from'])\n",
    "        aspect_to = int(x['to'])\n",
    "        polarity = '-' + polarity_map[x['polarity']]\n",
    "\n",
    "        aspect_from_index = [i for i, v in enumerate(self.token_span) if (v[0] <= aspect_from) & (v[1] >= aspect_from)][0]\n",
    "        aspect_to_index = [i for i, v in enumerate(self.token_span) if (v[0] <= aspect_to) & (v[1] >= aspect_to)][0]\n",
    "      \n",
    "        aspect_length = aspect_to_index - aspect_from_index\n",
    "        bio_tags = bio_tags[:aspect_from_index] + ['B'] + ['I'] * (aspect_length) + bio_tags[aspect_to_index+1:]\n",
    "        unified_bio_tags = unified_bio_tags[:aspect_from_index] + ['B' + polarity] + ['I'+ polarity] * (aspect_length) + unified_bio_tags[aspect_to_index+1:]\n",
    "\n",
    "    self.set_aspect_bio_tags(bio_tags)\n",
    "    self.set_aspect_unified_bio_tags(unified_bio_tags)\n",
    "\n",
    "  def rebuild_sentence_from_token(self):\n",
    "    '''\n",
    "    rebuild_sentence_from_token\n",
    "\n",
    "    Return sentence built from computed tokens\n",
    "    '''\n",
    "    return ''.join([(' ' if self.space_pre_token[i] else '') + self.sentence[k[0]:k[1]] for i, k in enumerate(self.token_span)])\n",
    " \n",
    "  def set_aspect_bio_tags(self, aspect_bio_tags):\n",
    "    '''\n",
    "    set_aspect_bio_tags\n",
    "\n",
    "    Setter method to set aspect_unified_bio_tags and aspect_bio_tags\n",
    "    '''\n",
    "    self.aspect_bio_tags = aspect_bio_tags\n",
    "    self.aspect_unified_bio_tags = aspect_bio_tags\n",
    "\n",
    "  def set_aspect_unified_bio_tags(self, aspect_unified_bio_tags):\n",
    "    ''''\n",
    "    set_aspect_unified_bio_tags\n",
    "    \n",
    "    Setter method to set aspect_unified_bio_tags and aspect_bio_tags\n",
    "    '''\n",
    "    self.aspect_unified_bio_tags = aspect_unified_bio_tags\n",
    "    self.aspect_bio_tags = [k[0:1] for k in aspect_unified_bio_tags]\n",
    "\n",
    "  def get_tokens(self):\n",
    "    '''\n",
    "    get_tokens()\n",
    "    Return an array of sentence word tokens\n",
    "    '''\n",
    "    return [self.sentence[k[0]:k[1]] for k in self.token_span]\n",
    "  \n",
    "  def check_rebuild_sentence_from_token(self):\n",
    "    '''\n",
    "    check_rebuild_sentence_from_token()\n",
    "\n",
    "    This is a test / debugger function.\n",
    "    This help validating if we have computed the sentence to token properly and whether we can re-compute the exact sentence from information stored.\n",
    "    '''\n",
    "    return re.sub(r'\\s+', ' ',self.sentence.strip()) == self.rebuild_sentence_from_token().strip()\n",
    "  \n",
    "  def check_rebuild_aspect_terms(self, aspect_dict):\n",
    "    '''\n",
    "    check_rebuild_aspect_terms(aspect_dict)\n",
    "\n",
    "    This is a test / debugger fucntion. \n",
    "    This help validate if we have compute the correct aspect terms as given by the aspect dict\n",
    "\n",
    "    INPUT:\n",
    "    aspect dict: array of aspect dictionaries in the following format\n",
    "      [{'term': 'storage', \n",
    "       'polarity': 'positive', \n",
    "       'from': '14', \n",
    "       'to': '21'}]\n",
    "    '''\n",
    "    aspect_dict = sorted(aspect_dict, key=lambda d: int(d['from']))\n",
    "    aspect_input = [k['term'].replace(u\"\\u00A0\", \" \").replace(u'\\xa0',' ') for k in aspect_dict if k['term'] != '' ]\n",
    "    aspect_computed = []\n",
    "    aspect = ''\n",
    "    \n",
    "    for i,k in enumerate(self.aspect_bio_tags):\n",
    "      token = self.sentence[self.token_span[i][0]:self.token_span[i][1]]\n",
    "      \n",
    "      if k == 'B':\n",
    "        if (self.aspect_bio_tags[i-1] == 'B' if i > 0 else False):\n",
    "          aspect_computed.append(aspect)\n",
    "        aspect = token\n",
    "      elif k == 'I':\n",
    "        aspect += ' ' * ((self.token_span[i][0] -  self.token_span[i-1][1]) if i > 0 else 0) + token\n",
    "      \n",
    "      if (aspect != '') & ((k == 'O') or (i == (len(self.aspect_bio_tags) - 1))):\n",
    "          aspect_computed.append(aspect)\n",
    "          aspect = ''\n",
    "\n",
    "    return [aspect_input == aspect_computed, aspect_input, aspect_computed]\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.rebuild_sentence_from_token()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Load data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_Data():\n",
    "  df_train = pd.read_json('data/laptop/train.json')\n",
    "  # First, I will need to drop some duplicated data in our training dataset, as identified in the EDA process.\n",
    "  # We have removed 12 duplicated records in our training dataset\n",
    "  df_train.drop_duplicates(subset='text', inplace=True)\n",
    "  print('df_train shape: ', df_train.shape)\n",
    "\n",
    "  df_val = pd.read_json('data/laptop/validate.json') # This will only be used for the very last step to evaluate how well the model is, but is input now for validating the BIO tagging to ensure the function works properly\n",
    "  print('df_val shape: ', df_val.shape)\n",
    "\n",
    "  df_train['sentence_token'] = df_train.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "  df_train['sentence_check'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_sentence_from_token(), axis=1)\n",
    "  df_train['aspect_check'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects']), axis=1)\n",
    "  df_train['aspect_check_TF'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects'])[0], axis=1)\n",
    "  df_train['tokens'] = df_train.apply(lambda x: x['sentence_token'].get_tokens(), axis=1)\n",
    "  df_train['tags'] = df_train.apply(lambda x: x['sentence_token'].aspect_unified_bio_tags, axis=1)\n",
    "\n",
    "  df_val['sentence_token'] = df_val.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "  df_val['sentence_check'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_sentence_from_token(), axis=1)\n",
    "  df_val['aspect_check'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects']), axis=1)\n",
    "  df_val['aspect_check_TF'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects'])[0], axis=1)\n",
    "  df_val['tokens'] = df_val.apply(lambda x: x['sentence_token'].get_tokens(), axis=1)\n",
    "  df_val['tags'] = df_val.apply(lambda x: x['sentence_token'].aspect_unified_bio_tags, axis=1)\n",
    "  return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape:  (3036, 3)\n",
      "df_val shape:  (800, 3)\n",
      "# of df_train records having tokenizing issues:  0\n",
      "# of df_train records having aspect bio tagging issues:  36\n",
      "# of df_test records having tokenizing issues:  0\n",
      "# of df_test records having aspect bio tagging issues:  9\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = reload_Data()\n",
    "print('# of df_train records having tokenizing issues: ', len(df_train[df_train['sentence_check']==False]))\n",
    "print('# of df_train records having aspect bio tagging issues: ', len(df_train[df_train['aspect_check_TF']==False]))\n",
    "print('# of df_test records having tokenizing issues: ', len(df_test[df_test['sentence_check']==False]))\n",
    "print('# of df_test records having aspect bio tagging issues: ', len(df_test[df_test['aspect_check_TF']==False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. <a id='toc1_3_'></a>[Inspect tagging issues](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([ 125,  140,  220,  293,  374,  375,  431,  612,  656,  834,  922,  924,\n",
      "        953,  999, 1031, 1374, 1456, 1502, 1631, 1716, 1936, 1958, 2113, 2160,\n",
      "       2244, 2392, 2502, 2533, 2587, 2606, 2783, 2831, 2842, 2876, 2930, 2940],\n",
      "      dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(df_train[df_train['aspect_check_TF']==False].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'term': 'delivery service', 'polarity': 'negative', 'from': '59', 'to': '75'}]\n",
      "After way too many times sending the thing in for repairs (delivery service was slow, and without the laptop I had no access to the internet, and thus no way of tracking it to find out when I might hope to see my computer again), it finally kicked the bucket after just over 2 years.\n",
      "[False, ['delivery service'], ['(delivery service']]\n",
      "['After', 'way', 'too', 'many', 'times', 'sending', 'the', 'thing', 'in', 'for', 'repairs', '(', 'delivery', 'service', 'was', 'slow', ',', 'and', 'without', 'the', 'laptop', 'I', 'had', 'no', 'access', 'to', 'the', 'internet', ',', 'and', 'thus', 'no', 'way', 'of', 'tracking', 'it', 'to', 'find', 'out', 'when', 'I', 'might', 'hope', 'to', 'see', 'my', 'computer', 'again', ')', ',', 'it', 'finally', 'kicked', 'the', 'bucket', 'after', 'just', 'over', '2', 'years', '.']\n"
     ]
    }
   ],
   "source": [
    "num = 2606\n",
    "\n",
    "print(df_train.loc[num]['aspects'])\n",
    "print(df_train.loc[num]['text'])\n",
    "print(df_train.loc[num]['aspect_check'])\n",
    "print(df_train.loc[num]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the tagging issues due to word that are not separated properly from special characters/ punctuations. The issue is unavoidable in practice as reviews may not adherent to perfect grammar.\n",
    "\n",
    "I have tried to fix this issues to have 100% accuracy with further token breakdown to match the specified aspect tokens, however, this can break some of the standard logics for word tokenizer and further modelling. \n",
    "\n",
    "Therefore, I decided to include a whole token where the aspect may start or end, even if the index is in the middle of token, which may results with aspect tokens that could include extra characters than planned. This is the risk we will accept for this approach, and we can perform a cleaning process to remove these extra characters during implementation with actual use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. <a id='toc1_4_'></a>[Merge conflict to negative & remove neutral](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tags'] = df_train['tags'].apply(lambda x: [tag[:2] + 'NEG' if tag[2:] == 'CON' else tag for tag in x ])\n",
    "df_test['tags'] = df_test['tags'].apply(lambda x: [tag[:2] + 'NEG' if tag[2:] == 'CON' else tag for tag in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['tags'] = df_train['tags'].apply(lambda x: ['O' if tag[2:] == 'NEU' else tag for tag in x])\n",
    "# df_test['tags'] = df_test['tags'].apply(lambda x: ['O' if tag[2:] == 'NEU' else tag for tag in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. <a id='toc1_5_'></a>[Convert df to HuggingFace datasets](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, ClassLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert aspect tag to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of unique aspect tags\n",
    "tags = list(set(sum(df_train['tags'],[])))\n",
    "tags.sort()\n",
    "\n",
    "tag2idx = {k:i for i,k in enumerate(tags)}\n",
    "idx2tag = {i:k for i,k in enumerate(tags)}\n",
    "\n",
    "# Convert aspect tag text to ids\n",
    "df_train['tags_idx'] = df_train['tags'].apply(lambda x: [tag2idx[k] for k in x])\n",
    "df_test['tags_idx'] = df_test['tags'].apply(lambda x: [tag2idx[k] for k in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-NEG',\n",
       " 1: 'B-NEU',\n",
       " 2: 'B-POS',\n",
       " 3: 'I-NEG',\n",
       " 4: 'I-NEU',\n",
       " 5: 'I-POS',\n",
       " 6: 'O'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NEG', 'B-NEU', 'B-POS', 'I-NEG', 'I-NEU', 'I-POS', 'O']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pandas to HuggingFace datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into train & validation set\n",
    "df_train_ori, df_val = train_test_split(df_train, test_size=0.3, random_state=42,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags_idx'],\n",
      "        num_rows: 2125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags_idx'],\n",
      "        num_rows: 911\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags_idx'],\n",
      "        num_rows: 800\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Define dataset features\n",
    "features = Features({'tokens': Sequence(Value(dtype='string', id=None)),\n",
    "                    'tags_idx': Sequence(ClassLabel(names=tags))\n",
    "                    })\n",
    "\n",
    "tds = Dataset.from_pandas(df_train_ori[['tokens','tags_idx']], features=features, preserve_index=False)\n",
    "vds = Dataset.from_pandas(df_val[['tokens','tags_idx']], features=features,  preserve_index=False)\n",
    "tsds = Dataset.from_pandas(df_test[['tokens','tags_idx']], features=features, preserve_index=False)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = tds\n",
    "ds['validation'] = vds\n",
    "ds['test'] = tsds\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for total counts per aspect type in each data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553f9c63d5144daaaecfe3910b3d9d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a95f20c68b41529a590e6f41953095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88144212f654726b5a5f690dee1fa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert aspect idx to aspect aspect text\n",
    "def create_tag_names(batch):\n",
    "    return {\"tags\": [ds[\"train\"].features[\"tags_idx\"].feature.int2str(idx) for idx in batch[\"tags_idx\"]]}\n",
    "\n",
    "ds = ds.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEG</th>\n",
       "      <th>POS</th>\n",
       "      <th>NEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>582</td>\n",
       "      <td>675</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>325</td>\n",
       "      <td>312</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>144</td>\n",
       "      <td>340</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NEG  POS  NEU\n",
       "train       582  675  304\n",
       "validation  325  312  157\n",
       "test        144  340  169"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform aspect type counts per each dataset split\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in ds.items():\n",
    "    for row in dataset[\"tags\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "                \n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. <a id='toc1_6_'></a>[Word features](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, opinion_lexicon\n",
    "import string\n",
    "\n",
    "# Load positive and negative words from the opinion lexicon\n",
    "POSITIVE_WORDS = set(opinion_lexicon.positive())\n",
    "NEGATIVE_WORDS = set(opinion_lexicon.negative())\n",
    "# Load common stop words in english\n",
    "EN_STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sentences into features\n",
    "def word2features(sent, word_idx, backward_window_size=5, forward_window_size=5): \n",
    "    word = sent[word_idx][0]\n",
    "\n",
    "    _, pos = zip(*nltk.pos_tag([x[0] for x in sent]))\n",
    "\n",
    "    tag_sentiment = lambda word: 'POS' if word in POSITIVE_WORDS else 'NEG' if word in NEGATIVE_WORDS else 'NEU'\n",
    "    features = {\n",
    "        'word.lower()': word.lower(), # word\n",
    "        'word.index()': word_idx,\n",
    "        'word.reverseindex()': len(sent) - 1 - word_idx, # reverse index - nth word from end of sentence\n",
    "        'word.pos': pos[word_idx],\n",
    "        'word.opinionlexicon': tag_sentiment(word.lower()),\n",
    "        'word.isstopword()': word.lower() in EN_STOP_WORDS,\n",
    "        'word[-3:]': word[-3:], # last 4 char\n",
    "        'word[-2:]': word[-2:], # last 3 char - in case of -ing, -ion, etc.\n",
    "        'word.isupper()': word.isupper(), # is the word in upper case\n",
    "        'word.istitle()': word.istitle(), # is the first letter of the word in upper case\n",
    "        'word.isdigit()': word.isdigit(), # is the word full of digit\n",
    "        'word.ispunctuation()': word.lower() in string.punctuation, # is punctuation\n",
    "    }\n",
    "\n",
    "    if word_idx > 0:\n",
    "        for k in range(1, min(backward_window_size, word_idx)+1):\n",
    "            prev_word = sent[word_idx - k][0]\n",
    "            prev_pos = pos[word_idx - k]\n",
    "            \n",
    "            features.update({\n",
    "                f'-{k}:word.lower()': prev_word.lower(),\n",
    "                f'-{k}:word.pos': prev_pos,\n",
    "                f'-{k}:word.opinionlexicon': tag_sentiment(word.lower()),\n",
    "                f'-{k}:word.isstopword()': prev_word in EN_STOP_WORDS,\n",
    "                f'-{k}:word.istitle()': prev_word.istitle(),\n",
    "                f'-{k}:word.isupper()': prev_word.isupper(),\n",
    "                f'-{k}:word.ispunctuation()': prev_word.lower() in string.punctuation, # is punctuation\n",
    "            })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sentence\n",
    "\n",
    "    if word_idx < len(sent) - 1:\n",
    "        for k in range(1, min(forward_window_size, len(sent) - word_idx - 1)+1):\n",
    "            next_word = sent[word_idx + k][0]\n",
    "            next_pos = pos[word_idx + k]\n",
    "\n",
    "            features.update({\n",
    "                f'+{k}:word.lower()': next_word.lower(),\n",
    "                f'+{k}:word.pos': next_pos,\n",
    "                f'+{k}:word.opinionlexicon': tag_sentiment(word.lower()),\n",
    "                f'+{k}:word.isstopword()': next_word in EN_STOP_WORDS,\n",
    "                f'+{k}:word.istitle()': next_word.istitle(),\n",
    "                f'+{k}:word.isupper()': next_word.isupper(),\n",
    "                f'+{k}:word.ispunctuation()': next_word.lower() in string.punctuation, # is punctuation\n",
    "            })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to convert sentences into feature sequences\n",
    "def sent2features(sent, backward_window_size=5, forward_window_size=5):\n",
    "    return [word2features(sent, i, backward_window_size, forward_window_size) for i in range(len(sent))]\n",
    "\n",
    "def get_features(example):\n",
    "    example['word_features'] = [ sent2features(sent) for sent in example['tokens']]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c483de91b55d49109447564e9c2bf657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732c67f1a540494396dc4a662b6dc9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1192231b5912429789fca68fdc5f9020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_features = ds.map(get_features, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[EDA](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Model performance class](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPerformanceLog:\n",
    "  \n",
    "  def __init__(self, y_true):\n",
    "    self.y_true = y_true\n",
    "    self.log = {}\n",
    "\n",
    "  def add_model_performance(self, model_name, y_pred, model=None): # Model can be path to the model or something like that\n",
    "    sklearn_f1_score, f1, clf_report = self.__calculate_metrics(y_pred)\n",
    "\n",
    "    model_perf = {'y_pred': y_pred\n",
    "                  ,'model': model\n",
    "                  ,'sklearn_f1_score': sklearn_f1_score.round(2)\n",
    "                  ,'f1': f1.round(4)\n",
    "                  ,'report': clf_report\n",
    "                  }\n",
    "    \n",
    "    self.log[model_name] = model_perf\n",
    "\n",
    "  def __calculate_sklearn_f1(self, y_pred):\n",
    "    y_pred = sum(y_pred,[])\n",
    "    y_true = sum(self.y_true,[])\n",
    "    return sklearn_f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "  def __calculate_metrics(self, y_pred):\n",
    "    return self.__calculate_sklearn_f1(y_pred), f1_score(self.y_true, y_pred), classification_report(self.y_true, y_pred, zero_division=0)\n",
    "\n",
    "  def get_logs(self):\n",
    "    logs_df = pd.DataFrame(list(self.log.values()), index=self.log.keys())\n",
    "    return logs_df[['sklearn_f1_score', 'f1']]\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = ds['validation'].to_pandas()['tags_idx']\n",
    "y_val = [[idx2tag[tag] for tag in sent] for sent in y_val]\n",
    "\n",
    "modelPerformanceLog = ModelPerformanceLog(y_val)\n",
    "# modelPerformanceLog.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[Random forest](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features2df(sent, sent_idx):\n",
    "  sent_df = pd.DataFrame(list(sent))\n",
    "  sent_df['sentence_idx'] = sent_idx\n",
    "\n",
    "  return sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags2df(tags, sent_idx):\n",
    "  tags_df = pd.DataFrame({'tags': tags})\n",
    "\n",
    "  tags_df['sentence_idx'] = sent_idx\n",
    "  return tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_data_prepare(ds, one_hot_encoder=None):\n",
    "  X_train = ds.to_pandas()['word_features']\n",
    "  X_train = pd.concat([features2df(sent, i) for i, sent in enumerate(X_train)], ignore_index=True)\n",
    "\n",
    "  X_train.replace(True,1, inplace=True)\n",
    "  X_train.replace(False,0, inplace=True)\n",
    "\n",
    "    # Fill NaN values in object-type columns with a 'missing'\n",
    "  object_columns = X_train.select_dtypes(include=['object']).columns\n",
    "  X_train[object_columns] = X_train[object_columns].fillna(value='missing')\n",
    "\n",
    "  # Fill the rest NaN with -1, since most of missing features are boolean\n",
    "  X_train = X_train.fillna(-1)\n",
    "\n",
    "  # We renove all categorical columns that are not POS, or sentiment lexicon + nth_sentence which the number of the sentence we we use to string back the data at the end, but is not needed for training\n",
    "  drop_cols = [x for x in list(X_train.columns) if re.match('(.*word\\.lower\\(\\))|(.*word\\[-\\d\\:])', x)] # word columns\n",
    "  X_train = X_train.drop(columns=drop_cols, axis=1)\n",
    "\n",
    "    # Perform one-hot-encoder on the reamining columns\n",
    "  if one_hot_encoder is None: # This is to make sure we use the same one-hot-encoder for both train & test split (avoiding data leakage)\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')  # 'drop' parameter is optional, set to 'first' to avoid multicollinearity\n",
    "    encoded_data = one_hot_encoder.fit_transform(X_train.select_dtypes(include=['object']))\n",
    "\n",
    "  else: # when we perform data prep for test data, we can reuse the one-hot-encoder used during training data preparation\n",
    "    encoded_data = one_hot_encoder.transform(X_train.select_dtypes(include=['object']))\n",
    "\n",
    "  # Re-added one-hot-encoding data back to the main dataframe\n",
    "  df_encoded = pd.DataFrame(encoded_data, columns=one_hot_encoder.get_feature_names_out(X_train.select_dtypes(include=['object']).columns))\n",
    "  X_train = pd.concat([X_train, df_encoded], axis=1)\n",
    "\n",
    "  # Dropped all the categorical features that have already been one-hot-encoded\n",
    "  X_train.drop(columns=X_train.select_dtypes(include=['object']).columns, inplace=True)\n",
    "\n",
    "  y_train =  pd.concat([tags2df(sent, i) for i, sent in enumerate(ds.to_pandas()['tags_idx'])], ignore_index=True)# sum(ds.to_pandas()['tags_idx'], [])\n",
    "  return X_train, y_train, one_hot_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tina.vu/work/fun/sentilens_env_wipp/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:228: UserWarning: Found unknown categories in columns [3, 5, 7, 9] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, one_hot_encoder = random_forest_data_prepare(ds_features['train'])\n",
    "X_val, y_val, _ = random_forest_data_prepare(ds_features['validation'], one_hot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop('sentence_idx', axis=1))\n",
    "X_val_scaled = scaler.transform(X_val.drop('sentence_idx', axis=1)) # Double check why we are seeing more columns in test????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=5, random_state=42)\n",
    "rf_classifier.fit(X_train_scaled, y_train['tags'])\n",
    "\n",
    "y_pred_rf = rf_classifier.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sklearn_f1_score</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sklearn_f1_score   f1\n",
       "random forest              0.14  0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add back to validation set to get sentence id\n",
    "y_val['predict'] = y_pred_rf \n",
    "\n",
    "# Collapse back to the required format\n",
    "y_pred_rf = y_val.groupby('sentence_idx')['predict'].agg(lambda x: x.tolist())\n",
    "\n",
    "# Convert to name lablel\n",
    "y_pred_rf = [[idx2tag[tag] for tag in sent] for sent in y_pred_rf]\n",
    "\n",
    "modelPerformanceLog.add_model_performance('random forest',y_pred_rf)\n",
    "modelPerformanceLog.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[CRF](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ds_features['train'].to_pandas()['word_features']\n",
    "y_train = ds_features['train'].to_pandas()['tags_idx']\n",
    "y_train = [[idx2tag[tag] for tag in sent] for sent in y_train]\n",
    "\n",
    "X_val = ds_features['validation'].to_pandas()['word_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train CRF model\n",
    "crf_model = CRF(algorithm='lbfgs',\n",
    "                max_iterations=100,\n",
    "                c1=0.5,\n",
    "                c2=0.05)\n",
    "\n",
    "# There is this error existing with this library: 'CRF' object has no attribute 'keep_tempfiles'\n",
    "# which has not been resolved and we can bypass it using this trick.\n",
    "try:\n",
    "  crf_model.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_crf = crf_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sklearn_f1_score</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crf</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sklearn_f1_score   f1\n",
       "random forest              0.14  0.0\n",
       "crf                        0.14  0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelPerformanceLog.add_model_performance('crf',y_pred_crf)\n",
    "modelPerformanceLog.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[Bi-LSTM](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Author: Robert Guthrie\n",
    "\n",
    "# import torch\n",
    "# import torch.autograd as autograd\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def argmax(vec):\n",
    "#     # return the argmax as a python int\n",
    "#     _, idx = torch.max(vec, 1)\n",
    "#     return idx.item()\n",
    "\n",
    "\n",
    "# def prepare_sequence(seq, to_ix):\n",
    "#     idxs = [to_ix[w] for w in seq]\n",
    "#     return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# # Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "# def log_sum_exp(vec):\n",
    "#     max_score = vec[0, argmax(vec)]\n",
    "#     max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "#     return max_score + \\\n",
    "#         torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "#     def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "#         super(BiLSTM_CRF, self).__init__()\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.tag_to_ix = tag_to_ix\n",
    "#         self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "#         self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "#                             num_layers=1, bidirectional=True)\n",
    "\n",
    "#         # Maps the output of the LSTM into tag space.\n",
    "#         self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "#         # Matrix of transition parameters.  Entry i,j is the score of\n",
    "#         # transitioning *to* i *from* j.\n",
    "#         self.transitions = nn.Parameter(\n",
    "#             torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "#         # These two statements enforce the constraint that we never transfer\n",
    "#         # to the start tag and we never transfer from the stop tag\n",
    "#         self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "#         self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "#         self.hidden = self.init_hidden()\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "#                 torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "#     def _forward_alg(self, feats):\n",
    "#         # Do the forward algorithm to compute the partition function\n",
    "#         init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "#         # START_TAG has all of the score.\n",
    "#         init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "#         # Wrap in a variable so that we will get automatic backprop\n",
    "#         forward_var = init_alphas\n",
    "\n",
    "#         # Iterate through the sentence\n",
    "#         for feat in feats:\n",
    "#             alphas_t = []  # The forward tensors at this timestep\n",
    "#             for next_tag in range(self.tagset_size):\n",
    "#                 # broadcast the emission score: it is the same regardless of\n",
    "#                 # the previous tag\n",
    "#                 emit_score = feat[next_tag].view(\n",
    "#                     1, -1).expand(1, self.tagset_size)\n",
    "#                 # the ith entry of trans_score is the score of transitioning to\n",
    "#                 # next_tag from i\n",
    "#                 trans_score = self.transitions[next_tag].view(1, -1)\n",
    "#                 # The ith entry of next_tag_var is the value for the\n",
    "#                 # edge (i -> next_tag) before we do log-sum-exp\n",
    "#                 next_tag_var = forward_var + trans_score + emit_score\n",
    "#                 # The forward variable for this tag is log-sum-exp of all the\n",
    "#                 # scores.\n",
    "#                 alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "#             forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "#         terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "#         alpha = log_sum_exp(terminal_var)\n",
    "#         return alpha\n",
    "\n",
    "#     def _get_lstm_features(self, sentence):\n",
    "#         self.hidden = self.init_hidden()\n",
    "#         embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "#         lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "#         lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "#         lstm_feats = self.hidden2tag(lstm_out)\n",
    "#         return lstm_feats\n",
    "\n",
    "#     def _score_sentence(self, feats, tags):\n",
    "#         # Gives the score of a provided tag sequence\n",
    "#         score = torch.zeros(1)\n",
    "#         tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "#         for i, feat in enumerate(feats):\n",
    "#             score = score + \\\n",
    "#                 self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "#         score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "#         return score\n",
    "\n",
    "#     def _viterbi_decode(self, feats):\n",
    "#         backpointers = []\n",
    "\n",
    "#         # Initialize the viterbi variables in log space\n",
    "#         init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "#         init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "#         # forward_var at step i holds the viterbi variables for step i-1\n",
    "#         forward_var = init_vvars\n",
    "#         for feat in feats:\n",
    "#             bptrs_t = []  # holds the backpointers for this step\n",
    "#             viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "#             for next_tag in range(self.tagset_size):\n",
    "#                 # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "#                 # previous step, plus the score of transitioning\n",
    "#                 # from tag i to next_tag.\n",
    "#                 # We don't include the emission scores here because the max\n",
    "#                 # does not depend on them (we add them in below)\n",
    "#                 next_tag_var = forward_var + self.transitions[next_tag]\n",
    "#                 best_tag_id = argmax(next_tag_var)\n",
    "#                 bptrs_t.append(best_tag_id)\n",
    "#                 viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "#             # Now add in the emission scores, and assign forward_var to the set\n",
    "#             # of viterbi variables we just computed\n",
    "#             forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "#             backpointers.append(bptrs_t)\n",
    "\n",
    "#         # Transition to STOP_TAG\n",
    "#         terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "#         best_tag_id = argmax(terminal_var)\n",
    "#         path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "#         # Follow the back pointers to decode the best path.\n",
    "#         best_path = [best_tag_id]\n",
    "#         for bptrs_t in reversed(backpointers):\n",
    "#             best_tag_id = bptrs_t[best_tag_id]\n",
    "#             best_path.append(best_tag_id)\n",
    "#         # Pop off the start tag (we dont want to return that to the caller)\n",
    "#         start = best_path.pop()\n",
    "#         assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "#         best_path.reverse()\n",
    "#         return path_score, best_path\n",
    "\n",
    "#     def neg_log_likelihood(self, sentence, tags):\n",
    "#         feats = self._get_lstm_features(sentence)\n",
    "#         forward_score = self._forward_alg(feats)\n",
    "#         gold_score = self._score_sentence(feats, tags)\n",
    "#         return forward_score - gold_score\n",
    "\n",
    "#     def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "#         # Get the emission scores from the BiLSTM\n",
    "#         lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "#         # Find the best path, given the features.\n",
    "#         score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "#         return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START_TAG = \"<START>\"\n",
    "# STOP_TAG = \"<STOP>\"\n",
    "# EMBEDDING_DIM = 5\n",
    "# HIDDEN_DIM = 4\n",
    "\n",
    "# # Make up some training data\n",
    "# training_data = [(\n",
    "#     \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "#     \"B I I I O O O B I O O\".split()\n",
    "# ), (\n",
    "#     \"georgia tech is a university in georgia\".split(),\n",
    "#     \"B I O O O O B\".split()\n",
    "# )]\n",
    "\n",
    "# word_to_ix = {}\n",
    "# for sentence, tags in training_data:\n",
    "#     for word in sentence:\n",
    "#         if word not in word_to_ix:\n",
    "#             word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "# tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# # Check predictions before training\n",
    "# with torch.no_grad():\n",
    "#     precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "#     precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "#     print(model(precheck_sent))\n",
    "\n",
    "# # Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "# for epoch in range(\n",
    "#         300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "#     for sentence, tags in training_data:\n",
    "#         # Step 1. Remember that Pytorch accumulates gradients.\n",
    "#         # We need to clear them out before each instance\n",
    "#         model.zero_grad()\n",
    "\n",
    "#         # Step 2. Get our inputs ready for the network, that is,\n",
    "#         # turn them into Tensors of word indices.\n",
    "#         sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "#         targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "#         # Step 3. Run our forward pass.\n",
    "#         loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "#         # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "#         # calling optimizer.step()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Check predictions after training\n",
    "# with torch.no_grad():\n",
    "#     precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "#     print(model(precheck_sent))\n",
    "# # We got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id='toc7_'></a>[BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 11:22:40.927536: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from transformers import AutoConfig, DistilBertConfig, AutoTokenizer, TrainingArguments, DataCollatorForTokenClassification, Trainer\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertModel\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertPreTrainedModel\n",
    "\n",
    "device = torch.device('mps') # This is required for Mac\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. <a id='toc7_1_'></a>[Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertForABSA(DistilBertPreTrainedModel):\n",
    "    config_class = DistilBertConfig\n",
    "\n",
    "    def __init__(self, config, architecture):\n",
    "        super().__init__(config)\n",
    "        # Roberta body\n",
    "        self.num_labels = config.num_labels\n",
    "        self.distilbert = DistilBertModel(config)#, add_pooling_layer=False)\n",
    "        self.architecture = architecture\n",
    "\n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(config.dropout)#hidden_dropout_prob)\n",
    "        \n",
    "\n",
    "        if architecture == 'additional_linear':\n",
    "            # Additional layer\n",
    "            self.additional_linear = nn.Linear(config.hidden_size, 10)\n",
    "            self.classifier = nn.Linear(10, config.num_labels)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Load and initialize weights from pretrained\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                labels=None, **kwargs):\n",
    "        \n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.distilbert(input_ids, attention_mask=attention_mask,\n",
    "                            #    token_type_ids=token_type_ids,\n",
    "                                 **kwargs)\n",
    "        \n",
    "        # Apply classifier to encoder representation (model head)\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "\n",
    "        if self.architecture == 'additional_linear':\n",
    "            # Additional layer\n",
    "            sequence_output = self.additional_linear(sequence_output)\n",
    "\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, \n",
    "                                     hidden_states=outputs.hidden_states, \n",
    "                                     attentions=outputs.attentions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. <a id='toc7_3_'></a>[Upsampling / downsampling](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Set the target number of samples per score\n",
    "# target_samples_per_score = 3000\n",
    "\n",
    "# # Use the custom sampling method\n",
    "# custom_sampled_df = custom_sampling_based_on_score(df_train_ori.copy(), 'nsCRD')\n",
    "\n",
    "# # Check the distribution of scores in the custom sampled DataFrame\n",
    "# # print(custom_sampled_df['entities_score'].value_counts())\n",
    "# print(len(custom_sampled_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform aspect type counts per each dataset split\n",
    "# split2freqs = defaultdict(Counter)\n",
    "\n",
    "# for row in df_train_ori[\"tags\"]:\n",
    "#     for tag in row:\n",
    "#         # if tag.startswith(\"B\"):\n",
    "#         #     tag_type = tag.split(\"-\")[1]\n",
    "#         split2freqs['train'][tag] += 1\n",
    "                \n",
    "# pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_ori['score'] = df_train_ori['tags'].apply(lambda x:sum([ 1 for tag in x if tag != 'O']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_train_ori))\n",
    "# print(len(df_train_ori[df_train_ori['score']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import randint\n",
    "# df_train_ori['select'] = df_train_ori['score'].apply(lambda x: randint(0,2) if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_sampled_df = df_train_ori[df_train_ori['select']==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in custom_sampled_df[\"tags\"]:\n",
    "#     for tag in row:\n",
    "#         # if tag.startswith(\"B\"):\n",
    "#         #     tag_type = tag.split(\"-\")[1]\n",
    "#         split2freqs['down_sample_1_3'][tag] += 1\n",
    "\n",
    "# pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_counts = pd.DataFrame.from_dict(split2freqs, orient=\"index\")\n",
    "\n",
    "# tag_counts['total_tag'] = tag_counts.sum(axis=1)\n",
    "# tag_counts['entity_ratio'] = tag_counts['O'] / tag_counts['total_tag']\n",
    "# tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math import log,sqrt, ceil\n",
    "\n",
    "# def custom_sampling_based_on_score(df, method):\n",
    "#   # Get stats of the class distribution of the dataset\n",
    "#   labels = sum(df['tags'],[])\n",
    "#   num_tokens = len(labels)\n",
    "#   ent = [label[2:] for label in labels if label != 'O']\n",
    "#   stats = Counter(ent)\n",
    "#   for key in stats:\n",
    "#       #Use frequency instead of count\n",
    "#       stats[key] = stats[key]/num_tokens\n",
    "\n",
    "#   if method not in ['sc','sCR','sCRD','nsCRD']:\n",
    "#     raise ValueError(\"Unidentified Resampling Method\")\n",
    "\n",
    "\n",
    "  \n",
    "#   # df['score'] = df['tags'].apply(lambda x: sum([ 0 if tag.startswith('O') else 1 for tag in x  ]) + 1)\n",
    "  \n",
    "#   custom_sampled_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "#   # for idx in range(len(df)):\n",
    "#   #     num_samples = df.iloc[idx]['score']\n",
    "#   #     sent = df.iloc[idx].to_dict()\n",
    "\n",
    "#   #     for i in range(num_samples):\n",
    "#   #     # Append the sampled subset to the list\n",
    "#   #       sampled_data.append(sent)\n",
    "#   #       # custom_sampled_df = pd.concat([custom_sampled_df, df.iloc[idx]], ignore_index=True, axis = 0)\n",
    "#   # # Concatenate the sampled subsets to create the final upsampled DataFrame\n",
    "#   # custom_sampled_df = pd.concat([custom_sampled_df, pd.DataFrame(sampled_data)], ignore_index=True)\n",
    "  \n",
    "#   for sen in range(len(df)):\n",
    "#     sampled_data = [] \n",
    "#     # Resampling time can at least be 1, which means sentence without \n",
    "#     # entity will be reserved in the dataset  \n",
    "#     rsp_time = 1\n",
    "#     sen_len = len(df.iloc[sen]['tags'])\n",
    "#     ents = Counter([label[2:] for label in df.iloc[sen]['tags'] if label != 'O'])\n",
    "#           # Pass if there's no entity in a sentence\n",
    "    \n",
    "    \n",
    "#     if ents:\n",
    "#       for ent in ents.keys():\n",
    "#         # Resampling method selection and resampling time calculation, \n",
    "#         # see section 'Resampling Functions' in our paper for details.\n",
    "#         if method == 'sc':\n",
    "#           rsp_time += ents[ent]\n",
    "#         if method == 'sCR' or method == 'sCRD':\n",
    "#           weight = -log(stats[ent],2)\n",
    "#           rsp_time += ents[ent]*weight\n",
    "#         if method == 'nsCRD':\n",
    "#           weight = -log(stats[ent],2)\n",
    "#           rsp_time += sqrt(ents[ent])*weight\n",
    "#         if method == 'sCR':\n",
    "#           rsp_time = sqrt(rsp_time)\n",
    "#         if method == 'sCRD' or method == 'nsCRD':\n",
    "#           rsp_time = rsp_time/sqrt(sen_len)\n",
    "#       # Ceiling to ensure the integrity of resamling time\n",
    "#       rsp_time = ceil(rsp_time) \n",
    "    \n",
    "#     for t in range(rsp_time):\n",
    "#       for token in range(sen_len):\n",
    "#         sampled_data.append(df.iloc[sen].to_dict())\n",
    "\n",
    "\n",
    "#     custom_sampled_df = pd.concat([custom_sampled_df, pd.DataFrame(sampled_data)], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   return custom_sampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. <a id='toc7_4_'></a>[Model tuning](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTuning:\n",
    "  def __init__(self, pretrained_model_name, index2tag, tag2index, y_validation):\n",
    "    \n",
    "    self.pretrained_model_name = pretrained_model_name\n",
    "    self.index2tag = index2tag\n",
    "    self.tag2index = tag2index\n",
    "\n",
    "    self.pretrained_model_config = AutoConfig.from_pretrained(pretrained_model_name, \n",
    "                                        num_labels=len(self.index2tag),\n",
    "                                        id2label=index2tag, label2id=tag2index)\n",
    "    \n",
    "    self.y_validation = y_validation\n",
    "    self.modelPerformanceLog = ModelPerformanceLog(y_validation)\n",
    "  \n",
    "  def model_init(self, architecture):\n",
    "    return (DistilBertForABSA\n",
    "            .from_pretrained(self.pretrained_model_name, config=self.pretrained_model_config, architecture=architecture)\n",
    "            .to(device))\n",
    "  \n",
    "  def align_predictions(self, predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                index2tag = self.pretrained_model_config.id2label.copy()\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list\n",
    "\n",
    "  # Define performance metrics\n",
    "  def compute_metrics(self, eval_pred):\n",
    "    y_pred, y_true = self.align_predictions(eval_pred.predictions, \n",
    "                                        eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred, average='macro')}\n",
    "\n",
    "  def forward_pass_with_label(self,data_collator, trainer, batch):\n",
    "    # Convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # Pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"]#.to(device)\n",
    "    attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "    labels = batch[\"labels\"]#.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Pass data through model  \n",
    "        output = trainer.model(input_ids, attention_mask)\n",
    "\n",
    "        # Logit.size: [batch_size, sequence_length, classes]\n",
    "        # Predict class with largest logit value on classes axis\n",
    "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "    # Calculate loss per token after flattening batch dimension with view\n",
    "    loss = cross_entropy(output.logits.view(-1, tags.num_classes), \n",
    "                          labels.view(-1), reduction=\"none\")\n",
    "    # Unflatten batch dimension and convert to numpy array\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "\n",
    "    return {\"loss\":loss, \"predicted_label\": predicted_label}\n",
    "\n",
    "  def tokenize_and_align_labels(self, tokenizer, examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, \n",
    "                                      is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"tags_idx\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "  \n",
    "  def encode_dataset(self, tokenizer, corpus):\n",
    "    return corpus.map(lambda examples: self.tokenize_and_align_labels(tokenizer, examples), \n",
    "                      batched=True, \n",
    "                      remove_columns=['tags_idx', 'tokens','tags']\n",
    "                      )\n",
    "     \n",
    "  def fine_tune_model(self, ds, model_name, architecture, num_epochs=6, batch_size=35, weight_decay=0.01, token_weights=None):\n",
    "    # Clear cache\n",
    "    torch.mps.empty_cache()\n",
    "    \n",
    "    # Define tokenizer\n",
    "    pretrained_tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model_name)\n",
    "\n",
    "    # Define data collator for data batching \n",
    "    data_collator = DataCollatorForTokenClassification(pretrained_tokenizer)\n",
    "\n",
    "    # self.data_collator = data_collator\n",
    "    # self.pretrained_tokenizer = pretrained_tokenizer\n",
    "    ds_encoded = self.encode_dataset(pretrained_tokenizer, ds)\n",
    "\n",
    "    logging_steps = len(ds_encoded['train']) // batch_size\n",
    "    model_name_ = f\"{self.pretrained_model_name}\" + model_name + f'_e_{num_epochs}_ld_{re.sub(r\"[^0-9]+\",\"\",str(weight_decay))}w_{\"T\" if token_weights != None else \"F\"}' \n",
    "    training_args = TrainingArguments(output_dir=\"models/\" + model_name_\n",
    "                                      ,log_level=\"error\"\n",
    "                                      ,num_train_epochs=num_epochs\n",
    "                                      ,per_device_train_batch_size=batch_size\n",
    "                                      ,per_device_eval_batch_size=batch_size\n",
    "                                      ,evaluation_strategy=\"epoch\"\n",
    "                                      ,save_steps=1e6\n",
    "                                      ,weight_decay=weight_decay\n",
    "                                      ,disable_tqdm=False\n",
    "                                      ,logging_steps=logging_steps\n",
    "                                      ,push_to_hub=False)\n",
    "    \n",
    "    # # Define token weights for loss calculation\n",
    "    # flatten_tags = sum(ds['train']['tags'],[])\n",
    "    # token_weights = Counter(flatten_tags)\n",
    "    # token_weights = [token_weights[x] for x in tags.names]\n",
    "    # token_weights = torch.tensor(1 / np.array(token_weights), dtype=torch.float32).to(device)\n",
    "    \n",
    "    trainer = Trainer(model_init=lambda: self.model_init(architecture)\n",
    "                      ,args=training_args\n",
    "                      ,data_collator=data_collator\n",
    "                      ,compute_metrics=self.compute_metrics\n",
    "                      ,train_dataset=ds_encoded[\"train\"]\n",
    "                      ,eval_dataset=ds_encoded[\"validation\"]\n",
    "                      ,tokenizer=pretrained_tokenizer)\n",
    "    \n",
    "    trainer.train()\n",
    "    # self.trainer = trainer\n",
    "\n",
    "    trainer_res_df = pd.DataFrame(trainer.state.log_history)[['epoch','loss' ,'eval_loss','eval_f1']]\n",
    "    trainer_res_df = trainer_res_df.rename(columns={\"epoch\":\"Epoch\",\"loss\": \"Training Loss\", \"eval_loss\": \"Validation Loss\", 'eval_f1':'F1'})\n",
    "    trainer_res_df['Epoch'] = trainer_res_df[\"Epoch\"].apply(lambda x: round(x))\n",
    "    trainer_res_df['Training Loss'] = trainer_res_df[\"Training Loss\"].ffill()\n",
    "    trainer_res_df[['Validation Loss', 'F1']] = trainer_res_df[['Validation Loss', 'F1']].bfill().ffill()\n",
    "    trainer_res_df.drop_duplicates()\n",
    "\n",
    "\n",
    "    validation_set = ds_encoded[\"validation\"]\n",
    "    validation_set = validation_set.map(lambda batch: self.forward_pass_with_label(data_collator, trainer, batch), batched=True, batch_size=32)\n",
    "    df_validation = validation_set.to_pandas()\n",
    "\n",
    "    # Cleanup & conver id2text \n",
    "    index2tag = trainer.model.config.id2label.copy()\n",
    "    index2tag[-100] = \"IGN\"\n",
    "    df_validation[\"input_tokens\"] = df_validation[\"input_ids\"].apply(\n",
    "        lambda x: pretrained_tokenizer.convert_ids_to_tokens(x))\n",
    "    df_validation[\"predicted_label_txt\"] = df_validation[\"predicted_label\"].apply(\n",
    "        lambda x: [index2tag[i] for i in x])\n",
    "    df_validation[\"labels_txt\"] = df_validation[\"labels\"].apply(\n",
    "        lambda x: [index2tag[i] for i in x])\n",
    "    df_validation['loss'] = df_validation.apply(\n",
    "        lambda x: x['loss'][:len(x['input_ids'])], axis=1) # Remove padding tokens\n",
    "    df_validation['predicted_label'] = df_validation.apply(\n",
    "        lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1) # Remove padding tokens\n",
    "    df_validation['predicted_label_txt'] = df_validation.apply(\n",
    "        lambda x: x['predicted_label_txt'][:len(x['input_ids'])], axis=1) # Remove padding tokens\n",
    "    df_validation['labels_txt'] = df_validation.apply(\n",
    "        lambda x: x['labels_txt'][:len(x['input_ids'])], axis=1) # Remove padding tokens\n",
    "\n",
    "\n",
    "    # Flatten the outputs\n",
    "    df_tokens = df_validation.apply(pd.Series.explode)\n",
    "    df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
    "    df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\n",
    "\n",
    "    df_validation_metrics = df_validation.copy()\n",
    "\n",
    "    df_validation_metrics['predicted_label_txt'] = df_validation_metrics.apply(lambda x: [x['predicted_label_txt'][i] for i,k in enumerate(x['labels_txt']) if k != 'IGN' ], axis=1)\n",
    "    # df_validation_metrics['labels_txt'] = df_validation_metrics.apply(lambda x: [k for i,k in enumerate(x['labels_txt']) if k != 'IGN' ], axis=1)\n",
    "\n",
    "    self.modelPerformanceLog.add_model_performance(model_name_,df_validation_metrics['predicted_label_txt'])\n",
    "\n",
    "    trainer.save_model('models/saved/'+ model_name_)\n",
    "    return trainer, trainer_res_df, df_validation, df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'tags_idx', 'tags'],\n",
       "    num_rows: 2125\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcaccfb746234b1d87fcd2ccccd7170b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb8361077464c0bbf0f6771171786ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b06f98ae00b46e783b1bff03a3666b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tina.vu/work/fun/sentilens_env_wipp/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f73c5b0c03480d8d70622c66fac66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3318, 'learning_rate': 8.19672131147541e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d771552b488840799b376d9f036f91f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22364850342273712, 'eval_f1': 0.13281226053639847, 'eval_runtime': 14.747, 'eval_samples_per_second': 61.775, 'eval_steps_per_second': 1.831, 'epoch': 1.0}\n",
      "{'train_runtime': 122.7856, 'train_samples_per_second': 17.307, 'train_steps_per_second': 0.497, 'train_loss': 0.3312143952142997, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5405d02d80e1450c9657937413ecccc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_model_name = \"distilbert-base-uncased\"\n",
    "tags = ds['train'].features['tags_idx'].feature\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "y_val = ds['validation'].to_pandas()['tags_idx']\n",
    "y_val = [[index2tag[tag] for tag in sent] for sent in y_val]\n",
    "\n",
    "modelTuning = ModelTuning(pretrained_model_name, index2tag, tag2index, y_val)\n",
    "\n",
    "trainer, trainer_res_df, df_validation_pred, df_tokens_pred = modelTuning.fine_tune_model(ds, 'test_bert', '', 1,35,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sklearn_f1_score</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>distilbert-base-uncasedtest_bert_e_1_ld_001w_F</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sklearn_f1_score     f1\n",
       "distilbert-base-uncasedtest_bert_e_1_ld_001w_F              0.22  0.181"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTuning.modelPerformanceLog.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531b1704af3149149fdec73e028242d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d9180d3a1d47dd8c7cf6c14d0565ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56895a84f01b4db7a936b49fe2974e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tina.vu/work/fun/sentilens_env_wipp/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2dc8165a5b04252824bad8fb544a00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.444, 'learning_rate': 8.19672131147541e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b301f548714e4132968ffb9b3663aaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3054786920547485, 'eval_f1': 0.0, 'eval_runtime': 14.9561, 'eval_samples_per_second': 60.912, 'eval_steps_per_second': 1.805, 'epoch': 1.0}\n",
      "{'train_runtime': 124.5677, 'train_samples_per_second': 17.059, 'train_steps_per_second': 0.49, 'train_loss': 1.4418267187525013, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b3a6695cf841d9887e367faf3462e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sklearn_f1_score</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>distilbert-base-uncasedtest_bert_e_1_ld_001w_F</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert-base-uncasedtest_bert_additional_linear_e_1_ld_001w_F</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    sklearn_f1_score     f1\n",
       "distilbert-base-uncasedtest_bert_e_1_ld_001w_F                  0.22  0.181\n",
       "distilbert-base-uncasedtest_bert_additional_lin...              0.14  0.000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer, trainer_res_df, df_validation_pred, df_tokens_pred = modelTuning.fine_tune_model(ds, 'test_bert_additional_linear', 'additional_linear', 1,35,0.01)\n",
    "modelTuning.modelPerformanceLog.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. <a id='toc7_5_'></a>[Error analysis](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1. <a id='toc7_5_1_'></a>[Group by word token](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "#     .agg([\"count\", \"mean\", \"sum\"])\n",
    "#     .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "#     .sort_values(by=\"sum\", ascending=False)\n",
    "#     .reset_index()\n",
    "#     .round(2)\n",
    "#     .head(10)\n",
    "#     .T\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#   df_tokens[df_tokens['input_tokens']=='battery'].groupby([\"predicted_label\", 'labels'])[[\"loss\"]]\n",
    "#     .agg([\"count\", \"mean\", \"sum\"])\n",
    "#     .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "#     .sort_values(by=\"sum\", ascending=False)\n",
    "#     .reset_index()\n",
    "#     .round(2)\n",
    "#     .head(10)\n",
    "#     .T\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.2. <a id='toc7_5_2_'></a>[Group by Tag ID](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     df_tokens.groupby(\"labels\")[[\"loss\"]] \n",
    "#     .agg([\"count\", \"mean\", \"sum\"])\n",
    "#     .droplevel(level=0, axis=1)\n",
    "#     .sort_values(by=\"mean\", ascending=False)\n",
    "#     .reset_index()\n",
    "#     .round(2)\n",
    "#     .T\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "#     cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "#     fig, ax = plt.subplots(figsize=(6, 6))\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "#     disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "#     plt.title(\"Normalized confusion matrix\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n",
    "#                       list(tag2index.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_samples(df):\n",
    "#     for _, row in df.iterrows():\n",
    "#         labels, preds, tokens, losses = [], [], [], []\n",
    "#         for i, mask in enumerate(row[\"attention_mask\"]):\n",
    "#             if i not in {0, len(row[\"attention_mask\"])}:\n",
    "#                 labels.append(row[\"labels\"][i])\n",
    "#                 preds.append(row[\"predicted_label\"][i])\n",
    "#                 tokens.append(row[\"input_tokens\"][i])\n",
    "#                 losses.append(f\"{row['loss'][i]:.2f}\")\n",
    "#         df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n",
    "#                                \"preds\": preds, \"losses\": losses}).T\n",
    "#         yield df_tmp\n",
    "\n",
    "# df_validation[\"total_loss\"] = df_validation[\"loss\"].apply(sum)\n",
    "# df_tmp = df_validation.sort_values(by=\"total_loss\", ascending=False).head(3)\n",
    "\n",
    "# for sample in get_samples(df_tmp):\n",
    "#     display(sample.T)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors could be from human / annotation errors: United Nations is ORG, not PER, similar to Central African Republic. This can happen as data was annotated using rule based, it is better with human annotations, but mistakes can always occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp = df_validation.loc[df_validation[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\n",
    "# for sample in get_samples(df_tmp):\n",
    "#     display(sample.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. <a id='toc7_6_'></a>[Load saved model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.1. <a id='toc7_6_1_'></a>[Load model manually](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# # Reload the model\n",
    "# loaded_model = AutoModelForTokenClassification.from_pretrained('model/distilbert-base-uncased-absa-downsample-1-3').to(device) #output_model_dir\n",
    "# loaded_tokenizer = AutoTokenizer.from_pretrained('model/distilbert-base-uncased-absa-downsample-1-3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 99\n",
    "# sample_input = df_train.iloc[num]['text']\n",
    "# print(sample_input)\n",
    "# print(df_train.iloc[num]['aspects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have some input data\n",
    "# input_data = [sample_input]\n",
    "\n",
    "# # Tokenize and get predictions\n",
    "# inputs = loaded_tokenizer(input_data, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "# input_ids = inputs[\"input_ids\"].to(device)\n",
    "# attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# # Make predictions\n",
    "# with torch.no_grad():\n",
    "#   outputs = loaded_model(input_ids, attention_mask)\n",
    "\n",
    "# predicted_label_idx = torch.argmax(outputs.logits, axis=-1).cpu().numpy()\n",
    "# df_res = pd.DataFrame({'predicted_label': predicted_label_idx.tolist(), \n",
    "#                       'input_ids': inputs['input_ids'].numpy().tolist()}\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index2tag_new = loaded_model.config.id2label.copy()\n",
    "# index2tag_new[-100] = \"IGN\"\n",
    "# df_res[\"input_tokens\"] = df_res[\"input_ids\"].apply(\n",
    "#     lambda x: loaded_tokenizer.convert_ids_to_tokens(x))\n",
    "# df_res[\"predicted_label_text\"] = df_res[\"predicted_label\"].apply(\n",
    "#     lambda x: [index2tag_new[i] for i in x])\n",
    "# df_res['predicted_label'] = df_res.apply(\n",
    "#     lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\n",
    "# df_res['predicted_label_text'] = df_res.apply(\n",
    "#     lambda x: x['predicted_label_text'][:len(x['input_ids'])], axis=1)\n",
    "\n",
    "# df_res_tokens = df_res.apply(pd.Series.explode)\n",
    "\n",
    "# df_res_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.2. <a id='toc7_6_2_'></a>[Pipeline](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# token_classifier = pipeline(\n",
    "#     \"token-classification\", model='model/distilbert-base-uncased-absa-downsample-1-3', aggregation_strategy=\"simple\"\n",
    "# )\n",
    "# token_classifier(sample_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentilens_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
