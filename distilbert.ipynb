{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Data preparation](#toc1_)    \n",
    "  - 1.1. [Tokenize sentence and aspect BIO encoding class](#toc1_1_)    \n",
    "  - 1.2. [Load data](#toc1_2_)    \n",
    "  - 1.3. [Inspect tagging issues](#toc1_3_)    \n",
    "  - 1.4. [Merge conflict to negative & remove neutral](#toc1_4_)    \n",
    "  - 1.5. [Convert df to HuggingFace datasets](#toc1_5_)    \n",
    "- 2. [EDA](#toc2_)    \n",
    "- 3. [Random forest](#toc3_)    \n",
    "- 4. [Logistic Regression](#toc4_)    \n",
    "- 5. [CRF](#toc5_)    \n",
    "- 6. [Bi-LSTM](#toc6_)    \n",
    "- 7. [BERT](#toc7_)    \n",
    "  - 7.1. [Define model](#toc7_1_)    \n",
    "    - 7.1.1. [Define tokenizer](#toc7_1_1_)    \n",
    "    - 7.1.2. [Define token classification model](#toc7_1_2_)    \n",
    "  - 7.2. [Data preparation](#toc7_2_)    \n",
    "  - 7.3. [Performance metrics](#toc7_3_)    \n",
    "  - 7.4. [Training](#toc7_4_)    \n",
    "    - 7.4.1. [Define trainer](#toc7_4_1_)    \n",
    "    - 7.4.2. [Training results](#toc7_4_2_)    \n",
    "  - 7.5. [Model performance](#toc7_5_)    \n",
    "    - 7.5.1. [Perform prediction on validation dataset](#toc7_5_1_)    \n",
    "    - 7.5.2. [Model performance](#toc7_5_2_)    \n",
    "  - 7.6. [Error analysis](#toc7_6_)    \n",
    "    - 7.6.1. [Group by word token](#toc7_6_1_)    \n",
    "    - 7.6.2. [Group by Tag ID](#toc7_6_2_)    \n",
    "  - 7.7. [Save model](#toc7_7_)    \n",
    "  - 7.8. [Load saved model](#toc7_8_)    \n",
    "    - 7.8.1. [Load model manually](#toc7_8_1_)    \n",
    "    - 7.8.2. [Pipeline](#toc7_8_2_)    \n",
    "- 8. [DistilBert with resampling](#toc8_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0' # this setting is needed to run NN on my Mac\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from highlight_text import HighlightText, ax_text, fig_text\n",
    "\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "# pip install torch==2.2.0 torchtext --index-url https://download.pytorch.org/whl/test/cpu\n",
    "# pip install torch==2.3.0.dev20240121 # this does not work\n",
    "\n",
    "# pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu # run this in CLI before running the notebook\n",
    "\n",
    "# import torch\n",
    "# if torch.backends.mps.is_available():\n",
    "#     mps_device = torch.device(\"mps\")\n",
    "#     x = torch.ones(1, device=mps_device)\n",
    "#     print (x)\n",
    "# else:\n",
    "#     print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Data preparation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Tokenize sentence and aspect BIO encoding class](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceToken:\n",
    "  '''\n",
    "    SentenceToken\n",
    "\n",
    "    This class takes care of word tokenize and tagging aspect entities\n",
    "  '''\n",
    "  def __init__(self, sentence, aspect_type=None, aspects=None, sentence_id=None):\n",
    "    \n",
    "    if sentence_id is not None:\n",
    "      print(sentence_id)\n",
    "\n",
    "    self.sentence_id = sentence_id\n",
    "    self.sentence = sentence.replace(u\"\\u00A0\", \" \").replace(u'\\xa0',' ') # replace unicode space character\n",
    "                            \n",
    "    self.aspect_bio_tags = None\n",
    "    self.unified_aspect_bio_tags = None\n",
    "    self.token_span = None\n",
    "    self.space_pre_token = None\n",
    "\n",
    "    # Tokenize sentence\n",
    "    self.__tokenize_sentence(self.sentence)\n",
    "\n",
    "    if aspect_type == 'dict':\n",
    "      self.set_aspect_tagging_from_dict(aspects)\n",
    "    elif aspect_type == 'bio':\n",
    "      self.set_aspect_bio_tags(aspects)\n",
    "    elif aspect_type == 'unified bio':\n",
    "      self.set_aspect_unified_bio_tags(aspects)\n",
    "  \n",
    "  def __tokenize_sentence(self, sentence):\n",
    "    '''\n",
    "    __tokenize_sentence\n",
    "\n",
    "    Break sentence into word token span\n",
    "    '''\n",
    "    token_span = list(TreebankWordTokenizer().span_tokenize(sentence))\n",
    "    \n",
    "    self.token_span = token_span\n",
    "    self.space_pre_token = [True if sentence[k[0]-1:k[0]] == ' ' else False for i,k in enumerate(token_span)]\n",
    "\n",
    "  def set_aspect_tagging_from_dict(self, aspects):\n",
    "    '''\n",
    "    set_aspect_tagging_from_dict\n",
    "\n",
    "    Calculate & assign aspect entities to token given an array of aspects (term, start_index, to_index, and polarity)\n",
    "    '''\n",
    "    polarity_map = {'positive':'POS'\n",
    "              ,'negative': 'NEG'\n",
    "              ,'conflict': 'CON'\n",
    "              ,'neutral': 'NEU'}\n",
    "    \n",
    "    bio_tags = ['O'] * len(self.token_span)\n",
    "    unified_bio_tags = bio_tags\n",
    "\n",
    "    for x in aspects:\n",
    "      if x['term'] != '':\n",
    "        aspect_from = int(x['from'])\n",
    "        aspect_to = int(x['to'])\n",
    "        polarity = '-' + polarity_map[x['polarity']]\n",
    "\n",
    "        aspect_from_index = [i for i, v in enumerate(self.token_span) if (v[0] <= aspect_from) & (v[1] >= aspect_from)][0]\n",
    "        aspect_to_index = [i for i, v in enumerate(self.token_span) if (v[0] <= aspect_to) & (v[1] >= aspect_to)][0]\n",
    "      \n",
    "        aspect_length = aspect_to_index - aspect_from_index\n",
    "        bio_tags = bio_tags[:aspect_from_index] + ['B'] + ['I'] * (aspect_length) + bio_tags[aspect_to_index+1:]\n",
    "        unified_bio_tags = unified_bio_tags[:aspect_from_index] + ['B' + polarity] + ['I'+ polarity] * (aspect_length) + unified_bio_tags[aspect_to_index+1:]\n",
    "\n",
    "    self.set_aspect_bio_tags(bio_tags)\n",
    "    self.set_aspect_unified_bio_tags(unified_bio_tags)\n",
    "\n",
    "  def rebuild_sentence_from_token(self):\n",
    "    '''\n",
    "    rebuild_sentence_from_token\n",
    "\n",
    "    Return sentence built from computed tokens\n",
    "    '''\n",
    "    return ''.join([(' ' if self.space_pre_token[i] else '') + self.sentence[k[0]:k[1]] for i, k in enumerate(self.token_span)])\n",
    " \n",
    "  def set_aspect_bio_tags(self, aspect_bio_tags):\n",
    "    '''\n",
    "    set_aspect_bio_tags\n",
    "\n",
    "    Setter method to set aspect_unified_bio_tags and aspect_bio_tags\n",
    "    '''\n",
    "    self.aspect_bio_tags = aspect_bio_tags\n",
    "    self.aspect_unified_bio_tags = aspect_bio_tags\n",
    "\n",
    "  def set_aspect_unified_bio_tags(self, aspect_unified_bio_tags):\n",
    "    ''''\n",
    "    set_aspect_unified_bio_tags\n",
    "    \n",
    "    Setter method to set aspect_unified_bio_tags and aspect_bio_tags\n",
    "    '''\n",
    "    self.aspect_unified_bio_tags = aspect_unified_bio_tags\n",
    "    self.aspect_bio_tags = [k[0:1] for k in aspect_unified_bio_tags]\n",
    "\n",
    "  def get_tokens(self):\n",
    "    '''\n",
    "    get_tokens()\n",
    "    Return an array of sentence word tokens\n",
    "    '''\n",
    "    return [self.sentence[k[0]:k[1]] for k in self.token_span]\n",
    "  \n",
    "  def check_rebuild_sentence_from_token(self):\n",
    "    '''\n",
    "    check_rebuild_sentence_from_token()\n",
    "\n",
    "    This is a test / debugger function.\n",
    "    This help validating if we have computed the sentence to token properly and whether we can re-compute the exact sentence from information stored.\n",
    "    '''\n",
    "    return re.sub(r'\\s+', ' ',self.sentence.strip()) == self.rebuild_sentence_from_token().strip()\n",
    "  \n",
    "  def check_rebuild_aspect_terms(self, aspect_dict):\n",
    "    '''\n",
    "    check_rebuild_aspect_terms(aspect_dict)\n",
    "\n",
    "    This is a test / debugger fucntion. \n",
    "    This help validate if we have compute the correct aspect terms as given by the aspect dict\n",
    "\n",
    "    INPUT:\n",
    "    aspect dict: array of aspect dictionaries in the following format\n",
    "      [{'term': 'storage', \n",
    "       'polarity': 'positive', \n",
    "       'from': '14', \n",
    "       'to': '21'}]\n",
    "    '''\n",
    "    aspect_dict = sorted(aspect_dict, key=lambda d: int(d['from']))\n",
    "    aspect_input = [k['term'].replace(u\"\\u00A0\", \" \").replace(u'\\xa0',' ') for k in aspect_dict if k['term'] != '' ]\n",
    "    aspect_computed = []\n",
    "    aspect = ''\n",
    "    \n",
    "    for i,k in enumerate(self.aspect_bio_tags):\n",
    "      token = self.sentence[self.token_span[i][0]:self.token_span[i][1]]\n",
    "      \n",
    "      if k == 'B':\n",
    "        if (self.aspect_bio_tags[i-1] == 'B' if i > 0 else False):\n",
    "          aspect_computed.append(aspect)\n",
    "        aspect = token\n",
    "      elif k == 'I':\n",
    "        aspect += ' ' * ((self.token_span[i][0] -  self.token_span[i-1][1]) if i > 0 else 0) + token\n",
    "      \n",
    "      if (aspect != '') & ((k == 'O') or (i == (len(self.aspect_bio_tags) - 1))):\n",
    "          aspect_computed.append(aspect)\n",
    "          aspect = ''\n",
    "\n",
    "    return [aspect_input == aspect_computed, aspect_input, aspect_computed]\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.rebuild_sentence_from_token()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Load data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_Data():\n",
    "  df_train = pd.read_json('data/laptop/train.json')\n",
    "  # First, I will need to drop some duplicated data in our training dataset, as identified in the EDA process.\n",
    "  # We have removed 12 duplicated records in our training dataset\n",
    "  df_train.drop_duplicates(subset='text', inplace=True)\n",
    "  print('df_train shape: ', df_train.shape)\n",
    "\n",
    "  df_val = pd.read_json('data/laptop/validate.json') # This will only be used for the very last step to evaluate how well the model is, but is input now for validating the BIO tagging to ensure the function works properly\n",
    "  print('df_val shape: ', df_val.shape)\n",
    "\n",
    "  df_train['sentence_token'] = df_train.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "  df_train['sentence_check'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_sentence_from_token(), axis=1)\n",
    "  df_train['aspect_check'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects']), axis=1)\n",
    "  df_train['aspect_check_TF'] = df_train.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects'])[0], axis=1)\n",
    "  df_train['tokens'] = df_train.apply(lambda x: x['sentence_token'].get_tokens(), axis=1)\n",
    "  df_train['tags'] = df_train.apply(lambda x: x['sentence_token'].aspect_unified_bio_tags, axis=1)\n",
    "\n",
    "  df_val['sentence_token'] = df_val.apply(lambda x: SentenceToken(x['text'], 'dict', x['aspects']), axis=1)\n",
    "  df_val['sentence_check'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_sentence_from_token(), axis=1)\n",
    "  df_val['aspect_check'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects']), axis=1)\n",
    "  df_val['aspect_check_TF'] = df_val.apply(lambda x: x['sentence_token'].check_rebuild_aspect_terms(x['aspects'])[0], axis=1)\n",
    "  df_val['tokens'] = df_val.apply(lambda x: x['sentence_token'].get_tokens(), axis=1)\n",
    "  df_val['tags'] = df_val.apply(lambda x: x['sentence_token'].aspect_unified_bio_tags, axis=1)\n",
    "  return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape:  (3036, 3)\n",
      "df_val shape:  (800, 3)\n",
      "# of df_train records having tokenizing issues:  0\n",
      "# of df_train records having aspect bio tagging issues:  36\n",
      "# of df_test records having tokenizing issues:  0\n",
      "# of df_test records having aspect bio tagging issues:  9\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = reload_Data()\n",
    "print('# of df_train records having tokenizing issues: ', len(df_train[df_train['sentence_check']==False]))\n",
    "print('# of df_train records having aspect bio tagging issues: ', len(df_train[df_train['aspect_check_TF']==False]))\n",
    "print('# of df_test records having tokenizing issues: ', len(df_test[df_test['sentence_check']==False]))\n",
    "print('# of df_test records having aspect bio tagging issues: ', len(df_test[df_test['aspect_check_TF']==False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. <a id='toc1_3_'></a>[Inspect tagging issues](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([ 125,  140,  220,  293,  374,  375,  431,  612,  656,  834,  922,  924,\n",
      "        953,  999, 1031, 1374, 1456, 1502, 1631, 1716, 1936, 1958, 2113, 2160,\n",
      "       2244, 2392, 2502, 2533, 2587, 2606, 2783, 2831, 2842, 2876, 2930, 2940],\n",
      "      dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(df_train[df_train['aspect_check_TF']==False].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'term': 'delivery service', 'polarity': 'negative', 'from': '59', 'to': '75'}]\n",
      "After way too many times sending the thing in for repairs (delivery service was slow, and without the laptop I had no access to the internet, and thus no way of tracking it to find out when I might hope to see my computer again), it finally kicked the bucket after just over 2 years.\n",
      "[False, ['delivery service'], ['(delivery service']]\n",
      "['After', 'way', 'too', 'many', 'times', 'sending', 'the', 'thing', 'in', 'for', 'repairs', '(', 'delivery', 'service', 'was', 'slow', ',', 'and', 'without', 'the', 'laptop', 'I', 'had', 'no', 'access', 'to', 'the', 'internet', ',', 'and', 'thus', 'no', 'way', 'of', 'tracking', 'it', 'to', 'find', 'out', 'when', 'I', 'might', 'hope', 'to', 'see', 'my', 'computer', 'again', ')', ',', 'it', 'finally', 'kicked', 'the', 'bucket', 'after', 'just', 'over', '2', 'years', '.']\n"
     ]
    }
   ],
   "source": [
    "num = 2606\n",
    "\n",
    "print(df_train.loc[num]['aspects'])\n",
    "print(df_train.loc[num]['text'])\n",
    "print(df_train.loc[num]['aspect_check'])\n",
    "print(df_train.loc[num]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the tagging issues due to word that are not separated properly from special characters/ punctuations. The issue is unavoidable in practice as reviews may not adherent to perfect grammar.\n",
    "\n",
    "I have tried to fix this issues to have 100% accuracy with further token breakdown to match the specified aspect tokens, however, this can break some of the standard logics for word tokenizer and further modelling. \n",
    "\n",
    "Therefore, I decided to include a whole token where the aspect may start or end, even if the index is in the middle of token, which may results with aspect tokens that could include extra characters than planned. This is the risk we will accept for this approach, and we can perform a cleaning process to remove these extra characters during implementation with actual use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. <a id='toc1_4_'></a>[Merge conflict to negative & remove neutral](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tags'] = df_train['tags'].apply(lambda x: [tag[:2] + 'NEG' if tag[2:] == 'CON' else tag for tag in x ])\n",
    "df_test['tags'] = df_test['tags'].apply(lambda x: [tag[:2] + 'NEG' if tag[2:] == 'CON' else tag for tag in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['tags'] = df_train['tags'].apply(lambda x: ['O' if tag[2:] == 'NEU' else tag for tag in x])\n",
    "# df_test['tags'] = df_test['tags'].apply(lambda x: ['O' if tag[2:] == 'NEU' else tag for tag in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. <a id='toc1_5_'></a>[Convert df to HuggingFace datasets](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, ClassLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert aspect tag to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of unique aspect tags\n",
    "tags = list(set(sum(df_train['tags'],[])))\n",
    "tags.sort()\n",
    "tag2idx = {k:i for i,k in enumerate(tags)}\n",
    "\n",
    "# Convert aspect tag text to ids\n",
    "df_train['tags_idx'] = df_train['tags'].apply(lambda x: [tag2idx[k] for k in x])\n",
    "df_test['tags_idx'] = df_test['tags'].apply(lambda x: [tag2idx[k] for k in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NEG', 'B-NEU', 'B-POS', 'I-NEG', 'I-NEU', 'I-POS', 'O']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pandas to HuggingFace datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into train & validation set\n",
    "df_train_ori, df_val = train_test_split(df_train, test_size=0.3, random_state=42,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags_idx'],\n",
      "        num_rows: 2125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags_idx'],\n",
      "        num_rows: 911\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags_idx'],\n",
      "        num_rows: 800\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Define dataset features\n",
    "features = Features({'tokens': Sequence(Value(dtype='string', id=None)),\n",
    "                    'tags_idx': Sequence(ClassLabel(names=tags))\n",
    "                    })\n",
    "\n",
    "tds = Dataset.from_pandas(df_train_ori[['tokens','tags_idx']], features=features, preserve_index=False)\n",
    "vds = Dataset.from_pandas(df_val[['tokens','tags_idx']], features=features,  preserve_index=False)\n",
    "tsds = Dataset.from_pandas(df_test[['tokens','tags_idx']], features=features, preserve_index=False)\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = tds\n",
    "ds['validation'] = vds\n",
    "ds['test'] = tsds\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for total counts per aspect type in each data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cc07d643374bed88302ad44643ccd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a88f5d497ae4ac58b4c914bc4d2ed74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57e6ac4149c433581a238f6a687d964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert aspect idx to aspect aspect text\n",
    "def create_tag_names(batch):\n",
    "    return {\"tags\": [ds[\"train\"].features[\"tags_idx\"].feature.int2str(idx) for idx in batch[\"tags_idx\"]]}\n",
    "\n",
    "ds = ds.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEG</th>\n",
       "      <th>POS</th>\n",
       "      <th>NEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>582</td>\n",
       "      <td>675</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>325</td>\n",
       "      <td>312</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>144</td>\n",
       "      <td>340</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NEG  POS  NEU\n",
       "train       582  675  304\n",
       "validation  325  312  157\n",
       "test        144  340  169"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform aspect type counts per each dataset split\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in ds.items():\n",
    "    for row in dataset[\"tags\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "                \n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[EDA](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPerformanceLog:\n",
    "  \n",
    "  def __init__(self, y_true):\n",
    "    self.y_true = y_true\n",
    "    self.log = {}\n",
    "\n",
    "  def add_model_performance(self, model_name, y_pred, model=None): # Model can be path to the model or something like that\n",
    "    f1, clf_report = self.__calculate_metrics(y_pred)\n",
    "\n",
    "    model_perf = {'y_pred': y_pred\n",
    "                  ,'model': model\n",
    "                  ,'f1': f1.round(2)\n",
    "                  ,'report': clf_report\n",
    "                  }\n",
    "    \n",
    "    self.log[model_name] = model_perf\n",
    "\n",
    "  def __calculate_metrics(self, y_pred):\n",
    "    return f1_score(self.y_true, y_pred), classification_report(self.y_true, y_pred, zero_division=0)\n",
    "  \n",
    "  def get_logs(self):\n",
    "    return pd.DataFrame(list(self.log.values()), index=self.log.keys())\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPerformanceLog = ModelPerformanceLog(ds['validation']['tags_idx'])\n",
    "# modelPerformanceLog.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Random forest](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[Logistic Regression](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[CRF](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[Bi-LSTM](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id='toc7_'></a>[BERT](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from transformers import AutoConfig, DistilBertConfig, AutoTokenizer, TrainingArguments, DataCollatorForTokenClassification, Trainer\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertModel\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertPreTrainedModel\n",
    "\n",
    "device = torch.device('mps') # This is required for Mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. <a id='toc7_1_'></a>[Define model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1. <a id='toc7_1_1_'></a>[Define tokenizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(distilbert_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2. <a id='toc7_1_2_'></a>[Define token classification model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ds['train'].features['tags_idx'].feature\n",
    "\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_config = AutoConfig.from_pretrained(distilbert_model_name, \n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define token weights for loss calculation\n",
    "\n",
    "flatten_tags = sum(ds['train']['tags'],[])\n",
    "token_weights = Counter(flatten_tags)\n",
    "token_weights = [token_weights[x] for x in tags.names]\n",
    "token_weights = torch.tensor(1 / np.array(token_weights), dtype=torch.float32).to(device)\n",
    "token_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertForABSA(DistilBertPreTrainedModel):\n",
    "    config_class = DistilBertConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Roberta body\n",
    "        self.num_labels = config.num_labels\n",
    "        self.distilbert = DistilBertModel(config)#, add_pooling_layer=False)\n",
    "\n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(config.dropout)#hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Load and initialize weights from pretrained\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                labels=None, **kwargs):\n",
    "        \n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.distilbert(input_ids, attention_mask=attention_mask,\n",
    "                            #    token_type_ids=token_type_ids,\n",
    "                                 **kwargs)\n",
    "        \n",
    "        # Apply classifier to encoder representation (model head)\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, \n",
    "                                     hidden_states=outputs.hidden_states, \n",
    "                                     attentions=outputs.attentions)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. <a id='toc7_2_'></a>[Data preparation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = distilbert_tokenizer(examples[\"tokens\"], truncation=True, \n",
    "                                      is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"tags_idx\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True, \n",
    "                      remove_columns=['tags_idx', 'tokens','tags']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "ds_encoded = encode_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. <a id='toc7_3_'></a>[Performance metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                index2tag = distilbert_config.id2label.copy()\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, \n",
    "                                       eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred, average='macro')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. <a id='toc7_4_'></a>[Training](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1. <a id='toc7_4_1_'></a>[Define trainer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "num_epochs = 15\n",
    "batch_size = 24\n",
    "logging_steps = len(ds[\"train\"]) // batch_size\n",
    "model_name = f\"{distilbert_model_name}-absa-resample\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model/\" + model_name, log_level=\"error\", num_train_epochs=num_epochs, \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\", \n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False, \n",
    "    logging_steps=logging_steps, push_to_hub=False)\n",
    "\n",
    "# Define data collator for data batching \n",
    "data_collator = DataCollatorForTokenClassification(distilbert_tokenizer)\n",
    "     \n",
    "# Model init\n",
    "def model_init():\n",
    "    return (DistilBertForABSA\n",
    "            .from_pretrained(distilbert_model_name, config=distilbert_config)\n",
    "            .to(device))\n",
    "     \n",
    "trainer = Trainer(model_init=model_init, args=training_args, \n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                  train_dataset=ds_encoded[\"train\"],\n",
    "                  eval_dataset=ds_encoded[\"validation\"], \n",
    "                  tokenizer=distilbert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2. <a id='toc7_4_2_'></a>[Training results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(trainer.state.log_history)[['epoch','loss' ,'eval_loss','eval_f1']]\n",
    "df = df.rename(columns={\"epoch\":\"Epoch\",\"loss\": \"Training Loss\", \"eval_loss\": \"Validation Loss\", 'eval_f1':'F1'})\n",
    "df['Epoch'] = df[\"Epoch\"].apply(lambda x: round(x))\n",
    "df['Training Loss'] = df[\"Training Loss\"].ffill()\n",
    "df[['Validation Loss', 'F1']] = df[['Validation Loss', 'F1']].bfill().ffill()\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. <a id='toc7_5_'></a>[Model performance](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1. <a id='toc7_5_1_'></a>[Perform prediction on validation dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_with_label(batch):\n",
    "    # Convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # Pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        # Pass data through model  \n",
    "        output = trainer.model(input_ids, attention_mask)\n",
    "\n",
    "        # Logit.size: [batch_size, sequence_length, classes]\n",
    "        # Predict class with largest logit value on classes axis\n",
    "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "    # Calculate loss per token after flattening batch dimension with view\n",
    "    loss = cross_entropy(output.logits.view(-1, tags.num_classes), \n",
    "                         labels.view(-1), reduction=\"none\")\n",
    "    # Unflatten batch dimension and convert to numpy array\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "\n",
    "    return {\"loss\":loss, \"predicted_label\": predicted_label}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = ds_encoded[\"validation\"]\n",
    "validation_set = validation_set.map(forward_pass_with_label, batched=True, batch_size=32)\n",
    "df_validation = validation_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup & conver id2text \n",
    "index2tag = trainer.model.config.id2label.copy()\n",
    "index2tag[-100] = \"IGN\"\n",
    "df_validation[\"input_tokens\"] = df_validation[\"input_ids\"].apply(\n",
    "    lambda x: distilbert_tokenizer.convert_ids_to_tokens(x))\n",
    "df_validation[\"predicted_label\"] = df_validation[\"predicted_label\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "df_validation[\"labels\"] = df_validation[\"labels\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "df_validation['loss'] = df_validation.apply(\n",
    "    lambda x: x['loss'][:len(x['input_ids'])], axis=1) # Remove padding tokens\n",
    "df_validation['predicted_label'] = df_validation.apply(\n",
    "    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1) # Remove padding tokens\n",
    "\n",
    "# Flatten the outputs\n",
    "df_tokens = df_validation.apply(pd.Series.explode)\n",
    "df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
    "df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.2. <a id='toc7_5_2_'></a>[Model performance](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_metrics = df_validation.copy()\n",
    "\n",
    "df_validation_metrics['predicted_label'] = df_validation_metrics.apply(lambda x: [x['predicted_label'][i] for i,k in enumerate(x['labels']) if k != 'IGN' ], axis=1)\n",
    "df_validation_metrics['labels'] = df_validation_metrics.apply(lambda x: [k for i,k in enumerate(x['labels']) if k != 'IGN' ], axis=1)\n",
    "\n",
    "print(f\"F1 Score: {f1_score(df_validation_metrics['labels'], df_validation_metrics['predicted_label']).round(2)}\")\n",
    "print(classification_report(df_validation_metrics['labels'], df_validation_metrics['predicted_label'], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. <a id='toc7_6_'></a>[Error analysis](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.1. <a id='toc7_6_1_'></a>[Group by word token](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "  df_tokens[df_tokens['input_tokens']=='battery'].groupby([\"predicted_label\", 'labels'])[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.2. <a id='toc7_6_2_'></a>[Group by Tag ID](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"labels\")[[\"loss\"]] \n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)\n",
    "    .sort_values(by=\"mean\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n",
    "                      list(tag2index.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(df):\n",
    "    for _, row in df.iterrows():\n",
    "        labels, preds, tokens, losses = [], [], [], []\n",
    "        for i, mask in enumerate(row[\"attention_mask\"]):\n",
    "            if i not in {0, len(row[\"attention_mask\"])}:\n",
    "                labels.append(row[\"labels\"][i])\n",
    "                preds.append(row[\"predicted_label\"][i])\n",
    "                tokens.append(row[\"input_tokens\"][i])\n",
    "                losses.append(f\"{row['loss'][i]:.2f}\")\n",
    "        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n",
    "                               \"preds\": preds, \"losses\": losses}).T\n",
    "        yield df_tmp\n",
    "\n",
    "df_validation[\"total_loss\"] = df_validation[\"loss\"].apply(sum)\n",
    "df_tmp = df_validation.sort_values(by=\"total_loss\", ascending=False).head(3)\n",
    "\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample.T)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors could be from human / annotation errors: United Nations is ORG, not PER, similar to Central African Republic. This can happen as data was annotated using rule based, it is better with human annotations, but mistakes can always occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_validation.loc[df_validation[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7. <a id='toc7_7_'></a>[Save model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model/distilbert-base-uncased-absa-downsample-1-3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8. <a id='toc7_8_'></a>[Load saved model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8.1. <a id='toc7_8_1_'></a>[Load model manually](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# Reload the model\n",
    "loaded_model = AutoModelForTokenClassification.from_pretrained('model/distilbert-base-uncased-absa-downsample-1-3').to(device) #output_model_dir\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained('model/distilbert-base-uncased-absa-downsample-1-3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 99\n",
    "sample_input = df_train.iloc[num]['text']\n",
    "print(sample_input)\n",
    "print(df_train.iloc[num]['aspects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have some input data\n",
    "input_data = [sample_input]\n",
    "\n",
    "# Tokenize and get predictions\n",
    "inputs = loaded_tokenizer(input_data, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "  outputs = loaded_model(input_ids, attention_mask)\n",
    "\n",
    "predicted_label_idx = torch.argmax(outputs.logits, axis=-1).cpu().numpy()\n",
    "df_res = pd.DataFrame({'predicted_label': predicted_label_idx.tolist(), \n",
    "                      'input_ids': inputs['input_ids'].numpy().tolist()}\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag_new = loaded_model.config.id2label.copy()\n",
    "index2tag_new[-100] = \"IGN\"\n",
    "df_res[\"input_tokens\"] = df_res[\"input_ids\"].apply(\n",
    "    lambda x: loaded_tokenizer.convert_ids_to_tokens(x))\n",
    "df_res[\"predicted_label_text\"] = df_res[\"predicted_label\"].apply(\n",
    "    lambda x: [index2tag_new[i] for i in x])\n",
    "df_res['predicted_label'] = df_res.apply(\n",
    "    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\n",
    "df_res['predicted_label_text'] = df_res.apply(\n",
    "    lambda x: x['predicted_label_text'][:len(x['input_ids'])], axis=1)\n",
    "\n",
    "df_res_tokens = df_res.apply(pd.Series.explode)\n",
    "\n",
    "df_res_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8.2. <a id='toc7_8_2_'></a>[Pipeline](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model='model/distilbert-base-uncased-absa-downsample-1-3', aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. <a id='toc8_'></a>[DistilBert with resampling](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_sampling_based_on_score(df, score_column='Score', target_samples_per_score=100):\n",
    "#     sampled_data = []\n",
    "    \n",
    "#     # Calculate the sampling probabilities based on the scores\n",
    "#     scores = df[score_column].unique() \n",
    "#     probabilities = scores / sum(scores)\n",
    "\n",
    "#     probabilities_df = pd.DataFrame({'probability': probabilities}, index=scores)\n",
    "#     probabilities_df['num_samples'] = np.round(probabilities_df['probability'] * target_samples_per_score).astype(int)\n",
    "#     probabilities_df.loc[probabilities_df.index[-1], 'num_samples'] += target_samples_per_score - probabilities_df['num_samples'].sum()\n",
    "#     # Determine the number of samples to draw for each score\n",
    "#     # samples_per_score = np.round(probabilities * target_samples_per_score).astype(int)\n",
    "    \n",
    "#     # Ensure the total number of samples is equal to the target_samples_per_score\n",
    "#     # samples_per_score[-1] += target_samples_per_score - samples_per_score.sum()\n",
    "    \n",
    "#     # Iterate over unique scores and sample sentences accordingly\n",
    "#     for score_value in probabilities_df.index:\n",
    "#         num_samples = probabilities_df.loc[score_value]['num_samples']\n",
    "        \n",
    "#         # Extract sentences with a specific score\n",
    "#         subset = df[df[score_column] == score_value]\n",
    "        \n",
    "#         # Sample sentences based on the calculated number of samples\n",
    "#         sampled_subset = subset.sample(n=int(num_samples), replace=True, random_state=42)\n",
    "        \n",
    "#         # Append the sampled subset to the list\n",
    "#         sampled_data.append(sampled_subset)\n",
    "    \n",
    "#     # Concatenate the sampled subsets to create the final upsampled DataFrame\n",
    "#     custom_sampled_df = pd.concat(sampled_data, ignore_index=True)\n",
    "    \n",
    "#     return custom_sampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_sampling_based_on_score(df):\n",
    "#     sampled_data = []\n",
    "    \n",
    "#     df['score'] = df['tags'].apply(lambda x: sum([ 0 if tag.startswith('O') else 1 for tag in x  ]) + 1)\n",
    "    \n",
    "#     custom_sampled_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "#     for idx in range(len(df)):\n",
    "#         num_samples = df.iloc[idx]['score']\n",
    "#         sent = df.iloc[idx].to_dict()\n",
    "\n",
    "#         for i in range(num_samples):\n",
    "#         # Append the sampled subset to the list\n",
    "#           sampled_data.append(sent)\n",
    "#           # custom_sampled_df = pd.concat([custom_sampled_df, df.iloc[idx]], ignore_index=True, axis = 0)\n",
    "#     # Concatenate the sampled subsets to create the final upsampled DataFrame\n",
    "#     custom_sampled_df = pd.concat([custom_sampled_df, pd.DataFrame(sampled_data)], ignore_index=True)\n",
    "    \n",
    "#     return custom_sampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the target number of samples per score\n",
    "target_samples_per_score = 3000\n",
    "\n",
    "# Use the custom sampling method\n",
    "custom_sampled_df = custom_sampling_based_on_score(df_train_ori.copy(), 'nsCRD')\n",
    "\n",
    "# Check the distribution of scores in the custom sampled DataFrame\n",
    "# print(custom_sampled_df['entities_score'].value_counts())\n",
    "print(len(custom_sampled_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform aspect type counts per each dataset split\n",
    "split2freqs = defaultdict(Counter)\n",
    "\n",
    "for row in df_train_ori[\"tags\"]:\n",
    "    for tag in row:\n",
    "        # if tag.startswith(\"B\"):\n",
    "        #     tag_type = tag.split(\"-\")[1]\n",
    "        split2freqs['train'][tag] += 1\n",
    "                \n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ori['score'] = df_train_ori['tags'].apply(lambda x:sum([ 1 for tag in x if tag != 'O']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train_ori))\n",
    "print(len(df_train_ori[df_train_ori['score']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "df_train_ori['select'] = df_train_ori['score'].apply(lambda x: randint(0,2) if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sampled_df = df_train_ori[df_train_ori['select']==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in custom_sampled_df[\"tags\"]:\n",
    "    for tag in row:\n",
    "        # if tag.startswith(\"B\"):\n",
    "        #     tag_type = tag.split(\"-\")[1]\n",
    "        split2freqs['down_sample_1_3'][tag] += 1\n",
    "\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counts = pd.DataFrame.from_dict(split2freqs, orient=\"index\")\n",
    "\n",
    "tag_counts['total_tag'] = tag_counts.sum(axis=1)\n",
    "tag_counts['entity_ratio'] = tag_counts['O'] / tag_counts['total_tag']\n",
    "tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log,sqrt, ceil\n",
    "\n",
    "def custom_sampling_based_on_score(df, method):\n",
    "  # Get stats of the class distribution of the dataset\n",
    "  labels = sum(df['tags'],[])\n",
    "  num_tokens = len(labels)\n",
    "  ent = [label[2:] for label in labels if label != 'O']\n",
    "  stats = Counter(ent)\n",
    "  for key in stats:\n",
    "      #Use frequency instead of count\n",
    "      stats[key] = stats[key]/num_tokens\n",
    "\n",
    "  if method not in ['sc','sCR','sCRD','nsCRD']:\n",
    "    raise ValueError(\"Unidentified Resampling Method\")\n",
    "\n",
    "\n",
    "  \n",
    "  # df['score'] = df['tags'].apply(lambda x: sum([ 0 if tag.startswith('O') else 1 for tag in x  ]) + 1)\n",
    "  \n",
    "  custom_sampled_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "  # for idx in range(len(df)):\n",
    "  #     num_samples = df.iloc[idx]['score']\n",
    "  #     sent = df.iloc[idx].to_dict()\n",
    "\n",
    "  #     for i in range(num_samples):\n",
    "  #     # Append the sampled subset to the list\n",
    "  #       sampled_data.append(sent)\n",
    "  #       # custom_sampled_df = pd.concat([custom_sampled_df, df.iloc[idx]], ignore_index=True, axis = 0)\n",
    "  # # Concatenate the sampled subsets to create the final upsampled DataFrame\n",
    "  # custom_sampled_df = pd.concat([custom_sampled_df, pd.DataFrame(sampled_data)], ignore_index=True)\n",
    "  \n",
    "  for sen in range(len(df)):\n",
    "    sampled_data = [] \n",
    "    # Resampling time can at least be 1, which means sentence without \n",
    "    # entity will be reserved in the dataset  \n",
    "    rsp_time = 1\n",
    "    sen_len = len(df.iloc[sen]['tags'])\n",
    "    ents = Counter([label[2:] for label in df.iloc[sen]['tags'] if label != 'O'])\n",
    "          # Pass if there's no entity in a sentence\n",
    "    \n",
    "    \n",
    "    if ents:\n",
    "      for ent in ents.keys():\n",
    "        # Resampling method selection and resampling time calculation, \n",
    "        # see section 'Resampling Functions' in our paper for details.\n",
    "        if method == 'sc':\n",
    "          rsp_time += ents[ent]\n",
    "        if method == 'sCR' or method == 'sCRD':\n",
    "          weight = -log(stats[ent],2)\n",
    "          rsp_time += ents[ent]*weight\n",
    "        if method == 'nsCRD':\n",
    "          weight = -log(stats[ent],2)\n",
    "          rsp_time += sqrt(ents[ent])*weight\n",
    "        if method == 'sCR':\n",
    "          rsp_time = sqrt(rsp_time)\n",
    "        if method == 'sCRD' or method == 'nsCRD':\n",
    "          rsp_time = rsp_time/sqrt(sen_len)\n",
    "      # Ceiling to ensure the integrity of resamling time\n",
    "      rsp_time = ceil(rsp_time) \n",
    "    \n",
    "    for t in range(rsp_time):\n",
    "      for token in range(sen_len):\n",
    "        sampled_data.append(df.iloc[sen].to_dict())\n",
    "\n",
    "\n",
    "    custom_sampled_df = pd.concat([custom_sampled_df, pd.DataFrame(sampled_data)], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  return custom_sampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentilens_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
